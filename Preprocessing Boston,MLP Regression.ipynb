{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "lIYdn1woOS1n"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "boston = fetch_openml(name='boston', version=1, as_frame=True)\n",
        "x= boston.data\n",
        "y = boston.target\n",
        "\n",
        "print(x.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiVoybHANMYi",
        "outputId": "2947e3fd-2fd5-44c9-8700-e660779e701a"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      CRIM    ZN  INDUS CHAS    NOX  ...  RAD    TAX  PTRATIO       B  LSTAT\n",
            "0  0.00632  18.0   2.31    0  0.538  ...    1  296.0     15.3  396.90   4.98\n",
            "1  0.02731   0.0   7.07    0  0.469  ...    2  242.0     17.8  396.90   9.14\n",
            "2  0.02729   0.0   7.07    0  0.469  ...    2  242.0     17.8  392.83   4.03\n",
            "3  0.03237   0.0   2.18    0  0.458  ...    3  222.0     18.7  394.63   2.94\n",
            "4  0.06905   0.0   2.18    0  0.458  ...    3  222.0     18.7  396.90   5.33\n",
            "\n",
            "[5 rows x 13 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n"
      ],
      "metadata": {
        "id": "dVQTePM6Nb0M"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.metrics as met\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "oZHEDe5_Q3iz"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.DataFrame(x, columns=boston.feature_names)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "2uBUicU0R2H7",
        "outputId": "ad66ffb0-481c-4668-89b2-c858df619039"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        CRIM    ZN  INDUS CHAS    NOX  ...  RAD    TAX  PTRATIO       B  LSTAT\n",
              "0    0.00632  18.0   2.31    0  0.538  ...    1  296.0     15.3  396.90   4.98\n",
              "1    0.02731   0.0   7.07    0  0.469  ...    2  242.0     17.8  396.90   9.14\n",
              "2    0.02729   0.0   7.07    0  0.469  ...    2  242.0     17.8  392.83   4.03\n",
              "3    0.03237   0.0   2.18    0  0.458  ...    3  222.0     18.7  394.63   2.94\n",
              "4    0.06905   0.0   2.18    0  0.458  ...    3  222.0     18.7  396.90   5.33\n",
              "..       ...   ...    ...  ...    ...  ...  ...    ...      ...     ...    ...\n",
              "501  0.06263   0.0  11.93    0  0.573  ...    1  273.0     21.0  391.99   9.67\n",
              "502  0.04527   0.0  11.93    0  0.573  ...    1  273.0     21.0  396.90   9.08\n",
              "503  0.06076   0.0  11.93    0  0.573  ...    1  273.0     21.0  396.90   5.64\n",
              "504  0.10959   0.0  11.93    0  0.573  ...    1  273.0     21.0  393.45   6.48\n",
              "505  0.04741   0.0  11.93    0  0.573  ...    1  273.0     21.0  396.90   7.88\n",
              "\n",
              "[506 rows x 13 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-19cc41ea-738e-4c0c-a746-3f5ff228e037\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1</td>\n",
              "      <td>296.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.8</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.63</td>\n",
              "      <td>2.94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.2</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>501</th>\n",
              "      <td>0.06263</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.593</td>\n",
              "      <td>69.1</td>\n",
              "      <td>2.4786</td>\n",
              "      <td>1</td>\n",
              "      <td>273.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>391.99</td>\n",
              "      <td>9.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>502</th>\n",
              "      <td>0.04527</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.120</td>\n",
              "      <td>76.7</td>\n",
              "      <td>2.2875</td>\n",
              "      <td>1</td>\n",
              "      <td>273.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>503</th>\n",
              "      <td>0.06076</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.976</td>\n",
              "      <td>91.0</td>\n",
              "      <td>2.1675</td>\n",
              "      <td>1</td>\n",
              "      <td>273.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>504</th>\n",
              "      <td>0.10959</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.794</td>\n",
              "      <td>89.3</td>\n",
              "      <td>2.3889</td>\n",
              "      <td>1</td>\n",
              "      <td>273.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>393.45</td>\n",
              "      <td>6.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>505</th>\n",
              "      <td>0.04741</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.030</td>\n",
              "      <td>80.8</td>\n",
              "      <td>2.5050</td>\n",
              "      <td>1</td>\n",
              "      <td>273.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>396.90</td>\n",
              "      <td>7.88</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>506 rows × 13 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-19cc41ea-738e-4c0c-a746-3f5ff228e037')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-19cc41ea-738e-4c0c-a746-3f5ff228e037 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-19cc41ea-738e-4c0c-a746-3f5ff228e037');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-41c64af6-a520-4d7a-af74-fbd0cedd0dc5\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-41c64af6-a520-4d7a-af74-fbd0cedd0dc5')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-41c64af6-a520-4d7a-af74-fbd0cedd0dc5 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 506,\n  \"fields\": [\n    {\n      \"column\": \"CRIM\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.601545105332487,\n        \"min\": 0.00632,\n        \"max\": 88.9762,\n        \"num_unique_values\": 504,\n        \"samples\": [\n          0.09178,\n          0.05644,\n          0.10574\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ZN\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 23.322452994515036,\n        \"min\": 0.0,\n        \"max\": 100.0,\n        \"num_unique_values\": 26,\n        \"samples\": [\n          25.0,\n          30.0,\n          18.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"INDUS\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.8603529408975845,\n        \"min\": 0.46,\n        \"max\": 27.74,\n        \"num_unique_values\": 76,\n        \"samples\": [\n          8.14,\n          1.47,\n          1.22\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CHAS\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"1\",\n          \"0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NOX\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11587767566755611,\n        \"min\": 0.385,\n        \"max\": 0.871,\n        \"num_unique_values\": 81,\n        \"samples\": [\n          0.401,\n          0.538\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RM\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7026171434153237,\n        \"min\": 3.561,\n        \"max\": 8.78,\n        \"num_unique_values\": 446,\n        \"samples\": [\n          6.849,\n          4.88\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AGE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 28.148861406903638,\n        \"min\": 2.9,\n        \"max\": 100.0,\n        \"num_unique_values\": 356,\n        \"samples\": [\n          51.8,\n          33.8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DIS\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.1057101266276104,\n        \"min\": 1.1296,\n        \"max\": 12.1265,\n        \"num_unique_values\": 412,\n        \"samples\": [\n          2.2955,\n          4.2515\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RAD\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"7\",\n          \"2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TAX\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 168.53711605495926,\n        \"min\": 187.0,\n        \"max\": 711.0,\n        \"num_unique_values\": 66,\n        \"samples\": [\n          370.0,\n          666.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PTRATIO\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.164945523714446,\n        \"min\": 12.6,\n        \"max\": 22.0,\n        \"num_unique_values\": 46,\n        \"samples\": [\n          19.6,\n          15.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 91.29486438415779,\n        \"min\": 0.32,\n        \"max\": 396.9,\n        \"num_unique_values\": 357,\n        \"samples\": [\n          396.24,\n          395.11\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LSTAT\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.141061511348571,\n        \"min\": 1.73,\n        \"max\": 37.97,\n        \"num_unique_values\": 455,\n        \"samples\": [\n          6.15,\n          4.32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['price']=y"
      ],
      "metadata": {
        "id": "drAY_jzXSxfc"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler=preprocessing.StandardScaler().fit(x)\n",
        "x_scalerd=scaler.transform(x)"
      ],
      "metadata": {
        "id": "3msoYmZjGJDM"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_scalerd.mean(axis=0)"
      ],
      "metadata": {
        "id": "Eq6uP10LHfC-",
        "outputId": "b8b0a919-7305-42df-efb1-396d8c2264d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1.12338772e-16,  7.89881994e-17,  2.10635198e-16, -3.51058664e-17,\n",
              "       -1.96592852e-16, -1.08828186e-16, -1.47444639e-16, -8.42540793e-17,\n",
              "       -1.12338772e-16,  0.00000000e+00, -4.21270397e-16, -7.44244367e-16,\n",
              "       -3.08931624e-16])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_scalerd.std(axis=0)"
      ],
      "metadata": {
        "id": "NjoaKgu0Hot8",
        "outputId": "5c999dd6-af5b-4e2e-ec6f-6049a8ee431f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test,y_train,y_test=train_test_split(x_scalerd,y,test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "3o-Hs0fZTGQy"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaC1dcOoVE4r",
        "outputId": "abfce015-b782-402b-ace2-df1cd10a4103"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(354, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-_WPRGNVILK",
        "outputId": "7807c5ba-3294-4440-88ae-9790361b4abb"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(152, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyekNfKIXkll",
        "outputId": "dd5e9b02-6c0a-4ebb-976b-00bc057fc834"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(354,)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x_train[:,12],y_train)\n",
        "plt.scatter(x_test[:,12],y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "q3vUS9I0YMqr",
        "outputId": "4d67f9fa-32f2-479c-e30f-0eecfe0f53f8"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x78081fed0d50>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=MLPRegressor(hidden_layer_sizes=(150,20), activation='logistic', solver='adam', random_state=1, max_iter=5000,verbose=True)\n",
        "model.fit(x_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fyO0USURa3U_",
        "outputId": "6b821bee-20c4-409a-910c-ec7c006bcaba"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 303.79135746\n",
            "Iteration 2, loss = 300.75540414\n",
            "Iteration 3, loss = 297.82107626\n",
            "Iteration 4, loss = 294.82073492\n",
            "Iteration 5, loss = 292.00072727\n",
            "Iteration 6, loss = 289.30113417\n",
            "Iteration 7, loss = 286.68983716\n",
            "Iteration 8, loss = 284.23131680\n",
            "Iteration 9, loss = 281.91586335\n",
            "Iteration 10, loss = 279.75654850\n",
            "Iteration 11, loss = 277.71074972\n",
            "Iteration 12, loss = 275.80350461\n",
            "Iteration 13, loss = 274.06902117\n",
            "Iteration 14, loss = 272.41641867\n",
            "Iteration 15, loss = 270.90366238\n",
            "Iteration 16, loss = 269.47692259\n",
            "Iteration 17, loss = 268.13007060\n",
            "Iteration 18, loss = 266.88311183\n",
            "Iteration 19, loss = 265.70022172\n",
            "Iteration 20, loss = 264.59878113\n",
            "Iteration 21, loss = 263.52848745\n",
            "Iteration 22, loss = 262.51106073\n",
            "Iteration 23, loss = 261.54519242\n",
            "Iteration 24, loss = 260.60944208\n",
            "Iteration 25, loss = 259.68995330\n",
            "Iteration 26, loss = 258.82117309\n",
            "Iteration 27, loss = 257.95491633\n",
            "Iteration 28, loss = 257.12931219\n",
            "Iteration 29, loss = 256.31162757\n",
            "Iteration 30, loss = 255.51144300\n",
            "Iteration 31, loss = 254.72230245\n",
            "Iteration 32, loss = 253.95860361\n",
            "Iteration 33, loss = 253.18804512\n",
            "Iteration 34, loss = 252.44032938\n",
            "Iteration 35, loss = 251.70458125\n",
            "Iteration 36, loss = 250.98174907\n",
            "Iteration 37, loss = 250.25428846\n",
            "Iteration 38, loss = 249.53285662\n",
            "Iteration 39, loss = 248.83066294\n",
            "Iteration 40, loss = 248.13019796\n",
            "Iteration 41, loss = 247.43765340\n",
            "Iteration 42, loss = 246.74029172\n",
            "Iteration 43, loss = 246.05584695\n",
            "Iteration 44, loss = 245.37925039\n",
            "Iteration 45, loss = 244.70526466\n",
            "Iteration 46, loss = 244.03773931\n",
            "Iteration 47, loss = 243.37296617\n",
            "Iteration 48, loss = 242.70811804\n",
            "Iteration 49, loss = 242.05660947\n",
            "Iteration 50, loss = 241.40160535\n",
            "Iteration 51, loss = 240.75441892\n",
            "Iteration 52, loss = 240.09815163\n",
            "Iteration 53, loss = 239.45979930\n",
            "Iteration 54, loss = 238.82169285\n",
            "Iteration 55, loss = 238.18564019\n",
            "Iteration 56, loss = 237.55290178\n",
            "Iteration 57, loss = 236.92190511\n",
            "Iteration 58, loss = 236.30190801\n",
            "Iteration 59, loss = 235.66494125\n",
            "Iteration 60, loss = 235.05038457\n",
            "Iteration 61, loss = 234.43203875\n",
            "Iteration 62, loss = 233.80766086\n",
            "Iteration 63, loss = 233.20523655\n",
            "Iteration 64, loss = 232.58943100\n",
            "Iteration 65, loss = 231.98269731\n",
            "Iteration 66, loss = 231.37921037\n",
            "Iteration 67, loss = 230.77173412\n",
            "Iteration 68, loss = 230.16611163\n",
            "Iteration 69, loss = 229.57278126\n",
            "Iteration 70, loss = 228.97318370\n",
            "Iteration 71, loss = 228.37549595\n",
            "Iteration 72, loss = 227.78865158\n",
            "Iteration 73, loss = 227.20014255\n",
            "Iteration 74, loss = 226.60067601\n",
            "Iteration 75, loss = 226.02557872\n",
            "Iteration 76, loss = 225.44015997\n",
            "Iteration 77, loss = 224.84994624\n",
            "Iteration 78, loss = 224.27494987\n",
            "Iteration 79, loss = 223.69530894\n",
            "Iteration 80, loss = 223.11767575\n",
            "Iteration 81, loss = 222.54409519\n",
            "Iteration 82, loss = 221.97094590\n",
            "Iteration 83, loss = 221.40701342\n",
            "Iteration 84, loss = 220.82828539\n",
            "Iteration 85, loss = 220.26187180\n",
            "Iteration 86, loss = 219.70483930\n",
            "Iteration 87, loss = 219.13710489\n",
            "Iteration 88, loss = 218.57749128\n",
            "Iteration 89, loss = 218.01565901\n",
            "Iteration 90, loss = 217.45411189\n",
            "Iteration 91, loss = 216.89496631\n",
            "Iteration 92, loss = 216.35255903\n",
            "Iteration 93, loss = 215.79643947\n",
            "Iteration 94, loss = 215.25227479\n",
            "Iteration 95, loss = 214.69067241\n",
            "Iteration 96, loss = 214.14980711\n",
            "Iteration 97, loss = 213.59992322\n",
            "Iteration 98, loss = 213.05771405\n",
            "Iteration 99, loss = 212.51621830\n",
            "Iteration 100, loss = 211.97711291\n",
            "Iteration 101, loss = 211.43847672\n",
            "Iteration 102, loss = 210.89426720\n",
            "Iteration 103, loss = 210.36458231\n",
            "Iteration 104, loss = 209.82889983\n",
            "Iteration 105, loss = 209.29544479\n",
            "Iteration 106, loss = 208.76977286\n",
            "Iteration 107, loss = 208.23385306\n",
            "Iteration 108, loss = 207.70816619\n",
            "Iteration 109, loss = 207.17247002\n",
            "Iteration 110, loss = 206.65988344\n",
            "Iteration 111, loss = 206.13422106\n",
            "Iteration 112, loss = 205.61270755\n",
            "Iteration 113, loss = 205.08767039\n",
            "Iteration 114, loss = 204.56818750\n",
            "Iteration 115, loss = 204.05592881\n",
            "Iteration 116, loss = 203.54092821\n",
            "Iteration 117, loss = 203.02162051\n",
            "Iteration 118, loss = 202.51323185\n",
            "Iteration 119, loss = 201.99746519\n",
            "Iteration 120, loss = 201.48643126\n",
            "Iteration 121, loss = 200.98059954\n",
            "Iteration 122, loss = 200.47022205\n",
            "Iteration 123, loss = 199.96630963\n",
            "Iteration 124, loss = 199.46792498\n",
            "Iteration 125, loss = 198.95897570\n",
            "Iteration 126, loss = 198.46670657\n",
            "Iteration 127, loss = 197.95514987\n",
            "Iteration 128, loss = 197.45562829\n",
            "Iteration 129, loss = 196.96103275\n",
            "Iteration 130, loss = 196.46694445\n",
            "Iteration 131, loss = 195.97310761\n",
            "Iteration 132, loss = 195.47704239\n",
            "Iteration 133, loss = 194.98205830\n",
            "Iteration 134, loss = 194.49814617\n",
            "Iteration 135, loss = 194.00291216\n",
            "Iteration 136, loss = 193.52296574\n",
            "Iteration 137, loss = 193.02902589\n",
            "Iteration 138, loss = 192.54127772\n",
            "Iteration 139, loss = 192.05324270\n",
            "Iteration 140, loss = 191.57996193\n",
            "Iteration 141, loss = 191.08991314\n",
            "Iteration 142, loss = 190.61369850\n",
            "Iteration 143, loss = 190.13414073\n",
            "Iteration 144, loss = 189.65875423\n",
            "Iteration 145, loss = 189.17449359\n",
            "Iteration 146, loss = 188.70508715\n",
            "Iteration 147, loss = 188.22292833\n",
            "Iteration 148, loss = 187.76061953\n",
            "Iteration 149, loss = 187.27731330\n",
            "Iteration 150, loss = 186.81086291\n",
            "Iteration 151, loss = 186.34834270\n",
            "Iteration 152, loss = 185.87491249\n",
            "Iteration 153, loss = 185.40655803\n",
            "Iteration 154, loss = 184.93984394\n",
            "Iteration 155, loss = 184.48596206\n",
            "Iteration 156, loss = 184.02078454\n",
            "Iteration 157, loss = 183.55226368\n",
            "Iteration 158, loss = 183.09730362\n",
            "Iteration 159, loss = 182.63982054\n",
            "Iteration 160, loss = 182.17561371\n",
            "Iteration 161, loss = 181.72641434\n",
            "Iteration 162, loss = 181.27034144\n",
            "Iteration 163, loss = 180.81372543\n",
            "Iteration 164, loss = 180.36369579\n",
            "Iteration 165, loss = 179.91036138\n",
            "Iteration 166, loss = 179.46231805\n",
            "Iteration 167, loss = 179.01697148\n",
            "Iteration 168, loss = 178.56227105\n",
            "Iteration 169, loss = 178.11759983\n",
            "Iteration 170, loss = 177.67364645\n",
            "Iteration 171, loss = 177.22718241\n",
            "Iteration 172, loss = 176.79036954\n",
            "Iteration 173, loss = 176.34388947\n",
            "Iteration 174, loss = 175.90316246\n",
            "Iteration 175, loss = 175.45964382\n",
            "Iteration 176, loss = 175.02299986\n",
            "Iteration 177, loss = 174.59032938\n",
            "Iteration 178, loss = 174.15376376\n",
            "Iteration 179, loss = 173.72024798\n",
            "Iteration 180, loss = 173.28045595\n",
            "Iteration 181, loss = 172.84905233\n",
            "Iteration 182, loss = 172.41262606\n",
            "Iteration 183, loss = 171.99136865\n",
            "Iteration 184, loss = 171.55982165\n",
            "Iteration 185, loss = 171.13372859\n",
            "Iteration 186, loss = 170.70410462\n",
            "Iteration 187, loss = 170.27746066\n",
            "Iteration 188, loss = 169.85105708\n",
            "Iteration 189, loss = 169.42992114\n",
            "Iteration 190, loss = 169.00941347\n",
            "Iteration 191, loss = 168.58729355\n",
            "Iteration 192, loss = 168.16330323\n",
            "Iteration 193, loss = 167.74363270\n",
            "Iteration 194, loss = 167.33305619\n",
            "Iteration 195, loss = 166.91216567\n",
            "Iteration 196, loss = 166.49580247\n",
            "Iteration 197, loss = 166.08532225\n",
            "Iteration 198, loss = 165.66768384\n",
            "Iteration 199, loss = 165.25592411\n",
            "Iteration 200, loss = 164.84586345\n",
            "Iteration 201, loss = 164.43727540\n",
            "Iteration 202, loss = 164.03199829\n",
            "Iteration 203, loss = 163.61875951\n",
            "Iteration 204, loss = 163.21231639\n",
            "Iteration 205, loss = 162.80376251\n",
            "Iteration 206, loss = 162.40389492\n",
            "Iteration 207, loss = 161.99835520\n",
            "Iteration 208, loss = 161.60145979\n",
            "Iteration 209, loss = 161.19833975\n",
            "Iteration 210, loss = 160.79675941\n",
            "Iteration 211, loss = 160.39919853\n",
            "Iteration 212, loss = 159.99751855\n",
            "Iteration 213, loss = 159.60618133\n",
            "Iteration 214, loss = 159.20873573\n",
            "Iteration 215, loss = 158.81219476\n",
            "Iteration 216, loss = 158.42059664\n",
            "Iteration 217, loss = 158.02036835\n",
            "Iteration 218, loss = 157.63454195\n",
            "Iteration 219, loss = 157.24907407\n",
            "Iteration 220, loss = 156.85448918\n",
            "Iteration 221, loss = 156.46956052\n",
            "Iteration 222, loss = 156.08282858\n",
            "Iteration 223, loss = 155.69111215\n",
            "Iteration 224, loss = 155.30699052\n",
            "Iteration 225, loss = 154.92730654\n",
            "Iteration 226, loss = 154.54290776\n",
            "Iteration 227, loss = 154.16050546\n",
            "Iteration 228, loss = 153.77129582\n",
            "Iteration 229, loss = 153.39775177\n",
            "Iteration 230, loss = 153.02083839\n",
            "Iteration 231, loss = 152.64104568\n",
            "Iteration 232, loss = 152.26156931\n",
            "Iteration 233, loss = 151.88974716\n",
            "Iteration 234, loss = 151.51619384\n",
            "Iteration 235, loss = 151.13411746\n",
            "Iteration 236, loss = 150.76709232\n",
            "Iteration 237, loss = 150.39237833\n",
            "Iteration 238, loss = 150.02144872\n",
            "Iteration 239, loss = 149.65477318\n",
            "Iteration 240, loss = 149.28753385\n",
            "Iteration 241, loss = 148.91315006\n",
            "Iteration 242, loss = 148.55079080\n",
            "Iteration 243, loss = 148.18523726\n",
            "Iteration 244, loss = 147.81927025\n",
            "Iteration 245, loss = 147.45357605\n",
            "Iteration 246, loss = 147.09823908\n",
            "Iteration 247, loss = 146.72994235\n",
            "Iteration 248, loss = 146.37205741\n",
            "Iteration 249, loss = 146.01531076\n",
            "Iteration 250, loss = 145.65295803\n",
            "Iteration 251, loss = 145.29729808\n",
            "Iteration 252, loss = 144.93786982\n",
            "Iteration 253, loss = 144.58354167\n",
            "Iteration 254, loss = 144.22320884\n",
            "Iteration 255, loss = 143.87451941\n",
            "Iteration 256, loss = 143.52018304\n",
            "Iteration 257, loss = 143.17396646\n",
            "Iteration 258, loss = 142.81364071\n",
            "Iteration 259, loss = 142.46646638\n",
            "Iteration 260, loss = 142.11959169\n",
            "Iteration 261, loss = 141.77159535\n",
            "Iteration 262, loss = 141.42063504\n",
            "Iteration 263, loss = 141.07580626\n",
            "Iteration 264, loss = 140.72956613\n",
            "Iteration 265, loss = 140.38193825\n",
            "Iteration 266, loss = 140.04603570\n",
            "Iteration 267, loss = 139.69503055\n",
            "Iteration 268, loss = 139.35415529\n",
            "Iteration 269, loss = 139.02443342\n",
            "Iteration 270, loss = 138.67202905\n",
            "Iteration 271, loss = 138.33404822\n",
            "Iteration 272, loss = 138.00346671\n",
            "Iteration 273, loss = 137.66603360\n",
            "Iteration 274, loss = 137.32591739\n",
            "Iteration 275, loss = 136.99527076\n",
            "Iteration 276, loss = 136.65557364\n",
            "Iteration 277, loss = 136.32596591\n",
            "Iteration 278, loss = 135.99216716\n",
            "Iteration 279, loss = 135.66406101\n",
            "Iteration 280, loss = 135.33033767\n",
            "Iteration 281, loss = 134.99958801\n",
            "Iteration 282, loss = 134.66705486\n",
            "Iteration 283, loss = 134.34528661\n",
            "Iteration 284, loss = 134.02014499\n",
            "Iteration 285, loss = 133.69248304\n",
            "Iteration 286, loss = 133.36675481\n",
            "Iteration 287, loss = 133.03963058\n",
            "Iteration 288, loss = 132.71599868\n",
            "Iteration 289, loss = 132.39258791\n",
            "Iteration 290, loss = 132.07340548\n",
            "Iteration 291, loss = 131.75540090\n",
            "Iteration 292, loss = 131.43183729\n",
            "Iteration 293, loss = 131.11610963\n",
            "Iteration 294, loss = 130.79161609\n",
            "Iteration 295, loss = 130.47577984\n",
            "Iteration 296, loss = 130.16133997\n",
            "Iteration 297, loss = 129.84805132\n",
            "Iteration 298, loss = 129.53071778\n",
            "Iteration 299, loss = 129.21950790\n",
            "Iteration 300, loss = 128.90627168\n",
            "Iteration 301, loss = 128.59312826\n",
            "Iteration 302, loss = 128.28537520\n",
            "Iteration 303, loss = 127.97075633\n",
            "Iteration 304, loss = 127.66411911\n",
            "Iteration 305, loss = 127.35502843\n",
            "Iteration 306, loss = 127.05294667\n",
            "Iteration 307, loss = 126.74334539\n",
            "Iteration 308, loss = 126.43845742\n",
            "Iteration 309, loss = 126.13086567\n",
            "Iteration 310, loss = 125.82977457\n",
            "Iteration 311, loss = 125.52896795\n",
            "Iteration 312, loss = 125.22685532\n",
            "Iteration 313, loss = 124.92278277\n",
            "Iteration 314, loss = 124.62448448\n",
            "Iteration 315, loss = 124.32182238\n",
            "Iteration 316, loss = 124.02559374\n",
            "Iteration 317, loss = 123.73107383\n",
            "Iteration 318, loss = 123.42915701\n",
            "Iteration 319, loss = 123.13304682\n",
            "Iteration 320, loss = 122.83876584\n",
            "Iteration 321, loss = 122.54539775\n",
            "Iteration 322, loss = 122.25736638\n",
            "Iteration 323, loss = 121.95558016\n",
            "Iteration 324, loss = 121.66872992\n",
            "Iteration 325, loss = 121.37439914\n",
            "Iteration 326, loss = 121.08977877\n",
            "Iteration 327, loss = 120.79473200\n",
            "Iteration 328, loss = 120.50972434\n",
            "Iteration 329, loss = 120.21780545\n",
            "Iteration 330, loss = 119.93670924\n",
            "Iteration 331, loss = 119.64841140\n",
            "Iteration 332, loss = 119.36329225\n",
            "Iteration 333, loss = 119.08304587\n",
            "Iteration 334, loss = 118.79535879\n",
            "Iteration 335, loss = 118.51563408\n",
            "Iteration 336, loss = 118.23375770\n",
            "Iteration 337, loss = 117.94809839\n",
            "Iteration 338, loss = 117.67251763\n",
            "Iteration 339, loss = 117.39865049\n",
            "Iteration 340, loss = 117.11558733\n",
            "Iteration 341, loss = 116.83893281\n",
            "Iteration 342, loss = 116.56217008\n",
            "Iteration 343, loss = 116.28333057\n",
            "Iteration 344, loss = 116.01292864\n",
            "Iteration 345, loss = 115.74265254\n",
            "Iteration 346, loss = 115.46741047\n",
            "Iteration 347, loss = 115.18984118\n",
            "Iteration 348, loss = 114.91798825\n",
            "Iteration 349, loss = 114.65207677\n",
            "Iteration 350, loss = 114.38635922\n",
            "Iteration 351, loss = 114.11491792\n",
            "Iteration 352, loss = 113.84377583\n",
            "Iteration 353, loss = 113.57561831\n",
            "Iteration 354, loss = 113.31056219\n",
            "Iteration 355, loss = 113.04223414\n",
            "Iteration 356, loss = 112.78202057\n",
            "Iteration 357, loss = 112.51381967\n",
            "Iteration 358, loss = 112.25447220\n",
            "Iteration 359, loss = 111.98807371\n",
            "Iteration 360, loss = 111.72742368\n",
            "Iteration 361, loss = 111.46392320\n",
            "Iteration 362, loss = 111.19906269\n",
            "Iteration 363, loss = 110.94676974\n",
            "Iteration 364, loss = 110.68797086\n",
            "Iteration 365, loss = 110.42803099\n",
            "Iteration 366, loss = 110.16872179\n",
            "Iteration 367, loss = 109.90709139\n",
            "Iteration 368, loss = 109.66166754\n",
            "Iteration 369, loss = 109.40011763\n",
            "Iteration 370, loss = 109.14673321\n",
            "Iteration 371, loss = 108.89280966\n",
            "Iteration 372, loss = 108.63864191\n",
            "Iteration 373, loss = 108.38863227\n",
            "Iteration 374, loss = 108.13533803\n",
            "Iteration 375, loss = 107.88114576\n",
            "Iteration 376, loss = 107.63315046\n",
            "Iteration 377, loss = 107.38450290\n",
            "Iteration 378, loss = 107.13810837\n",
            "Iteration 379, loss = 106.88659697\n",
            "Iteration 380, loss = 106.63696296\n",
            "Iteration 381, loss = 106.39814365\n",
            "Iteration 382, loss = 106.14547702\n",
            "Iteration 383, loss = 105.89979889\n",
            "Iteration 384, loss = 105.65612818\n",
            "Iteration 385, loss = 105.41264439\n",
            "Iteration 386, loss = 105.16549071\n",
            "Iteration 387, loss = 104.93176711\n",
            "Iteration 388, loss = 104.68179995\n",
            "Iteration 389, loss = 104.44434377\n",
            "Iteration 390, loss = 104.20249735\n",
            "Iteration 391, loss = 103.96207166\n",
            "Iteration 392, loss = 103.72308164\n",
            "Iteration 393, loss = 103.48605959\n",
            "Iteration 394, loss = 103.24771485\n",
            "Iteration 395, loss = 103.01266613\n",
            "Iteration 396, loss = 102.77312641\n",
            "Iteration 397, loss = 102.53558221\n",
            "Iteration 398, loss = 102.30855874\n",
            "Iteration 399, loss = 102.07353015\n",
            "Iteration 400, loss = 101.83769133\n",
            "Iteration 401, loss = 101.60321633\n",
            "Iteration 402, loss = 101.37327698\n",
            "Iteration 403, loss = 101.14468661\n",
            "Iteration 404, loss = 100.91027201\n",
            "Iteration 405, loss = 100.68778859\n",
            "Iteration 406, loss = 100.45474170\n",
            "Iteration 407, loss = 100.22655879\n",
            "Iteration 408, loss = 99.99835891\n",
            "Iteration 409, loss = 99.77237062\n",
            "Iteration 410, loss = 99.54950461\n",
            "Iteration 411, loss = 99.31960131\n",
            "Iteration 412, loss = 99.09352267\n",
            "Iteration 413, loss = 98.87486348\n",
            "Iteration 414, loss = 98.65057795\n",
            "Iteration 415, loss = 98.42269347\n",
            "Iteration 416, loss = 98.20432959\n",
            "Iteration 417, loss = 97.97954438\n",
            "Iteration 418, loss = 97.76006328\n",
            "Iteration 419, loss = 97.54153736\n",
            "Iteration 420, loss = 97.31981069\n",
            "Iteration 421, loss = 97.09635993\n",
            "Iteration 422, loss = 96.88252532\n",
            "Iteration 423, loss = 96.66455255\n",
            "Iteration 424, loss = 96.44343059\n",
            "Iteration 425, loss = 96.22642037\n",
            "Iteration 426, loss = 96.01151812\n",
            "Iteration 427, loss = 95.79332510\n",
            "Iteration 428, loss = 95.57908261\n",
            "Iteration 429, loss = 95.36483886\n",
            "Iteration 430, loss = 95.14872205\n",
            "Iteration 431, loss = 94.93889649\n",
            "Iteration 432, loss = 94.72182223\n",
            "Iteration 433, loss = 94.50853607\n",
            "Iteration 434, loss = 94.29886094\n",
            "Iteration 435, loss = 94.09086786\n",
            "Iteration 436, loss = 93.87379303\n",
            "Iteration 437, loss = 93.66539905\n",
            "Iteration 438, loss = 93.45532745\n",
            "Iteration 439, loss = 93.24809141\n",
            "Iteration 440, loss = 93.03764654\n",
            "Iteration 441, loss = 92.83371636\n",
            "Iteration 442, loss = 92.62096535\n",
            "Iteration 443, loss = 92.41379642\n",
            "Iteration 444, loss = 92.21459189\n",
            "Iteration 445, loss = 92.00212958\n",
            "Iteration 446, loss = 91.79834685\n",
            "Iteration 447, loss = 91.58864466\n",
            "Iteration 448, loss = 91.39011151\n",
            "Iteration 449, loss = 91.18517841\n",
            "Iteration 450, loss = 90.98164412\n",
            "Iteration 451, loss = 90.77554105\n",
            "Iteration 452, loss = 90.57449007\n",
            "Iteration 453, loss = 90.37158117\n",
            "Iteration 454, loss = 90.16727949\n",
            "Iteration 455, loss = 89.96474401\n",
            "Iteration 456, loss = 89.76750388\n",
            "Iteration 457, loss = 89.56048436\n",
            "Iteration 458, loss = 89.36353121\n",
            "Iteration 459, loss = 89.16294816\n",
            "Iteration 460, loss = 88.96250994\n",
            "Iteration 461, loss = 88.76024677\n",
            "Iteration 462, loss = 88.55980415\n",
            "Iteration 463, loss = 88.36245032\n",
            "Iteration 464, loss = 88.15901593\n",
            "Iteration 465, loss = 87.96506615\n",
            "Iteration 466, loss = 87.76179118\n",
            "Iteration 467, loss = 87.56717162\n",
            "Iteration 468, loss = 87.37063968\n",
            "Iteration 469, loss = 87.17271838\n",
            "Iteration 470, loss = 86.97499072\n",
            "Iteration 471, loss = 86.77810513\n",
            "Iteration 472, loss = 86.58586746\n",
            "Iteration 473, loss = 86.38961188\n",
            "Iteration 474, loss = 86.19177824\n",
            "Iteration 475, loss = 86.00019736\n",
            "Iteration 476, loss = 85.80686269\n",
            "Iteration 477, loss = 85.61360309\n",
            "Iteration 478, loss = 85.41894076\n",
            "Iteration 479, loss = 85.22782033\n",
            "Iteration 480, loss = 85.03893192\n",
            "Iteration 481, loss = 84.84494691\n",
            "Iteration 482, loss = 84.65750761\n",
            "Iteration 483, loss = 84.46133407\n",
            "Iteration 484, loss = 84.27634853\n",
            "Iteration 485, loss = 84.08769271\n",
            "Iteration 486, loss = 83.90220181\n",
            "Iteration 487, loss = 83.71186744\n",
            "Iteration 488, loss = 83.52349958\n",
            "Iteration 489, loss = 83.33554218\n",
            "Iteration 490, loss = 83.15257404\n",
            "Iteration 491, loss = 82.96447815\n",
            "Iteration 492, loss = 82.78048609\n",
            "Iteration 493, loss = 82.59963578\n",
            "Iteration 494, loss = 82.40851028\n",
            "Iteration 495, loss = 82.22395378\n",
            "Iteration 496, loss = 82.04462537\n",
            "Iteration 497, loss = 81.85937840\n",
            "Iteration 498, loss = 81.68388865\n",
            "Iteration 499, loss = 81.49939820\n",
            "Iteration 500, loss = 81.31767975\n",
            "Iteration 501, loss = 81.13545415\n",
            "Iteration 502, loss = 80.95534280\n",
            "Iteration 503, loss = 80.77706997\n",
            "Iteration 504, loss = 80.59636122\n",
            "Iteration 505, loss = 80.41869470\n",
            "Iteration 506, loss = 80.24368735\n",
            "Iteration 507, loss = 80.06327760\n",
            "Iteration 508, loss = 79.88477657\n",
            "Iteration 509, loss = 79.70476240\n",
            "Iteration 510, loss = 79.53185324\n",
            "Iteration 511, loss = 79.35957148\n",
            "Iteration 512, loss = 79.18096830\n",
            "Iteration 513, loss = 79.00394149\n",
            "Iteration 514, loss = 78.83146224\n",
            "Iteration 515, loss = 78.65603910\n",
            "Iteration 516, loss = 78.48387371\n",
            "Iteration 517, loss = 78.31204679\n",
            "Iteration 518, loss = 78.13417450\n",
            "Iteration 519, loss = 77.96630940\n",
            "Iteration 520, loss = 77.79341824\n",
            "Iteration 521, loss = 77.62435060\n",
            "Iteration 522, loss = 77.45258681\n",
            "Iteration 523, loss = 77.27955535\n",
            "Iteration 524, loss = 77.10666130\n",
            "Iteration 525, loss = 76.94256617\n",
            "Iteration 526, loss = 76.77300367\n",
            "Iteration 527, loss = 76.60416916\n",
            "Iteration 528, loss = 76.43343866\n",
            "Iteration 529, loss = 76.26998973\n",
            "Iteration 530, loss = 76.10146706\n",
            "Iteration 531, loss = 75.93626390\n",
            "Iteration 532, loss = 75.76802137\n",
            "Iteration 533, loss = 75.60051850\n",
            "Iteration 534, loss = 75.43881341\n",
            "Iteration 535, loss = 75.27226626\n",
            "Iteration 536, loss = 75.11056054\n",
            "Iteration 537, loss = 74.94463282\n",
            "Iteration 538, loss = 74.78101560\n",
            "Iteration 539, loss = 74.61645979\n",
            "Iteration 540, loss = 74.45866382\n",
            "Iteration 541, loss = 74.29768594\n",
            "Iteration 542, loss = 74.13111537\n",
            "Iteration 543, loss = 73.97395275\n",
            "Iteration 544, loss = 73.81462883\n",
            "Iteration 545, loss = 73.65312193\n",
            "Iteration 546, loss = 73.48940118\n",
            "Iteration 547, loss = 73.33214347\n",
            "Iteration 548, loss = 73.17815596\n",
            "Iteration 549, loss = 73.01467307\n",
            "Iteration 550, loss = 72.86272877\n",
            "Iteration 551, loss = 72.70261099\n",
            "Iteration 552, loss = 72.54869323\n",
            "Iteration 553, loss = 72.38693147\n",
            "Iteration 554, loss = 72.23052901\n",
            "Iteration 555, loss = 72.07641976\n",
            "Iteration 556, loss = 71.91924182\n",
            "Iteration 557, loss = 71.76722789\n",
            "Iteration 558, loss = 71.61096360\n",
            "Iteration 559, loss = 71.46055783\n",
            "Iteration 560, loss = 71.30075944\n",
            "Iteration 561, loss = 71.15185536\n",
            "Iteration 562, loss = 71.00208594\n",
            "Iteration 563, loss = 70.84513674\n",
            "Iteration 564, loss = 70.69597691\n",
            "Iteration 565, loss = 70.54223113\n",
            "Iteration 566, loss = 70.39046532\n",
            "Iteration 567, loss = 70.23898726\n",
            "Iteration 568, loss = 70.09076883\n",
            "Iteration 569, loss = 69.94284310\n",
            "Iteration 570, loss = 69.79530110\n",
            "Iteration 571, loss = 69.64098256\n",
            "Iteration 572, loss = 69.49686462\n",
            "Iteration 573, loss = 69.34836046\n",
            "Iteration 574, loss = 69.20061015\n",
            "Iteration 575, loss = 69.05049953\n",
            "Iteration 576, loss = 68.90419699\n",
            "Iteration 577, loss = 68.76166421\n",
            "Iteration 578, loss = 68.61225449\n",
            "Iteration 579, loss = 68.46980031\n",
            "Iteration 580, loss = 68.32184375\n",
            "Iteration 581, loss = 68.17811514\n",
            "Iteration 582, loss = 68.03411573\n",
            "Iteration 583, loss = 67.89059105\n",
            "Iteration 584, loss = 67.74254563\n",
            "Iteration 585, loss = 67.60599996\n",
            "Iteration 586, loss = 67.46018174\n",
            "Iteration 587, loss = 67.32048788\n",
            "Iteration 588, loss = 67.17216552\n",
            "Iteration 589, loss = 67.03243433\n",
            "Iteration 590, loss = 66.89204349\n",
            "Iteration 591, loss = 66.75264070\n",
            "Iteration 592, loss = 66.61416914\n",
            "Iteration 593, loss = 66.47445508\n",
            "Iteration 594, loss = 66.32973549\n",
            "Iteration 595, loss = 66.19434195\n",
            "Iteration 596, loss = 66.05554450\n",
            "Iteration 597, loss = 65.91758009\n",
            "Iteration 598, loss = 65.78132551\n",
            "Iteration 599, loss = 65.64441548\n",
            "Iteration 600, loss = 65.50885577\n",
            "Iteration 601, loss = 65.36943011\n",
            "Iteration 602, loss = 65.23706763\n",
            "Iteration 603, loss = 65.09669385\n",
            "Iteration 604, loss = 64.96242715\n",
            "Iteration 605, loss = 64.83183338\n",
            "Iteration 606, loss = 64.69621741\n",
            "Iteration 607, loss = 64.56062393\n",
            "Iteration 608, loss = 64.42890545\n",
            "Iteration 609, loss = 64.29628690\n",
            "Iteration 610, loss = 64.16477894\n",
            "Iteration 611, loss = 64.03214344\n",
            "Iteration 612, loss = 63.89779153\n",
            "Iteration 613, loss = 63.76529788\n",
            "Iteration 614, loss = 63.63733965\n",
            "Iteration 615, loss = 63.50633902\n",
            "Iteration 616, loss = 63.37347370\n",
            "Iteration 617, loss = 63.24943529\n",
            "Iteration 618, loss = 63.11404724\n",
            "Iteration 619, loss = 62.98689634\n",
            "Iteration 620, loss = 62.85524765\n",
            "Iteration 621, loss = 62.73005922\n",
            "Iteration 622, loss = 62.60053240\n",
            "Iteration 623, loss = 62.47216255\n",
            "Iteration 624, loss = 62.34532602\n",
            "Iteration 625, loss = 62.22003984\n",
            "Iteration 626, loss = 62.08931176\n",
            "Iteration 627, loss = 61.96338838\n",
            "Iteration 628, loss = 61.83679718\n",
            "Iteration 629, loss = 61.71144006\n",
            "Iteration 630, loss = 61.58454791\n",
            "Iteration 631, loss = 61.46074249\n",
            "Iteration 632, loss = 61.33696128\n",
            "Iteration 633, loss = 61.21029355\n",
            "Iteration 634, loss = 61.08624841\n",
            "Iteration 635, loss = 60.96455339\n",
            "Iteration 636, loss = 60.84107317\n",
            "Iteration 637, loss = 60.71530349\n",
            "Iteration 638, loss = 60.59447618\n",
            "Iteration 639, loss = 60.47174614\n",
            "Iteration 640, loss = 60.34906377\n",
            "Iteration 641, loss = 60.22669683\n",
            "Iteration 642, loss = 60.11000731\n",
            "Iteration 643, loss = 59.98772105\n",
            "Iteration 644, loss = 59.87037432\n",
            "Iteration 645, loss = 59.74643703\n",
            "Iteration 646, loss = 59.62607894\n",
            "Iteration 647, loss = 59.51116127\n",
            "Iteration 648, loss = 59.38958794\n",
            "Iteration 649, loss = 59.27108787\n",
            "Iteration 650, loss = 59.14964685\n",
            "Iteration 651, loss = 59.03950705\n",
            "Iteration 652, loss = 58.91631984\n",
            "Iteration 653, loss = 58.80097976\n",
            "Iteration 654, loss = 58.68400364\n",
            "Iteration 655, loss = 58.56674289\n",
            "Iteration 656, loss = 58.45304516\n",
            "Iteration 657, loss = 58.33428042\n",
            "Iteration 658, loss = 58.21902798\n",
            "Iteration 659, loss = 58.10336855\n",
            "Iteration 660, loss = 57.98789496\n",
            "Iteration 661, loss = 57.87954317\n",
            "Iteration 662, loss = 57.76052774\n",
            "Iteration 663, loss = 57.64570349\n",
            "Iteration 664, loss = 57.53590366\n",
            "Iteration 665, loss = 57.41914155\n",
            "Iteration 666, loss = 57.30711234\n",
            "Iteration 667, loss = 57.19537905\n",
            "Iteration 668, loss = 57.08618637\n",
            "Iteration 669, loss = 56.97482437\n",
            "Iteration 670, loss = 56.85981619\n",
            "Iteration 671, loss = 56.74986659\n",
            "Iteration 672, loss = 56.63827977\n",
            "Iteration 673, loss = 56.52812485\n",
            "Iteration 674, loss = 56.41744550\n",
            "Iteration 675, loss = 56.30686030\n",
            "Iteration 676, loss = 56.20137340\n",
            "Iteration 677, loss = 56.08844582\n",
            "Iteration 678, loss = 55.97865227\n",
            "Iteration 679, loss = 55.87567974\n",
            "Iteration 680, loss = 55.76320303\n",
            "Iteration 681, loss = 55.65134453\n",
            "Iteration 682, loss = 55.54928122\n",
            "Iteration 683, loss = 55.43979933\n",
            "Iteration 684, loss = 55.33269129\n",
            "Iteration 685, loss = 55.22675693\n",
            "Iteration 686, loss = 55.12378576\n",
            "Iteration 687, loss = 55.01212812\n",
            "Iteration 688, loss = 54.91030013\n",
            "Iteration 689, loss = 54.80416196\n",
            "Iteration 690, loss = 54.69973628\n",
            "Iteration 691, loss = 54.59294043\n",
            "Iteration 692, loss = 54.48847321\n",
            "Iteration 693, loss = 54.38489559\n",
            "Iteration 694, loss = 54.28449234\n",
            "Iteration 695, loss = 54.17969905\n",
            "Iteration 696, loss = 54.07616407\n",
            "Iteration 697, loss = 53.97308465\n",
            "Iteration 698, loss = 53.86809120\n",
            "Iteration 699, loss = 53.76771787\n",
            "Iteration 700, loss = 53.66822773\n",
            "Iteration 701, loss = 53.56498065\n",
            "Iteration 702, loss = 53.46353934\n",
            "Iteration 703, loss = 53.36140963\n",
            "Iteration 704, loss = 53.26109251\n",
            "Iteration 705, loss = 53.15906504\n",
            "Iteration 706, loss = 53.06221675\n",
            "Iteration 707, loss = 52.96065252\n",
            "Iteration 708, loss = 52.86156223\n",
            "Iteration 709, loss = 52.76089632\n",
            "Iteration 710, loss = 52.66364727\n",
            "Iteration 711, loss = 52.56340915\n",
            "Iteration 712, loss = 52.46547338\n",
            "Iteration 713, loss = 52.36808390\n",
            "Iteration 714, loss = 52.26920198\n",
            "Iteration 715, loss = 52.17135749\n",
            "Iteration 716, loss = 52.07343365\n",
            "Iteration 717, loss = 51.97803027\n",
            "Iteration 718, loss = 51.88148265\n",
            "Iteration 719, loss = 51.78594952\n",
            "Iteration 720, loss = 51.68862365\n",
            "Iteration 721, loss = 51.59327508\n",
            "Iteration 722, loss = 51.49467760\n",
            "Iteration 723, loss = 51.40329735\n",
            "Iteration 724, loss = 51.30704416\n",
            "Iteration 725, loss = 51.20970907\n",
            "Iteration 726, loss = 51.11832731\n",
            "Iteration 727, loss = 51.02595699\n",
            "Iteration 728, loss = 50.92852831\n",
            "Iteration 729, loss = 50.83431959\n",
            "Iteration 730, loss = 50.74286115\n",
            "Iteration 731, loss = 50.64969552\n",
            "Iteration 732, loss = 50.55659809\n",
            "Iteration 733, loss = 50.46465889\n",
            "Iteration 734, loss = 50.37375473\n",
            "Iteration 735, loss = 50.28013530\n",
            "Iteration 736, loss = 50.19175978\n",
            "Iteration 737, loss = 50.10047238\n",
            "Iteration 738, loss = 50.00888610\n",
            "Iteration 739, loss = 49.91726205\n",
            "Iteration 740, loss = 49.82500029\n",
            "Iteration 741, loss = 49.73475136\n",
            "Iteration 742, loss = 49.64834058\n",
            "Iteration 743, loss = 49.55893249\n",
            "Iteration 744, loss = 49.46893078\n",
            "Iteration 745, loss = 49.38120821\n",
            "Iteration 746, loss = 49.29401043\n",
            "Iteration 747, loss = 49.20092182\n",
            "Iteration 748, loss = 49.11440235\n",
            "Iteration 749, loss = 49.02702909\n",
            "Iteration 750, loss = 48.93905749\n",
            "Iteration 751, loss = 48.85224739\n",
            "Iteration 752, loss = 48.76506287\n",
            "Iteration 753, loss = 48.68076133\n",
            "Iteration 754, loss = 48.59471123\n",
            "Iteration 755, loss = 48.50531896\n",
            "Iteration 756, loss = 48.42004391\n",
            "Iteration 757, loss = 48.33342151\n",
            "Iteration 758, loss = 48.24997163\n",
            "Iteration 759, loss = 48.16530382\n",
            "Iteration 760, loss = 48.07994393\n",
            "Iteration 761, loss = 47.99385417\n",
            "Iteration 762, loss = 47.91122396\n",
            "Iteration 763, loss = 47.82424414\n",
            "Iteration 764, loss = 47.74038618\n",
            "Iteration 765, loss = 47.65881034\n",
            "Iteration 766, loss = 47.57532116\n",
            "Iteration 767, loss = 47.48929744\n",
            "Iteration 768, loss = 47.40859821\n",
            "Iteration 769, loss = 47.32522774\n",
            "Iteration 770, loss = 47.24308214\n",
            "Iteration 771, loss = 47.15613515\n",
            "Iteration 772, loss = 47.07845075\n",
            "Iteration 773, loss = 46.99733669\n",
            "Iteration 774, loss = 46.91074285\n",
            "Iteration 775, loss = 46.83428498\n",
            "Iteration 776, loss = 46.74654579\n",
            "Iteration 777, loss = 46.66917533\n",
            "Iteration 778, loss = 46.59005006\n",
            "Iteration 779, loss = 46.50765455\n",
            "Iteration 780, loss = 46.42716933\n",
            "Iteration 781, loss = 46.35074934\n",
            "Iteration 782, loss = 46.26769552\n",
            "Iteration 783, loss = 46.19095841\n",
            "Iteration 784, loss = 46.10616689\n",
            "Iteration 785, loss = 46.03160087\n",
            "Iteration 786, loss = 45.95432299\n",
            "Iteration 787, loss = 45.87193576\n",
            "Iteration 788, loss = 45.79413159\n",
            "Iteration 789, loss = 45.72012174\n",
            "Iteration 790, loss = 45.63966427\n",
            "Iteration 791, loss = 45.56100052\n",
            "Iteration 792, loss = 45.48542212\n",
            "Iteration 793, loss = 45.40768535\n",
            "Iteration 794, loss = 45.33167852\n",
            "Iteration 795, loss = 45.25230076\n",
            "Iteration 796, loss = 45.17762704\n",
            "Iteration 797, loss = 45.10087849\n",
            "Iteration 798, loss = 45.02383670\n",
            "Iteration 799, loss = 44.95077524\n",
            "Iteration 800, loss = 44.87282215\n",
            "Iteration 801, loss = 44.79720066\n",
            "Iteration 802, loss = 44.72172732\n",
            "Iteration 803, loss = 44.64778804\n",
            "Iteration 804, loss = 44.57066157\n",
            "Iteration 805, loss = 44.49906212\n",
            "Iteration 806, loss = 44.42352114\n",
            "Iteration 807, loss = 44.34616760\n",
            "Iteration 808, loss = 44.27391036\n",
            "Iteration 809, loss = 44.19976241\n",
            "Iteration 810, loss = 44.12594531\n",
            "Iteration 811, loss = 44.05289976\n",
            "Iteration 812, loss = 43.98002335\n",
            "Iteration 813, loss = 43.90706158\n",
            "Iteration 814, loss = 43.83378173\n",
            "Iteration 815, loss = 43.75948890\n",
            "Iteration 816, loss = 43.68889883\n",
            "Iteration 817, loss = 43.61850260\n",
            "Iteration 818, loss = 43.54397206\n",
            "Iteration 819, loss = 43.47392493\n",
            "Iteration 820, loss = 43.40170673\n",
            "Iteration 821, loss = 43.33069599\n",
            "Iteration 822, loss = 43.26193080\n",
            "Iteration 823, loss = 43.19093877\n",
            "Iteration 824, loss = 43.11996292\n",
            "Iteration 825, loss = 43.04750393\n",
            "Iteration 826, loss = 42.97755215\n",
            "Iteration 827, loss = 42.91244761\n",
            "Iteration 828, loss = 42.83861270\n",
            "Iteration 829, loss = 42.77046272\n",
            "Iteration 830, loss = 42.70672756\n",
            "Iteration 831, loss = 42.63194224\n",
            "Iteration 832, loss = 42.56492199\n",
            "Iteration 833, loss = 42.49650854\n",
            "Iteration 834, loss = 42.42764032\n",
            "Iteration 835, loss = 42.36046252\n",
            "Iteration 836, loss = 42.29192867\n",
            "Iteration 837, loss = 42.22661467\n",
            "Iteration 838, loss = 42.15796344\n",
            "Iteration 839, loss = 42.09204060\n",
            "Iteration 840, loss = 42.02179453\n",
            "Iteration 841, loss = 41.95629806\n",
            "Iteration 842, loss = 41.89351472\n",
            "Iteration 843, loss = 41.82403352\n",
            "Iteration 844, loss = 41.76062455\n",
            "Iteration 845, loss = 41.69151410\n",
            "Iteration 846, loss = 41.62755445\n",
            "Iteration 847, loss = 41.56555756\n",
            "Iteration 848, loss = 41.49820739\n",
            "Iteration 849, loss = 41.43328019\n",
            "Iteration 850, loss = 41.36805608\n",
            "Iteration 851, loss = 41.30465416\n",
            "Iteration 852, loss = 41.24082524\n",
            "Iteration 853, loss = 41.17681000\n",
            "Iteration 854, loss = 41.11196139\n",
            "Iteration 855, loss = 41.04996915\n",
            "Iteration 856, loss = 40.98358578\n",
            "Iteration 857, loss = 40.92248725\n",
            "Iteration 858, loss = 40.86056398\n",
            "Iteration 859, loss = 40.79601539\n",
            "Iteration 860, loss = 40.73324855\n",
            "Iteration 861, loss = 40.67106356\n",
            "Iteration 862, loss = 40.60908480\n",
            "Iteration 863, loss = 40.54598542\n",
            "Iteration 864, loss = 40.48445157\n",
            "Iteration 865, loss = 40.42101568\n",
            "Iteration 866, loss = 40.35848387\n",
            "Iteration 867, loss = 40.29897589\n",
            "Iteration 868, loss = 40.23585186\n",
            "Iteration 869, loss = 40.17487370\n",
            "Iteration 870, loss = 40.11318632\n",
            "Iteration 871, loss = 40.05511422\n",
            "Iteration 872, loss = 39.99256131\n",
            "Iteration 873, loss = 39.93506875\n",
            "Iteration 874, loss = 39.87347447\n",
            "Iteration 875, loss = 39.81033190\n",
            "Iteration 876, loss = 39.75289102\n",
            "Iteration 877, loss = 39.69240019\n",
            "Iteration 878, loss = 39.63217310\n",
            "Iteration 879, loss = 39.57477414\n",
            "Iteration 880, loss = 39.51689729\n",
            "Iteration 881, loss = 39.45576464\n",
            "Iteration 882, loss = 39.39675151\n",
            "Iteration 883, loss = 39.33924013\n",
            "Iteration 884, loss = 39.27894189\n",
            "Iteration 885, loss = 39.22440957\n",
            "Iteration 886, loss = 39.16455488\n",
            "Iteration 887, loss = 39.10807119\n",
            "Iteration 888, loss = 39.04936969\n",
            "Iteration 889, loss = 38.99009561\n",
            "Iteration 890, loss = 38.93469220\n",
            "Iteration 891, loss = 38.87807699\n",
            "Iteration 892, loss = 38.82196791\n",
            "Iteration 893, loss = 38.76316464\n",
            "Iteration 894, loss = 38.70677754\n",
            "Iteration 895, loss = 38.64974123\n",
            "Iteration 896, loss = 38.59513966\n",
            "Iteration 897, loss = 38.54159135\n",
            "Iteration 898, loss = 38.48422753\n",
            "Iteration 899, loss = 38.42740300\n",
            "Iteration 900, loss = 38.37434497\n",
            "Iteration 901, loss = 38.31566788\n",
            "Iteration 902, loss = 38.26465356\n",
            "Iteration 903, loss = 38.20815547\n",
            "Iteration 904, loss = 38.15368761\n",
            "Iteration 905, loss = 38.09816925\n",
            "Iteration 906, loss = 38.04699482\n",
            "Iteration 907, loss = 37.99145816\n",
            "Iteration 908, loss = 37.93732732\n",
            "Iteration 909, loss = 37.88391731\n",
            "Iteration 910, loss = 37.82953346\n",
            "Iteration 911, loss = 37.77890908\n",
            "Iteration 912, loss = 37.72451702\n",
            "Iteration 913, loss = 37.67051117\n",
            "Iteration 914, loss = 37.61877279\n",
            "Iteration 915, loss = 37.56602878\n",
            "Iteration 916, loss = 37.51306731\n",
            "Iteration 917, loss = 37.46052203\n",
            "Iteration 918, loss = 37.40812537\n",
            "Iteration 919, loss = 37.35737426\n",
            "Iteration 920, loss = 37.30588423\n",
            "Iteration 921, loss = 37.25291973\n",
            "Iteration 922, loss = 37.20112911\n",
            "Iteration 923, loss = 37.15272401\n",
            "Iteration 924, loss = 37.09944468\n",
            "Iteration 925, loss = 37.05038283\n",
            "Iteration 926, loss = 36.99874196\n",
            "Iteration 927, loss = 36.94839395\n",
            "Iteration 928, loss = 36.89832273\n",
            "Iteration 929, loss = 36.84592908\n",
            "Iteration 930, loss = 36.79790641\n",
            "Iteration 931, loss = 36.74854732\n",
            "Iteration 932, loss = 36.69739328\n",
            "Iteration 933, loss = 36.64916402\n",
            "Iteration 934, loss = 36.60092113\n",
            "Iteration 935, loss = 36.55032784\n",
            "Iteration 936, loss = 36.50225182\n",
            "Iteration 937, loss = 36.45262555\n",
            "Iteration 938, loss = 36.40564954\n",
            "Iteration 939, loss = 36.35739304\n",
            "Iteration 940, loss = 36.30807592\n",
            "Iteration 941, loss = 36.25618839\n",
            "Iteration 942, loss = 36.21382713\n",
            "Iteration 943, loss = 36.16432947\n",
            "Iteration 944, loss = 36.11274665\n",
            "Iteration 945, loss = 36.06839464\n",
            "Iteration 946, loss = 36.02198081\n",
            "Iteration 947, loss = 35.97270569\n",
            "Iteration 948, loss = 35.92854699\n",
            "Iteration 949, loss = 35.88020975\n",
            "Iteration 950, loss = 35.83218802\n",
            "Iteration 951, loss = 35.78207652\n",
            "Iteration 952, loss = 35.73808522\n",
            "Iteration 953, loss = 35.69390021\n",
            "Iteration 954, loss = 35.64378463\n",
            "Iteration 955, loss = 35.60205806\n",
            "Iteration 956, loss = 35.55452992\n",
            "Iteration 957, loss = 35.50754583\n",
            "Iteration 958, loss = 35.46148699\n",
            "Iteration 959, loss = 35.41732811\n",
            "Iteration 960, loss = 35.37071318\n",
            "Iteration 961, loss = 35.32753158\n",
            "Iteration 962, loss = 35.28236789\n",
            "Iteration 963, loss = 35.23639769\n",
            "Iteration 964, loss = 35.19253709\n",
            "Iteration 965, loss = 35.14672113\n",
            "Iteration 966, loss = 35.10273742\n",
            "Iteration 967, loss = 35.05946446\n",
            "Iteration 968, loss = 35.01392648\n",
            "Iteration 969, loss = 34.97030444\n",
            "Iteration 970, loss = 34.92667372\n",
            "Iteration 971, loss = 34.88349477\n",
            "Iteration 972, loss = 34.83846719\n",
            "Iteration 973, loss = 34.79500676\n",
            "Iteration 974, loss = 34.75405228\n",
            "Iteration 975, loss = 34.70495514\n",
            "Iteration 976, loss = 34.66573714\n",
            "Iteration 977, loss = 34.62165670\n",
            "Iteration 978, loss = 34.57753524\n",
            "Iteration 979, loss = 34.53521635\n",
            "Iteration 980, loss = 34.49226160\n",
            "Iteration 981, loss = 34.45051756\n",
            "Iteration 982, loss = 34.40766533\n",
            "Iteration 983, loss = 34.36666449\n",
            "Iteration 984, loss = 34.32191417\n",
            "Iteration 985, loss = 34.27978285\n",
            "Iteration 986, loss = 34.24018780\n",
            "Iteration 987, loss = 34.19620854\n",
            "Iteration 988, loss = 34.15459361\n",
            "Iteration 989, loss = 34.11351493\n",
            "Iteration 990, loss = 34.07295434\n",
            "Iteration 991, loss = 34.03359414\n",
            "Iteration 992, loss = 33.99056131\n",
            "Iteration 993, loss = 33.94927845\n",
            "Iteration 994, loss = 33.90950084\n",
            "Iteration 995, loss = 33.86788642\n",
            "Iteration 996, loss = 33.82758186\n",
            "Iteration 997, loss = 33.78760226\n",
            "Iteration 998, loss = 33.74564613\n",
            "Iteration 999, loss = 33.70782599\n",
            "Iteration 1000, loss = 33.66739797\n",
            "Iteration 1001, loss = 33.62642957\n",
            "Iteration 1002, loss = 33.58666985\n",
            "Iteration 1003, loss = 33.54583363\n",
            "Iteration 1004, loss = 33.50845244\n",
            "Iteration 1005, loss = 33.46748515\n",
            "Iteration 1006, loss = 33.42803143\n",
            "Iteration 1007, loss = 33.38931318\n",
            "Iteration 1008, loss = 33.34915039\n",
            "Iteration 1009, loss = 33.31034814\n",
            "Iteration 1010, loss = 33.27171129\n",
            "Iteration 1011, loss = 33.23301112\n",
            "Iteration 1012, loss = 33.19412119\n",
            "Iteration 1013, loss = 33.15540897\n",
            "Iteration 1014, loss = 33.11690306\n",
            "Iteration 1015, loss = 33.07747978\n",
            "Iteration 1016, loss = 33.04045802\n",
            "Iteration 1017, loss = 33.00244648\n",
            "Iteration 1018, loss = 32.96392456\n",
            "Iteration 1019, loss = 32.92470956\n",
            "Iteration 1020, loss = 32.88715067\n",
            "Iteration 1021, loss = 32.84996024\n",
            "Iteration 1022, loss = 32.81311185\n",
            "Iteration 1023, loss = 32.77348513\n",
            "Iteration 1024, loss = 32.73772111\n",
            "Iteration 1025, loss = 32.69935163\n",
            "Iteration 1026, loss = 32.65943383\n",
            "Iteration 1027, loss = 32.62443044\n",
            "Iteration 1028, loss = 32.58830512\n",
            "Iteration 1029, loss = 32.54954349\n",
            "Iteration 1030, loss = 32.51147882\n",
            "Iteration 1031, loss = 32.47588323\n",
            "Iteration 1032, loss = 32.43923175\n",
            "Iteration 1033, loss = 32.40195959\n",
            "Iteration 1034, loss = 32.36534022\n",
            "Iteration 1035, loss = 32.32706350\n",
            "Iteration 1036, loss = 32.29053087\n",
            "Iteration 1037, loss = 32.25320230\n",
            "Iteration 1038, loss = 32.21930588\n",
            "Iteration 1039, loss = 32.18300599\n",
            "Iteration 1040, loss = 32.14495836\n",
            "Iteration 1041, loss = 32.10980415\n",
            "Iteration 1042, loss = 32.07280560\n",
            "Iteration 1043, loss = 32.03593967\n",
            "Iteration 1044, loss = 31.99860422\n",
            "Iteration 1045, loss = 31.96524926\n",
            "Iteration 1046, loss = 31.92793317\n",
            "Iteration 1047, loss = 31.89173898\n",
            "Iteration 1048, loss = 31.85838414\n",
            "Iteration 1049, loss = 31.82218868\n",
            "Iteration 1050, loss = 31.78375925\n",
            "Iteration 1051, loss = 31.74891659\n",
            "Iteration 1052, loss = 31.71263304\n",
            "Iteration 1053, loss = 31.67746083\n",
            "Iteration 1054, loss = 31.64140316\n",
            "Iteration 1055, loss = 31.60487508\n",
            "Iteration 1056, loss = 31.57347254\n",
            "Iteration 1057, loss = 31.53529131\n",
            "Iteration 1058, loss = 31.49908575\n",
            "Iteration 1059, loss = 31.46526920\n",
            "Iteration 1060, loss = 31.42673178\n",
            "Iteration 1061, loss = 31.39210521\n",
            "Iteration 1062, loss = 31.35563524\n",
            "Iteration 1063, loss = 31.31992808\n",
            "Iteration 1064, loss = 31.28596821\n",
            "Iteration 1065, loss = 31.24932358\n",
            "Iteration 1066, loss = 31.21278370\n",
            "Iteration 1067, loss = 31.17626208\n",
            "Iteration 1068, loss = 31.14197438\n",
            "Iteration 1069, loss = 31.10624972\n",
            "Iteration 1070, loss = 31.06855012\n",
            "Iteration 1071, loss = 31.03307100\n",
            "Iteration 1072, loss = 30.99596926\n",
            "Iteration 1073, loss = 30.95920332\n",
            "Iteration 1074, loss = 30.92293186\n",
            "Iteration 1075, loss = 30.88564138\n",
            "Iteration 1076, loss = 30.85110146\n",
            "Iteration 1077, loss = 30.81396881\n",
            "Iteration 1078, loss = 30.77716473\n",
            "Iteration 1079, loss = 30.74051922\n",
            "Iteration 1080, loss = 30.70390046\n",
            "Iteration 1081, loss = 30.66613364\n",
            "Iteration 1082, loss = 30.62853912\n",
            "Iteration 1083, loss = 30.59241668\n",
            "Iteration 1084, loss = 30.55580176\n",
            "Iteration 1085, loss = 30.51781041\n",
            "Iteration 1086, loss = 30.48118752\n",
            "Iteration 1087, loss = 30.44519202\n",
            "Iteration 1088, loss = 30.40733844\n",
            "Iteration 1089, loss = 30.36874680\n",
            "Iteration 1090, loss = 30.33402500\n",
            "Iteration 1091, loss = 30.29725316\n",
            "Iteration 1092, loss = 30.25946040\n",
            "Iteration 1093, loss = 30.22277203\n",
            "Iteration 1094, loss = 30.18607249\n",
            "Iteration 1095, loss = 30.14841464\n",
            "Iteration 1096, loss = 30.11195977\n",
            "Iteration 1097, loss = 30.07351852\n",
            "Iteration 1098, loss = 30.03940786\n",
            "Iteration 1099, loss = 30.00218415\n",
            "Iteration 1100, loss = 29.96486704\n",
            "Iteration 1101, loss = 29.92803805\n",
            "Iteration 1102, loss = 29.89288152\n",
            "Iteration 1103, loss = 29.85444058\n",
            "Iteration 1104, loss = 29.81868137\n",
            "Iteration 1105, loss = 29.78238302\n",
            "Iteration 1106, loss = 29.74348763\n",
            "Iteration 1107, loss = 29.70898347\n",
            "Iteration 1108, loss = 29.67114715\n",
            "Iteration 1109, loss = 29.63641892\n",
            "Iteration 1110, loss = 29.59997367\n",
            "Iteration 1111, loss = 29.56322076\n",
            "Iteration 1112, loss = 29.52524468\n",
            "Iteration 1113, loss = 29.49049692\n",
            "Iteration 1114, loss = 29.45463686\n",
            "Iteration 1115, loss = 29.41778658\n",
            "Iteration 1116, loss = 29.38161430\n",
            "Iteration 1117, loss = 29.34501509\n",
            "Iteration 1118, loss = 29.31006144\n",
            "Iteration 1119, loss = 29.27234847\n",
            "Iteration 1120, loss = 29.23538388\n",
            "Iteration 1121, loss = 29.19990234\n",
            "Iteration 1122, loss = 29.16300494\n",
            "Iteration 1123, loss = 29.12743289\n",
            "Iteration 1124, loss = 29.09341005\n",
            "Iteration 1125, loss = 29.05387527\n",
            "Iteration 1126, loss = 29.01790582\n",
            "Iteration 1127, loss = 28.98263552\n",
            "Iteration 1128, loss = 28.94528551\n",
            "Iteration 1129, loss = 28.90705580\n",
            "Iteration 1130, loss = 28.87223032\n",
            "Iteration 1131, loss = 28.83552640\n",
            "Iteration 1132, loss = 28.79950615\n",
            "Iteration 1133, loss = 28.76170652\n",
            "Iteration 1134, loss = 28.72473607\n",
            "Iteration 1135, loss = 28.68828044\n",
            "Iteration 1136, loss = 28.65242230\n",
            "Iteration 1137, loss = 28.61575236\n",
            "Iteration 1138, loss = 28.57885174\n",
            "Iteration 1139, loss = 28.54216612\n",
            "Iteration 1140, loss = 28.50356594\n",
            "Iteration 1141, loss = 28.46685841\n",
            "Iteration 1142, loss = 28.42914129\n",
            "Iteration 1143, loss = 28.39429022\n",
            "Iteration 1144, loss = 28.35453195\n",
            "Iteration 1145, loss = 28.31888786\n",
            "Iteration 1146, loss = 28.27945138\n",
            "Iteration 1147, loss = 28.24393610\n",
            "Iteration 1148, loss = 28.20871498\n",
            "Iteration 1149, loss = 28.16928308\n",
            "Iteration 1150, loss = 28.13289956\n",
            "Iteration 1151, loss = 28.09363117\n",
            "Iteration 1152, loss = 28.05931841\n",
            "Iteration 1153, loss = 28.02062851\n",
            "Iteration 1154, loss = 27.98378866\n",
            "Iteration 1155, loss = 27.94622206\n",
            "Iteration 1156, loss = 27.90660035\n",
            "Iteration 1157, loss = 27.87298440\n",
            "Iteration 1158, loss = 27.83289847\n",
            "Iteration 1159, loss = 27.79616418\n",
            "Iteration 1160, loss = 27.75737100\n",
            "Iteration 1161, loss = 27.72057927\n",
            "Iteration 1162, loss = 27.68406511\n",
            "Iteration 1163, loss = 27.64727488\n",
            "Iteration 1164, loss = 27.60849585\n",
            "Iteration 1165, loss = 27.57052267\n",
            "Iteration 1166, loss = 27.53398362\n",
            "Iteration 1167, loss = 27.49723466\n",
            "Iteration 1168, loss = 27.46016575\n",
            "Iteration 1169, loss = 27.42494402\n",
            "Iteration 1170, loss = 27.38547879\n",
            "Iteration 1171, loss = 27.34713517\n",
            "Iteration 1172, loss = 27.31141005\n",
            "Iteration 1173, loss = 27.27257881\n",
            "Iteration 1174, loss = 27.23467997\n",
            "Iteration 1175, loss = 27.19944543\n",
            "Iteration 1176, loss = 27.16096287\n",
            "Iteration 1177, loss = 27.12489321\n",
            "Iteration 1178, loss = 27.08968110\n",
            "Iteration 1179, loss = 27.05212986\n",
            "Iteration 1180, loss = 27.01712260\n",
            "Iteration 1181, loss = 26.97643801\n",
            "Iteration 1182, loss = 26.94222484\n",
            "Iteration 1183, loss = 26.90575095\n",
            "Iteration 1184, loss = 26.86967679\n",
            "Iteration 1185, loss = 26.83374245\n",
            "Iteration 1186, loss = 26.79689409\n",
            "Iteration 1187, loss = 26.76094608\n",
            "Iteration 1188, loss = 26.72512172\n",
            "Iteration 1189, loss = 26.69005402\n",
            "Iteration 1190, loss = 26.65319764\n",
            "Iteration 1191, loss = 26.61830638\n",
            "Iteration 1192, loss = 26.58112664\n",
            "Iteration 1193, loss = 26.54645702\n",
            "Iteration 1194, loss = 26.51386027\n",
            "Iteration 1195, loss = 26.47618230\n",
            "Iteration 1196, loss = 26.44257970\n",
            "Iteration 1197, loss = 26.40638248\n",
            "Iteration 1198, loss = 26.36998935\n",
            "Iteration 1199, loss = 26.33599735\n",
            "Iteration 1200, loss = 26.29972476\n",
            "Iteration 1201, loss = 26.26971744\n",
            "Iteration 1202, loss = 26.23193731\n",
            "Iteration 1203, loss = 26.19742632\n",
            "Iteration 1204, loss = 26.16435636\n",
            "Iteration 1205, loss = 26.12836373\n",
            "Iteration 1206, loss = 26.09596127\n",
            "Iteration 1207, loss = 26.05729832\n",
            "Iteration 1208, loss = 26.02302457\n",
            "Iteration 1209, loss = 25.98949414\n",
            "Iteration 1210, loss = 25.95596224\n",
            "Iteration 1211, loss = 25.92254221\n",
            "Iteration 1212, loss = 25.88763052\n",
            "Iteration 1213, loss = 25.85379128\n",
            "Iteration 1214, loss = 25.82006210\n",
            "Iteration 1215, loss = 25.78660721\n",
            "Iteration 1216, loss = 25.75191656\n",
            "Iteration 1217, loss = 25.71714139\n",
            "Iteration 1218, loss = 25.68704079\n",
            "Iteration 1219, loss = 25.65334899\n",
            "Iteration 1220, loss = 25.61969336\n",
            "Iteration 1221, loss = 25.58802380\n",
            "Iteration 1222, loss = 25.55495710\n",
            "Iteration 1223, loss = 25.52072377\n",
            "Iteration 1224, loss = 25.48813101\n",
            "Iteration 1225, loss = 25.45598619\n",
            "Iteration 1226, loss = 25.42322389\n",
            "Iteration 1227, loss = 25.39174645\n",
            "Iteration 1228, loss = 25.36032166\n",
            "Iteration 1229, loss = 25.32828844\n",
            "Iteration 1230, loss = 25.29618757\n",
            "Iteration 1231, loss = 25.26537630\n",
            "Iteration 1232, loss = 25.23221945\n",
            "Iteration 1233, loss = 25.20019358\n",
            "Iteration 1234, loss = 25.16952423\n",
            "Iteration 1235, loss = 25.13548507\n",
            "Iteration 1236, loss = 25.10536342\n",
            "Iteration 1237, loss = 25.07463023\n",
            "Iteration 1238, loss = 25.04460711\n",
            "Iteration 1239, loss = 25.01091132\n",
            "Iteration 1240, loss = 24.97968687\n",
            "Iteration 1241, loss = 24.94892772\n",
            "Iteration 1242, loss = 24.91785458\n",
            "Iteration 1243, loss = 24.88604170\n",
            "Iteration 1244, loss = 24.85579531\n",
            "Iteration 1245, loss = 24.82474496\n",
            "Iteration 1246, loss = 24.79349621\n",
            "Iteration 1247, loss = 24.76500891\n",
            "Iteration 1248, loss = 24.73135759\n",
            "Iteration 1249, loss = 24.70130983\n",
            "Iteration 1250, loss = 24.67090832\n",
            "Iteration 1251, loss = 24.63994225\n",
            "Iteration 1252, loss = 24.61260833\n",
            "Iteration 1253, loss = 24.57927347\n",
            "Iteration 1254, loss = 24.54994031\n",
            "Iteration 1255, loss = 24.51726694\n",
            "Iteration 1256, loss = 24.48823761\n",
            "Iteration 1257, loss = 24.45773825\n",
            "Iteration 1258, loss = 24.42920316\n",
            "Iteration 1259, loss = 24.39777707\n",
            "Iteration 1260, loss = 24.36865107\n",
            "Iteration 1261, loss = 24.33732342\n",
            "Iteration 1262, loss = 24.30714875\n",
            "Iteration 1263, loss = 24.27851422\n",
            "Iteration 1264, loss = 24.24707286\n",
            "Iteration 1265, loss = 24.21796509\n",
            "Iteration 1266, loss = 24.18871374\n",
            "Iteration 1267, loss = 24.15807860\n",
            "Iteration 1268, loss = 24.12801040\n",
            "Iteration 1269, loss = 24.09904118\n",
            "Iteration 1270, loss = 24.06952046\n",
            "Iteration 1271, loss = 24.03926804\n",
            "Iteration 1272, loss = 24.01020048\n",
            "Iteration 1273, loss = 23.98192009\n",
            "Iteration 1274, loss = 23.95135889\n",
            "Iteration 1275, loss = 23.92202071\n",
            "Iteration 1276, loss = 23.89329828\n",
            "Iteration 1277, loss = 23.86770664\n",
            "Iteration 1278, loss = 23.83708766\n",
            "Iteration 1279, loss = 23.80776536\n",
            "Iteration 1280, loss = 23.77849409\n",
            "Iteration 1281, loss = 23.74785485\n",
            "Iteration 1282, loss = 23.72054375\n",
            "Iteration 1283, loss = 23.69150248\n",
            "Iteration 1284, loss = 23.66362247\n",
            "Iteration 1285, loss = 23.63484222\n",
            "Iteration 1286, loss = 23.60607650\n",
            "Iteration 1287, loss = 23.57903991\n",
            "Iteration 1288, loss = 23.54912142\n",
            "Iteration 1289, loss = 23.52134130\n",
            "Iteration 1290, loss = 23.49269866\n",
            "Iteration 1291, loss = 23.46443504\n",
            "Iteration 1292, loss = 23.43695821\n",
            "Iteration 1293, loss = 23.40685754\n",
            "Iteration 1294, loss = 23.38029415\n",
            "Iteration 1295, loss = 23.35373623\n",
            "Iteration 1296, loss = 23.32583821\n",
            "Iteration 1297, loss = 23.29603192\n",
            "Iteration 1298, loss = 23.26722072\n",
            "Iteration 1299, loss = 23.24246484\n",
            "Iteration 1300, loss = 23.21417697\n",
            "Iteration 1301, loss = 23.18564806\n",
            "Iteration 1302, loss = 23.15689943\n",
            "Iteration 1303, loss = 23.12918268\n",
            "Iteration 1304, loss = 23.10346035\n",
            "Iteration 1305, loss = 23.07525390\n",
            "Iteration 1306, loss = 23.04769558\n",
            "Iteration 1307, loss = 23.02048628\n",
            "Iteration 1308, loss = 22.99287947\n",
            "Iteration 1309, loss = 22.96440677\n",
            "Iteration 1310, loss = 22.93792802\n",
            "Iteration 1311, loss = 22.91040001\n",
            "Iteration 1312, loss = 22.88308110\n",
            "Iteration 1313, loss = 22.85640650\n",
            "Iteration 1314, loss = 22.82887336\n",
            "Iteration 1315, loss = 22.80143750\n",
            "Iteration 1316, loss = 22.77399742\n",
            "Iteration 1317, loss = 22.74570352\n",
            "Iteration 1318, loss = 22.71920887\n",
            "Iteration 1319, loss = 22.69366324\n",
            "Iteration 1320, loss = 22.66922523\n",
            "Iteration 1321, loss = 22.63978819\n",
            "Iteration 1322, loss = 22.61035866\n",
            "Iteration 1323, loss = 22.58620129\n",
            "Iteration 1324, loss = 22.55961763\n",
            "Iteration 1325, loss = 22.53144424\n",
            "Iteration 1326, loss = 22.50598379\n",
            "Iteration 1327, loss = 22.48080408\n",
            "Iteration 1328, loss = 22.45235659\n",
            "Iteration 1329, loss = 22.42732045\n",
            "Iteration 1330, loss = 22.40156825\n",
            "Iteration 1331, loss = 22.37360133\n",
            "Iteration 1332, loss = 22.34867037\n",
            "Iteration 1333, loss = 22.32091428\n",
            "Iteration 1334, loss = 22.29658510\n",
            "Iteration 1335, loss = 22.26999939\n",
            "Iteration 1336, loss = 22.24417220\n",
            "Iteration 1337, loss = 22.21692434\n",
            "Iteration 1338, loss = 22.19164012\n",
            "Iteration 1339, loss = 22.16568408\n",
            "Iteration 1340, loss = 22.13943394\n",
            "Iteration 1341, loss = 22.11553683\n",
            "Iteration 1342, loss = 22.08919128\n",
            "Iteration 1343, loss = 22.06422066\n",
            "Iteration 1344, loss = 22.03664974\n",
            "Iteration 1345, loss = 22.01128936\n",
            "Iteration 1346, loss = 21.98498559\n",
            "Iteration 1347, loss = 21.95973082\n",
            "Iteration 1348, loss = 21.93423577\n",
            "Iteration 1349, loss = 21.90857069\n",
            "Iteration 1350, loss = 21.88444420\n",
            "Iteration 1351, loss = 21.85832749\n",
            "Iteration 1352, loss = 21.83116160\n",
            "Iteration 1353, loss = 21.80638861\n",
            "Iteration 1354, loss = 21.78409247\n",
            "Iteration 1355, loss = 21.75842267\n",
            "Iteration 1356, loss = 21.73158791\n",
            "Iteration 1357, loss = 21.70608522\n",
            "Iteration 1358, loss = 21.68177633\n",
            "Iteration 1359, loss = 21.65464900\n",
            "Iteration 1360, loss = 21.63266914\n",
            "Iteration 1361, loss = 21.60680021\n",
            "Iteration 1362, loss = 21.58200339\n",
            "Iteration 1363, loss = 21.55602401\n",
            "Iteration 1364, loss = 21.52958217\n",
            "Iteration 1365, loss = 21.50666895\n",
            "Iteration 1366, loss = 21.47983547\n",
            "Iteration 1367, loss = 21.45718565\n",
            "Iteration 1368, loss = 21.43290382\n",
            "Iteration 1369, loss = 21.40693220\n",
            "Iteration 1370, loss = 21.38219458\n",
            "Iteration 1371, loss = 21.35746048\n",
            "Iteration 1372, loss = 21.33254407\n",
            "Iteration 1373, loss = 21.30737502\n",
            "Iteration 1374, loss = 21.28361783\n",
            "Iteration 1375, loss = 21.25892059\n",
            "Iteration 1376, loss = 21.23354466\n",
            "Iteration 1377, loss = 21.20922379\n",
            "Iteration 1378, loss = 21.18596152\n",
            "Iteration 1379, loss = 21.16170963\n",
            "Iteration 1380, loss = 21.13655819\n",
            "Iteration 1381, loss = 21.11264706\n",
            "Iteration 1382, loss = 21.08575411\n",
            "Iteration 1383, loss = 21.06397195\n",
            "Iteration 1384, loss = 21.03940484\n",
            "Iteration 1385, loss = 21.01544411\n",
            "Iteration 1386, loss = 20.98879893\n",
            "Iteration 1387, loss = 20.96424961\n",
            "Iteration 1388, loss = 20.94348145\n",
            "Iteration 1389, loss = 20.91701852\n",
            "Iteration 1390, loss = 20.89222982\n",
            "Iteration 1391, loss = 20.86679350\n",
            "Iteration 1392, loss = 20.84344955\n",
            "Iteration 1393, loss = 20.81845959\n",
            "Iteration 1394, loss = 20.79370073\n",
            "Iteration 1395, loss = 20.77241541\n",
            "Iteration 1396, loss = 20.74463353\n",
            "Iteration 1397, loss = 20.72230725\n",
            "Iteration 1398, loss = 20.69707267\n",
            "Iteration 1399, loss = 20.67223567\n",
            "Iteration 1400, loss = 20.64673750\n",
            "Iteration 1401, loss = 20.62336121\n",
            "Iteration 1402, loss = 20.59953143\n",
            "Iteration 1403, loss = 20.57393605\n",
            "Iteration 1404, loss = 20.54952378\n",
            "Iteration 1405, loss = 20.52518939\n",
            "Iteration 1406, loss = 20.50162560\n",
            "Iteration 1407, loss = 20.47554581\n",
            "Iteration 1408, loss = 20.45113934\n",
            "Iteration 1409, loss = 20.42563530\n",
            "Iteration 1410, loss = 20.40015801\n",
            "Iteration 1411, loss = 20.37440974\n",
            "Iteration 1412, loss = 20.35231352\n",
            "Iteration 1413, loss = 20.32619456\n",
            "Iteration 1414, loss = 20.30005968\n",
            "Iteration 1415, loss = 20.27502983\n",
            "Iteration 1416, loss = 20.24872873\n",
            "Iteration 1417, loss = 20.22429376\n",
            "Iteration 1418, loss = 20.19704028\n",
            "Iteration 1419, loss = 20.17277888\n",
            "Iteration 1420, loss = 20.14601108\n",
            "Iteration 1421, loss = 20.12095208\n",
            "Iteration 1422, loss = 20.09396702\n",
            "Iteration 1423, loss = 20.06822221\n",
            "Iteration 1424, loss = 20.04310697\n",
            "Iteration 1425, loss = 20.01651471\n",
            "Iteration 1426, loss = 19.99022000\n",
            "Iteration 1427, loss = 19.96561742\n",
            "Iteration 1428, loss = 19.93882117\n",
            "Iteration 1429, loss = 19.91230408\n",
            "Iteration 1430, loss = 19.88614388\n",
            "Iteration 1431, loss = 19.86014332\n",
            "Iteration 1432, loss = 19.83681779\n",
            "Iteration 1433, loss = 19.81025869\n",
            "Iteration 1434, loss = 19.78265544\n",
            "Iteration 1435, loss = 19.75622324\n",
            "Iteration 1436, loss = 19.73052560\n",
            "Iteration 1437, loss = 19.70455984\n",
            "Iteration 1438, loss = 19.68169002\n",
            "Iteration 1439, loss = 19.65392718\n",
            "Iteration 1440, loss = 19.62688533\n",
            "Iteration 1441, loss = 19.60234030\n",
            "Iteration 1442, loss = 19.57716877\n",
            "Iteration 1443, loss = 19.55297773\n",
            "Iteration 1444, loss = 19.52537555\n",
            "Iteration 1445, loss = 19.49990909\n",
            "Iteration 1446, loss = 19.47528870\n",
            "Iteration 1447, loss = 19.44965446\n",
            "Iteration 1448, loss = 19.42363737\n",
            "Iteration 1449, loss = 19.39874992\n",
            "Iteration 1450, loss = 19.37554576\n",
            "Iteration 1451, loss = 19.34838633\n",
            "Iteration 1452, loss = 19.32325032\n",
            "Iteration 1453, loss = 19.29778984\n",
            "Iteration 1454, loss = 19.27332647\n",
            "Iteration 1455, loss = 19.24988803\n",
            "Iteration 1456, loss = 19.22302310\n",
            "Iteration 1457, loss = 19.19915614\n",
            "Iteration 1458, loss = 19.17411328\n",
            "Iteration 1459, loss = 19.14861287\n",
            "Iteration 1460, loss = 19.12615704\n",
            "Iteration 1461, loss = 19.09844136\n",
            "Iteration 1462, loss = 19.07700901\n",
            "Iteration 1463, loss = 19.05018665\n",
            "Iteration 1464, loss = 19.02943541\n",
            "Iteration 1465, loss = 19.00285118\n",
            "Iteration 1466, loss = 18.97874640\n",
            "Iteration 1467, loss = 18.95294282\n",
            "Iteration 1468, loss = 18.93034400\n",
            "Iteration 1469, loss = 18.90430889\n",
            "Iteration 1470, loss = 18.88138847\n",
            "Iteration 1471, loss = 18.86014292\n",
            "Iteration 1472, loss = 18.83236307\n",
            "Iteration 1473, loss = 18.81003567\n",
            "Iteration 1474, loss = 18.78629610\n",
            "Iteration 1475, loss = 18.76184570\n",
            "Iteration 1476, loss = 18.74116426\n",
            "Iteration 1477, loss = 18.71249722\n",
            "Iteration 1478, loss = 18.69052744\n",
            "Iteration 1479, loss = 18.66822952\n",
            "Iteration 1480, loss = 18.64438312\n",
            "Iteration 1481, loss = 18.62056072\n",
            "Iteration 1482, loss = 18.59840094\n",
            "Iteration 1483, loss = 18.57341250\n",
            "Iteration 1484, loss = 18.55214461\n",
            "Iteration 1485, loss = 18.52840550\n",
            "Iteration 1486, loss = 18.50565393\n",
            "Iteration 1487, loss = 18.48005573\n",
            "Iteration 1488, loss = 18.45938883\n",
            "Iteration 1489, loss = 18.43512732\n",
            "Iteration 1490, loss = 18.41245460\n",
            "Iteration 1491, loss = 18.38970704\n",
            "Iteration 1492, loss = 18.36783340\n",
            "Iteration 1493, loss = 18.34278654\n",
            "Iteration 1494, loss = 18.32173121\n",
            "Iteration 1495, loss = 18.29878337\n",
            "Iteration 1496, loss = 18.27645644\n",
            "Iteration 1497, loss = 18.25481711\n",
            "Iteration 1498, loss = 18.22967843\n",
            "Iteration 1499, loss = 18.20682907\n",
            "Iteration 1500, loss = 18.18582530\n",
            "Iteration 1501, loss = 18.16344123\n",
            "Iteration 1502, loss = 18.14069730\n",
            "Iteration 1503, loss = 18.11864888\n",
            "Iteration 1504, loss = 18.09575407\n",
            "Iteration 1505, loss = 18.07430745\n",
            "Iteration 1506, loss = 18.05161628\n",
            "Iteration 1507, loss = 18.02847355\n",
            "Iteration 1508, loss = 18.00807461\n",
            "Iteration 1509, loss = 17.98546558\n",
            "Iteration 1510, loss = 17.96398991\n",
            "Iteration 1511, loss = 17.94074780\n",
            "Iteration 1512, loss = 17.91833111\n",
            "Iteration 1513, loss = 17.89523323\n",
            "Iteration 1514, loss = 17.87693235\n",
            "Iteration 1515, loss = 17.85445345\n",
            "Iteration 1516, loss = 17.83228698\n",
            "Iteration 1517, loss = 17.81007407\n",
            "Iteration 1518, loss = 17.78669077\n",
            "Iteration 1519, loss = 17.76842602\n",
            "Iteration 1520, loss = 17.74364477\n",
            "Iteration 1521, loss = 17.72348308\n",
            "Iteration 1522, loss = 17.70153640\n",
            "Iteration 1523, loss = 17.68204374\n",
            "Iteration 1524, loss = 17.65695878\n",
            "Iteration 1525, loss = 17.63778469\n",
            "Iteration 1526, loss = 17.61684052\n",
            "Iteration 1527, loss = 17.59507748\n",
            "Iteration 1528, loss = 17.57327259\n",
            "Iteration 1529, loss = 17.55160366\n",
            "Iteration 1530, loss = 17.53043634\n",
            "Iteration 1531, loss = 17.50970139\n",
            "Iteration 1532, loss = 17.48801901\n",
            "Iteration 1533, loss = 17.46679557\n",
            "Iteration 1534, loss = 17.44758657\n",
            "Iteration 1535, loss = 17.42572780\n",
            "Iteration 1536, loss = 17.40371898\n",
            "Iteration 1537, loss = 17.38306805\n",
            "Iteration 1538, loss = 17.36316064\n",
            "Iteration 1539, loss = 17.34222869\n",
            "Iteration 1540, loss = 17.31980060\n",
            "Iteration 1541, loss = 17.30048177\n",
            "Iteration 1542, loss = 17.27820796\n",
            "Iteration 1543, loss = 17.25985576\n",
            "Iteration 1544, loss = 17.23830610\n",
            "Iteration 1545, loss = 17.21705330\n",
            "Iteration 1546, loss = 17.19845403\n",
            "Iteration 1547, loss = 17.17710169\n",
            "Iteration 1548, loss = 17.15637941\n",
            "Iteration 1549, loss = 17.13692331\n",
            "Iteration 1550, loss = 17.11475670\n",
            "Iteration 1551, loss = 17.09404559\n",
            "Iteration 1552, loss = 17.07451520\n",
            "Iteration 1553, loss = 17.05478990\n",
            "Iteration 1554, loss = 17.03460654\n",
            "Iteration 1555, loss = 17.01349934\n",
            "Iteration 1556, loss = 16.99338046\n",
            "Iteration 1557, loss = 16.97367866\n",
            "Iteration 1558, loss = 16.95238094\n",
            "Iteration 1559, loss = 16.93339914\n",
            "Iteration 1560, loss = 16.91273097\n",
            "Iteration 1561, loss = 16.89302133\n",
            "Iteration 1562, loss = 16.87300503\n",
            "Iteration 1563, loss = 16.85366330\n",
            "Iteration 1564, loss = 16.83311583\n",
            "Iteration 1565, loss = 16.81270276\n",
            "Iteration 1566, loss = 16.79343091\n",
            "Iteration 1567, loss = 16.77275906\n",
            "Iteration 1568, loss = 16.75333856\n",
            "Iteration 1569, loss = 16.73238285\n",
            "Iteration 1570, loss = 16.71588491\n",
            "Iteration 1571, loss = 16.69465883\n",
            "Iteration 1572, loss = 16.67364326\n",
            "Iteration 1573, loss = 16.65589598\n",
            "Iteration 1574, loss = 16.63581562\n",
            "Iteration 1575, loss = 16.61502070\n",
            "Iteration 1576, loss = 16.59606914\n",
            "Iteration 1577, loss = 16.57724110\n",
            "Iteration 1578, loss = 16.55556429\n",
            "Iteration 1579, loss = 16.53842491\n",
            "Iteration 1580, loss = 16.51778396\n",
            "Iteration 1581, loss = 16.49802721\n",
            "Iteration 1582, loss = 16.47831644\n",
            "Iteration 1583, loss = 16.45974821\n",
            "Iteration 1584, loss = 16.43971209\n",
            "Iteration 1585, loss = 16.42045451\n",
            "Iteration 1586, loss = 16.40102881\n",
            "Iteration 1587, loss = 16.38132570\n",
            "Iteration 1588, loss = 16.36205822\n",
            "Iteration 1589, loss = 16.34348656\n",
            "Iteration 1590, loss = 16.32543322\n",
            "Iteration 1591, loss = 16.30322535\n",
            "Iteration 1592, loss = 16.28686452\n",
            "Iteration 1593, loss = 16.26669913\n",
            "Iteration 1594, loss = 16.24680462\n",
            "Iteration 1595, loss = 16.22768670\n",
            "Iteration 1596, loss = 16.20932886\n",
            "Iteration 1597, loss = 16.18983157\n",
            "Iteration 1598, loss = 16.17298223\n",
            "Iteration 1599, loss = 16.15203086\n",
            "Iteration 1600, loss = 16.13267099\n",
            "Iteration 1601, loss = 16.11411294\n",
            "Iteration 1602, loss = 16.09406888\n",
            "Iteration 1603, loss = 16.07547602\n",
            "Iteration 1604, loss = 16.05748965\n",
            "Iteration 1605, loss = 16.03922873\n",
            "Iteration 1606, loss = 16.01888491\n",
            "Iteration 1607, loss = 16.00013578\n",
            "Iteration 1608, loss = 15.98469965\n",
            "Iteration 1609, loss = 15.96239152\n",
            "Iteration 1610, loss = 15.94531415\n",
            "Iteration 1611, loss = 15.92513084\n",
            "Iteration 1612, loss = 15.90890495\n",
            "Iteration 1613, loss = 15.88845734\n",
            "Iteration 1614, loss = 15.86978053\n",
            "Iteration 1615, loss = 15.85141958\n",
            "Iteration 1616, loss = 15.83367900\n",
            "Iteration 1617, loss = 15.81518172\n",
            "Iteration 1618, loss = 15.79560945\n",
            "Iteration 1619, loss = 15.77673992\n",
            "Iteration 1620, loss = 15.75866235\n",
            "Iteration 1621, loss = 15.73999353\n",
            "Iteration 1622, loss = 15.72245233\n",
            "Iteration 1623, loss = 15.70396693\n",
            "Iteration 1624, loss = 15.68498191\n",
            "Iteration 1625, loss = 15.66677877\n",
            "Iteration 1626, loss = 15.64889209\n",
            "Iteration 1627, loss = 15.62964930\n",
            "Iteration 1628, loss = 15.61091202\n",
            "Iteration 1629, loss = 15.59155916\n",
            "Iteration 1630, loss = 15.57365932\n",
            "Iteration 1631, loss = 15.55505417\n",
            "Iteration 1632, loss = 15.53836447\n",
            "Iteration 1633, loss = 15.51903625\n",
            "Iteration 1634, loss = 15.49927431\n",
            "Iteration 1635, loss = 15.48268459\n",
            "Iteration 1636, loss = 15.46542640\n",
            "Iteration 1637, loss = 15.44569315\n",
            "Iteration 1638, loss = 15.42713581\n",
            "Iteration 1639, loss = 15.41073288\n",
            "Iteration 1640, loss = 15.39013442\n",
            "Iteration 1641, loss = 15.37380483\n",
            "Iteration 1642, loss = 15.35465996\n",
            "Iteration 1643, loss = 15.33618178\n",
            "Iteration 1644, loss = 15.31735245\n",
            "Iteration 1645, loss = 15.29956443\n",
            "Iteration 1646, loss = 15.28243122\n",
            "Iteration 1647, loss = 15.26274591\n",
            "Iteration 1648, loss = 15.24562287\n",
            "Iteration 1649, loss = 15.22483279\n",
            "Iteration 1650, loss = 15.20858075\n",
            "Iteration 1651, loss = 15.18892500\n",
            "Iteration 1652, loss = 15.17241061\n",
            "Iteration 1653, loss = 15.15269861\n",
            "Iteration 1654, loss = 15.13640664\n",
            "Iteration 1655, loss = 15.11715202\n",
            "Iteration 1656, loss = 15.09883891\n",
            "Iteration 1657, loss = 15.08051055\n",
            "Iteration 1658, loss = 15.06320564\n",
            "Iteration 1659, loss = 15.04410972\n",
            "Iteration 1660, loss = 15.02797000\n",
            "Iteration 1661, loss = 15.00760504\n",
            "Iteration 1662, loss = 14.98941176\n",
            "Iteration 1663, loss = 14.97219259\n",
            "Iteration 1664, loss = 14.95322028\n",
            "Iteration 1665, loss = 14.93489497\n",
            "Iteration 1666, loss = 14.91620139\n",
            "Iteration 1667, loss = 14.89808081\n",
            "Iteration 1668, loss = 14.87926006\n",
            "Iteration 1669, loss = 14.86297278\n",
            "Iteration 1670, loss = 14.84315901\n",
            "Iteration 1671, loss = 14.82407221\n",
            "Iteration 1672, loss = 14.80545062\n",
            "Iteration 1673, loss = 14.78606482\n",
            "Iteration 1674, loss = 14.76752770\n",
            "Iteration 1675, loss = 14.74776471\n",
            "Iteration 1676, loss = 14.72898460\n",
            "Iteration 1677, loss = 14.71013146\n",
            "Iteration 1678, loss = 14.69122808\n",
            "Iteration 1679, loss = 14.67236868\n",
            "Iteration 1680, loss = 14.65205124\n",
            "Iteration 1681, loss = 14.63165011\n",
            "Iteration 1682, loss = 14.61324115\n",
            "Iteration 1683, loss = 14.59342489\n",
            "Iteration 1684, loss = 14.57597471\n",
            "Iteration 1685, loss = 14.55667807\n",
            "Iteration 1686, loss = 14.53345802\n",
            "Iteration 1687, loss = 14.51240174\n",
            "Iteration 1688, loss = 14.49200752\n",
            "Iteration 1689, loss = 14.47193698\n",
            "Iteration 1690, loss = 14.45152458\n",
            "Iteration 1691, loss = 14.42938010\n",
            "Iteration 1692, loss = 14.41109192\n",
            "Iteration 1693, loss = 14.38883364\n",
            "Iteration 1694, loss = 14.37068000\n",
            "Iteration 1695, loss = 14.34765780\n",
            "Iteration 1696, loss = 14.32513679\n",
            "Iteration 1697, loss = 14.30385596\n",
            "Iteration 1698, loss = 14.28175934\n",
            "Iteration 1699, loss = 14.26423179\n",
            "Iteration 1700, loss = 14.23955440\n",
            "Iteration 1701, loss = 14.21810220\n",
            "Iteration 1702, loss = 14.19680552\n",
            "Iteration 1703, loss = 14.17660792\n",
            "Iteration 1704, loss = 14.15458128\n",
            "Iteration 1705, loss = 14.13249314\n",
            "Iteration 1706, loss = 14.11774386\n",
            "Iteration 1707, loss = 14.09026224\n",
            "Iteration 1708, loss = 14.06800445\n",
            "Iteration 1709, loss = 14.04899179\n",
            "Iteration 1710, loss = 14.02724622\n",
            "Iteration 1711, loss = 14.00703721\n",
            "Iteration 1712, loss = 13.98581376\n",
            "Iteration 1713, loss = 13.96431503\n",
            "Iteration 1714, loss = 13.94276493\n",
            "Iteration 1715, loss = 13.92207642\n",
            "Iteration 1716, loss = 13.90178574\n",
            "Iteration 1717, loss = 13.88092290\n",
            "Iteration 1718, loss = 13.86177986\n",
            "Iteration 1719, loss = 13.84004146\n",
            "Iteration 1720, loss = 13.81805723\n",
            "Iteration 1721, loss = 13.79671886\n",
            "Iteration 1722, loss = 13.77753671\n",
            "Iteration 1723, loss = 13.75818325\n",
            "Iteration 1724, loss = 13.73651114\n",
            "Iteration 1725, loss = 13.71626056\n",
            "Iteration 1726, loss = 13.69404364\n",
            "Iteration 1727, loss = 13.67290275\n",
            "Iteration 1728, loss = 13.65472456\n",
            "Iteration 1729, loss = 13.63357780\n",
            "Iteration 1730, loss = 13.61366932\n",
            "Iteration 1731, loss = 13.59169778\n",
            "Iteration 1732, loss = 13.57097387\n",
            "Iteration 1733, loss = 13.55125680\n",
            "Iteration 1734, loss = 13.53167955\n",
            "Iteration 1735, loss = 13.51240557\n",
            "Iteration 1736, loss = 13.49038896\n",
            "Iteration 1737, loss = 13.46986332\n",
            "Iteration 1738, loss = 13.45097149\n",
            "Iteration 1739, loss = 13.43042592\n",
            "Iteration 1740, loss = 13.40977944\n",
            "Iteration 1741, loss = 13.39149014\n",
            "Iteration 1742, loss = 13.36898040\n",
            "Iteration 1743, loss = 13.35050023\n",
            "Iteration 1744, loss = 13.33074870\n",
            "Iteration 1745, loss = 13.31096876\n",
            "Iteration 1746, loss = 13.29058940\n",
            "Iteration 1747, loss = 13.27188079\n",
            "Iteration 1748, loss = 13.25122825\n",
            "Iteration 1749, loss = 13.23322018\n",
            "Iteration 1750, loss = 13.21341576\n",
            "Iteration 1751, loss = 13.19288556\n",
            "Iteration 1752, loss = 13.17318571\n",
            "Iteration 1753, loss = 13.15400253\n",
            "Iteration 1754, loss = 13.13533519\n",
            "Iteration 1755, loss = 13.11695004\n",
            "Iteration 1756, loss = 13.09660717\n",
            "Iteration 1757, loss = 13.07775811\n",
            "Iteration 1758, loss = 13.05658272\n",
            "Iteration 1759, loss = 13.03871415\n",
            "Iteration 1760, loss = 13.01995018\n",
            "Iteration 1761, loss = 13.00233851\n",
            "Iteration 1762, loss = 12.98235160\n",
            "Iteration 1763, loss = 12.96214233\n",
            "Iteration 1764, loss = 12.94483090\n",
            "Iteration 1765, loss = 12.92712786\n",
            "Iteration 1766, loss = 12.90755855\n",
            "Iteration 1767, loss = 12.89112551\n",
            "Iteration 1768, loss = 12.87352028\n",
            "Iteration 1769, loss = 12.85432030\n",
            "Iteration 1770, loss = 12.83421100\n",
            "Iteration 1771, loss = 12.81980610\n",
            "Iteration 1772, loss = 12.79847406\n",
            "Iteration 1773, loss = 12.77879843\n",
            "Iteration 1774, loss = 12.76210980\n",
            "Iteration 1775, loss = 12.74364284\n",
            "Iteration 1776, loss = 12.72559597\n",
            "Iteration 1777, loss = 12.70973421\n",
            "Iteration 1778, loss = 12.68950883\n",
            "Iteration 1779, loss = 12.67232466\n",
            "Iteration 1780, loss = 12.65237786\n",
            "Iteration 1781, loss = 12.63490561\n",
            "Iteration 1782, loss = 12.61682769\n",
            "Iteration 1783, loss = 12.59994465\n",
            "Iteration 1784, loss = 12.58276629\n",
            "Iteration 1785, loss = 12.56299660\n",
            "Iteration 1786, loss = 12.54630114\n",
            "Iteration 1787, loss = 12.52867832\n",
            "Iteration 1788, loss = 12.51415335\n",
            "Iteration 1789, loss = 12.49404131\n",
            "Iteration 1790, loss = 12.47708678\n",
            "Iteration 1791, loss = 12.45875406\n",
            "Iteration 1792, loss = 12.44298381\n",
            "Iteration 1793, loss = 12.42386584\n",
            "Iteration 1794, loss = 12.40625417\n",
            "Iteration 1795, loss = 12.38991123\n",
            "Iteration 1796, loss = 12.37217322\n",
            "Iteration 1797, loss = 12.35453357\n",
            "Iteration 1798, loss = 12.33742480\n",
            "Iteration 1799, loss = 12.32165997\n",
            "Iteration 1800, loss = 12.30418406\n",
            "Iteration 1801, loss = 12.28694254\n",
            "Iteration 1802, loss = 12.26931265\n",
            "Iteration 1803, loss = 12.25327594\n",
            "Iteration 1804, loss = 12.23831219\n",
            "Iteration 1805, loss = 12.22111759\n",
            "Iteration 1806, loss = 12.20140788\n",
            "Iteration 1807, loss = 12.18512684\n",
            "Iteration 1808, loss = 12.16898286\n",
            "Iteration 1809, loss = 12.15211295\n",
            "Iteration 1810, loss = 12.13474298\n",
            "Iteration 1811, loss = 12.11869029\n",
            "Iteration 1812, loss = 12.10127662\n",
            "Iteration 1813, loss = 12.08567425\n",
            "Iteration 1814, loss = 12.06830258\n",
            "Iteration 1815, loss = 12.05382477\n",
            "Iteration 1816, loss = 12.03514093\n",
            "Iteration 1817, loss = 12.01950305\n",
            "Iteration 1818, loss = 12.00197166\n",
            "Iteration 1819, loss = 11.98507921\n",
            "Iteration 1820, loss = 11.97232521\n",
            "Iteration 1821, loss = 11.95408552\n",
            "Iteration 1822, loss = 11.93859887\n",
            "Iteration 1823, loss = 11.92249107\n",
            "Iteration 1824, loss = 11.90527463\n",
            "Iteration 1825, loss = 11.88986601\n",
            "Iteration 1826, loss = 11.87547372\n",
            "Iteration 1827, loss = 11.85884478\n",
            "Iteration 1828, loss = 11.84111487\n",
            "Iteration 1829, loss = 11.82448413\n",
            "Iteration 1830, loss = 11.81294390\n",
            "Iteration 1831, loss = 11.79385975\n",
            "Iteration 1832, loss = 11.77726040\n",
            "Iteration 1833, loss = 11.76212968\n",
            "Iteration 1834, loss = 11.74492141\n",
            "Iteration 1835, loss = 11.73048483\n",
            "Iteration 1836, loss = 11.71411283\n",
            "Iteration 1837, loss = 11.69788547\n",
            "Iteration 1838, loss = 11.68226696\n",
            "Iteration 1839, loss = 11.66594071\n",
            "Iteration 1840, loss = 11.65007538\n",
            "Iteration 1841, loss = 11.63581572\n",
            "Iteration 1842, loss = 11.62021945\n",
            "Iteration 1843, loss = 11.60289275\n",
            "Iteration 1844, loss = 11.58755768\n",
            "Iteration 1845, loss = 11.57334461\n",
            "Iteration 1846, loss = 11.55778297\n",
            "Iteration 1847, loss = 11.54224594\n",
            "Iteration 1848, loss = 11.52695513\n",
            "Iteration 1849, loss = 11.50956974\n",
            "Iteration 1850, loss = 11.49442081\n",
            "Iteration 1851, loss = 11.48063959\n",
            "Iteration 1852, loss = 11.46542127\n",
            "Iteration 1853, loss = 11.45087663\n",
            "Iteration 1854, loss = 11.43486392\n",
            "Iteration 1855, loss = 11.42053388\n",
            "Iteration 1856, loss = 11.40468761\n",
            "Iteration 1857, loss = 11.38969868\n",
            "Iteration 1858, loss = 11.37439625\n",
            "Iteration 1859, loss = 11.35926177\n",
            "Iteration 1860, loss = 11.34397445\n",
            "Iteration 1861, loss = 11.32769223\n",
            "Iteration 1862, loss = 11.31731253\n",
            "Iteration 1863, loss = 11.30225197\n",
            "Iteration 1864, loss = 11.28367513\n",
            "Iteration 1865, loss = 11.26899489\n",
            "Iteration 1866, loss = 11.25490108\n",
            "Iteration 1867, loss = 11.23799738\n",
            "Iteration 1868, loss = 11.22559759\n",
            "Iteration 1869, loss = 11.20991521\n",
            "Iteration 1870, loss = 11.19555435\n",
            "Iteration 1871, loss = 11.18042238\n",
            "Iteration 1872, loss = 11.17080786\n",
            "Iteration 1873, loss = 11.15055972\n",
            "Iteration 1874, loss = 11.13848251\n",
            "Iteration 1875, loss = 11.12251456\n",
            "Iteration 1876, loss = 11.11128004\n",
            "Iteration 1877, loss = 11.09384753\n",
            "Iteration 1878, loss = 11.07991262\n",
            "Iteration 1879, loss = 11.06442153\n",
            "Iteration 1880, loss = 11.04971304\n",
            "Iteration 1881, loss = 11.03641835\n",
            "Iteration 1882, loss = 11.02306755\n",
            "Iteration 1883, loss = 11.01009388\n",
            "Iteration 1884, loss = 10.99472383\n",
            "Iteration 1885, loss = 10.98076010\n",
            "Iteration 1886, loss = 10.96629304\n",
            "Iteration 1887, loss = 10.95124762\n",
            "Iteration 1888, loss = 10.93663473\n",
            "Iteration 1889, loss = 10.92415433\n",
            "Iteration 1890, loss = 10.90821365\n",
            "Iteration 1891, loss = 10.89518929\n",
            "Iteration 1892, loss = 10.88108673\n",
            "Iteration 1893, loss = 10.86839690\n",
            "Iteration 1894, loss = 10.85381967\n",
            "Iteration 1895, loss = 10.84090831\n",
            "Iteration 1896, loss = 10.82650384\n",
            "Iteration 1897, loss = 10.81322916\n",
            "Iteration 1898, loss = 10.79912141\n",
            "Iteration 1899, loss = 10.78816069\n",
            "Iteration 1900, loss = 10.77187696\n",
            "Iteration 1901, loss = 10.75924741\n",
            "Iteration 1902, loss = 10.74658092\n",
            "Iteration 1903, loss = 10.73441816\n",
            "Iteration 1904, loss = 10.72033623\n",
            "Iteration 1905, loss = 10.70445368\n",
            "Iteration 1906, loss = 10.69110848\n",
            "Iteration 1907, loss = 10.67823302\n",
            "Iteration 1908, loss = 10.66656101\n",
            "Iteration 1909, loss = 10.65138653\n",
            "Iteration 1910, loss = 10.63798060\n",
            "Iteration 1911, loss = 10.62542634\n",
            "Iteration 1912, loss = 10.61179910\n",
            "Iteration 1913, loss = 10.59808551\n",
            "Iteration 1914, loss = 10.58684993\n",
            "Iteration 1915, loss = 10.57241702\n",
            "Iteration 1916, loss = 10.55897604\n",
            "Iteration 1917, loss = 10.54534406\n",
            "Iteration 1918, loss = 10.53375634\n",
            "Iteration 1919, loss = 10.51883648\n",
            "Iteration 1920, loss = 10.50748895\n",
            "Iteration 1921, loss = 10.49422485\n",
            "Iteration 1922, loss = 10.48121335\n",
            "Iteration 1923, loss = 10.46722082\n",
            "Iteration 1924, loss = 10.45405073\n",
            "Iteration 1925, loss = 10.44100547\n",
            "Iteration 1926, loss = 10.42845761\n",
            "Iteration 1927, loss = 10.41559044\n",
            "Iteration 1928, loss = 10.40284472\n",
            "Iteration 1929, loss = 10.39093793\n",
            "Iteration 1930, loss = 10.37710379\n",
            "Iteration 1931, loss = 10.36393573\n",
            "Iteration 1932, loss = 10.35050236\n",
            "Iteration 1933, loss = 10.33828198\n",
            "Iteration 1934, loss = 10.32680795\n",
            "Iteration 1935, loss = 10.31287266\n",
            "Iteration 1936, loss = 10.29992260\n",
            "Iteration 1937, loss = 10.28706983\n",
            "Iteration 1938, loss = 10.27400716\n",
            "Iteration 1939, loss = 10.26217337\n",
            "Iteration 1940, loss = 10.24874712\n",
            "Iteration 1941, loss = 10.23664643\n",
            "Iteration 1942, loss = 10.22484675\n",
            "Iteration 1943, loss = 10.21174056\n",
            "Iteration 1944, loss = 10.19768224\n",
            "Iteration 1945, loss = 10.18635323\n",
            "Iteration 1946, loss = 10.17299295\n",
            "Iteration 1947, loss = 10.16138288\n",
            "Iteration 1948, loss = 10.14872691\n",
            "Iteration 1949, loss = 10.13466009\n",
            "Iteration 1950, loss = 10.12308769\n",
            "Iteration 1951, loss = 10.11207399\n",
            "Iteration 1952, loss = 10.09887445\n",
            "Iteration 1953, loss = 10.08634512\n",
            "Iteration 1954, loss = 10.07417890\n",
            "Iteration 1955, loss = 10.06169376\n",
            "Iteration 1956, loss = 10.04956203\n",
            "Iteration 1957, loss = 10.03713990\n",
            "Iteration 1958, loss = 10.02518641\n",
            "Iteration 1959, loss = 10.01195052\n",
            "Iteration 1960, loss = 10.00158722\n",
            "Iteration 1961, loss = 9.98838638\n",
            "Iteration 1962, loss = 9.97663666\n",
            "Iteration 1963, loss = 9.96462635\n",
            "Iteration 1964, loss = 9.95225120\n",
            "Iteration 1965, loss = 9.93972870\n",
            "Iteration 1966, loss = 9.92826990\n",
            "Iteration 1967, loss = 9.91592213\n",
            "Iteration 1968, loss = 9.90552155\n",
            "Iteration 1969, loss = 9.89298535\n",
            "Iteration 1970, loss = 9.88184831\n",
            "Iteration 1971, loss = 9.86829849\n",
            "Iteration 1972, loss = 9.85796172\n",
            "Iteration 1973, loss = 9.84530990\n",
            "Iteration 1974, loss = 9.83621859\n",
            "Iteration 1975, loss = 9.82234311\n",
            "Iteration 1976, loss = 9.81029836\n",
            "Iteration 1977, loss = 9.79936819\n",
            "Iteration 1978, loss = 9.78629516\n",
            "Iteration 1979, loss = 9.77610780\n",
            "Iteration 1980, loss = 9.76395445\n",
            "Iteration 1981, loss = 9.75284201\n",
            "Iteration 1982, loss = 9.74155132\n",
            "Iteration 1983, loss = 9.72893359\n",
            "Iteration 1984, loss = 9.71726180\n",
            "Iteration 1985, loss = 9.70626879\n",
            "Iteration 1986, loss = 9.69431961\n",
            "Iteration 1987, loss = 9.68214590\n",
            "Iteration 1988, loss = 9.67321500\n",
            "Iteration 1989, loss = 9.66000215\n",
            "Iteration 1990, loss = 9.64874362\n",
            "Iteration 1991, loss = 9.63755068\n",
            "Iteration 1992, loss = 9.62594188\n",
            "Iteration 1993, loss = 9.61572356\n",
            "Iteration 1994, loss = 9.60356384\n",
            "Iteration 1995, loss = 9.59213307\n",
            "Iteration 1996, loss = 9.58053640\n",
            "Iteration 1997, loss = 9.56992225\n",
            "Iteration 1998, loss = 9.55739484\n",
            "Iteration 1999, loss = 9.54798189\n",
            "Iteration 2000, loss = 9.53580776\n",
            "Iteration 2001, loss = 9.52437762\n",
            "Iteration 2002, loss = 9.51277896\n",
            "Iteration 2003, loss = 9.50122577\n",
            "Iteration 2004, loss = 9.49233280\n",
            "Iteration 2005, loss = 9.48024503\n",
            "Iteration 2006, loss = 9.46820449\n",
            "Iteration 2007, loss = 9.45885771\n",
            "Iteration 2008, loss = 9.44569231\n",
            "Iteration 2009, loss = 9.43743971\n",
            "Iteration 2010, loss = 9.42445627\n",
            "Iteration 2011, loss = 9.41536833\n",
            "Iteration 2012, loss = 9.40445881\n",
            "Iteration 2013, loss = 9.39117097\n",
            "Iteration 2014, loss = 9.38057037\n",
            "Iteration 2015, loss = 9.37050049\n",
            "Iteration 2016, loss = 9.35874742\n",
            "Iteration 2017, loss = 9.34690938\n",
            "Iteration 2018, loss = 9.33713246\n",
            "Iteration 2019, loss = 9.32547990\n",
            "Iteration 2020, loss = 9.31693985\n",
            "Iteration 2021, loss = 9.30638603\n",
            "Iteration 2022, loss = 9.29446326\n",
            "Iteration 2023, loss = 9.28410543\n",
            "Iteration 2024, loss = 9.27200059\n",
            "Iteration 2025, loss = 9.26197489\n",
            "Iteration 2026, loss = 9.25207055\n",
            "Iteration 2027, loss = 9.24014447\n",
            "Iteration 2028, loss = 9.22906631\n",
            "Iteration 2029, loss = 9.21883010\n",
            "Iteration 2030, loss = 9.20823305\n",
            "Iteration 2031, loss = 9.19862439\n",
            "Iteration 2032, loss = 9.18758742\n",
            "Iteration 2033, loss = 9.17642663\n",
            "Iteration 2034, loss = 9.16661174\n",
            "Iteration 2035, loss = 9.15655180\n",
            "Iteration 2036, loss = 9.14548681\n",
            "Iteration 2037, loss = 9.13469936\n",
            "Iteration 2038, loss = 9.12323186\n",
            "Iteration 2039, loss = 9.11425657\n",
            "Iteration 2040, loss = 9.10469861\n",
            "Iteration 2041, loss = 9.09292899\n",
            "Iteration 2042, loss = 9.08228649\n",
            "Iteration 2043, loss = 9.07165305\n",
            "Iteration 2044, loss = 9.06279263\n",
            "Iteration 2045, loss = 9.05062243\n",
            "Iteration 2046, loss = 9.04066824\n",
            "Iteration 2047, loss = 9.03008617\n",
            "Iteration 2048, loss = 9.02079662\n",
            "Iteration 2049, loss = 9.01006831\n",
            "Iteration 2050, loss = 8.99762737\n",
            "Iteration 2051, loss = 8.99152286\n",
            "Iteration 2052, loss = 8.97746337\n",
            "Iteration 2053, loss = 8.96775316\n",
            "Iteration 2054, loss = 8.95873350\n",
            "Iteration 2055, loss = 8.94678476\n",
            "Iteration 2056, loss = 8.93760139\n",
            "Iteration 2057, loss = 8.92659155\n",
            "Iteration 2058, loss = 8.91449234\n",
            "Iteration 2059, loss = 8.90539371\n",
            "Iteration 2060, loss = 8.89644904\n",
            "Iteration 2061, loss = 8.88494312\n",
            "Iteration 2062, loss = 8.87535421\n",
            "Iteration 2063, loss = 8.86432191\n",
            "Iteration 2064, loss = 8.85439147\n",
            "Iteration 2065, loss = 8.84630153\n",
            "Iteration 2066, loss = 8.83492892\n",
            "Iteration 2067, loss = 8.82273913\n",
            "Iteration 2068, loss = 8.81348638\n",
            "Iteration 2069, loss = 8.80366971\n",
            "Iteration 2070, loss = 8.79487818\n",
            "Iteration 2071, loss = 8.78428212\n",
            "Iteration 2072, loss = 8.77309882\n",
            "Iteration 2073, loss = 8.76265181\n",
            "Iteration 2074, loss = 8.75321722\n",
            "Iteration 2075, loss = 8.74367909\n",
            "Iteration 2076, loss = 8.73345970\n",
            "Iteration 2077, loss = 8.72142200\n",
            "Iteration 2078, loss = 8.71119943\n",
            "Iteration 2079, loss = 8.70137731\n",
            "Iteration 2080, loss = 8.69139375\n",
            "Iteration 2081, loss = 8.68099684\n",
            "Iteration 2082, loss = 8.67130654\n",
            "Iteration 2083, loss = 8.66301977\n",
            "Iteration 2084, loss = 8.65067042\n",
            "Iteration 2085, loss = 8.64282411\n",
            "Iteration 2086, loss = 8.63034260\n",
            "Iteration 2087, loss = 8.62062073\n",
            "Iteration 2088, loss = 8.60956854\n",
            "Iteration 2089, loss = 8.59982556\n",
            "Iteration 2090, loss = 8.58970016\n",
            "Iteration 2091, loss = 8.58010316\n",
            "Iteration 2092, loss = 8.56872593\n",
            "Iteration 2093, loss = 8.55936000\n",
            "Iteration 2094, loss = 8.54833765\n",
            "Iteration 2095, loss = 8.53850584\n",
            "Iteration 2096, loss = 8.52717824\n",
            "Iteration 2097, loss = 8.51710919\n",
            "Iteration 2098, loss = 8.50714405\n",
            "Iteration 2099, loss = 8.49676421\n",
            "Iteration 2100, loss = 8.48690853\n",
            "Iteration 2101, loss = 8.47592477\n",
            "Iteration 2102, loss = 8.46488399\n",
            "Iteration 2103, loss = 8.45409333\n",
            "Iteration 2104, loss = 8.44333366\n",
            "Iteration 2105, loss = 8.43218764\n",
            "Iteration 2106, loss = 8.42199586\n",
            "Iteration 2107, loss = 8.41432582\n",
            "Iteration 2108, loss = 8.39979527\n",
            "Iteration 2109, loss = 8.38832487\n",
            "Iteration 2110, loss = 8.38128992\n",
            "Iteration 2111, loss = 8.36865708\n",
            "Iteration 2112, loss = 8.35634754\n",
            "Iteration 2113, loss = 8.34552579\n",
            "Iteration 2114, loss = 8.33647709\n",
            "Iteration 2115, loss = 8.32636449\n",
            "Iteration 2116, loss = 8.31604485\n",
            "Iteration 2117, loss = 8.30107347\n",
            "Iteration 2118, loss = 8.29107030\n",
            "Iteration 2119, loss = 8.27949122\n",
            "Iteration 2120, loss = 8.26975034\n",
            "Iteration 2121, loss = 8.25783795\n",
            "Iteration 2122, loss = 8.24769136\n",
            "Iteration 2123, loss = 8.23664382\n",
            "Iteration 2124, loss = 8.22491537\n",
            "Iteration 2125, loss = 8.21467073\n",
            "Iteration 2126, loss = 8.20337719\n",
            "Iteration 2127, loss = 8.19331652\n",
            "Iteration 2128, loss = 8.18254185\n",
            "Iteration 2129, loss = 8.17276709\n",
            "Iteration 2130, loss = 8.15991254\n",
            "Iteration 2131, loss = 8.15007272\n",
            "Iteration 2132, loss = 8.14160759\n",
            "Iteration 2133, loss = 8.12782380\n",
            "Iteration 2134, loss = 8.11761660\n",
            "Iteration 2135, loss = 8.10713852\n",
            "Iteration 2136, loss = 8.09759528\n",
            "Iteration 2137, loss = 8.08471560\n",
            "Iteration 2138, loss = 8.07713874\n",
            "Iteration 2139, loss = 8.06458222\n",
            "Iteration 2140, loss = 8.05267784\n",
            "Iteration 2141, loss = 8.04312718\n",
            "Iteration 2142, loss = 8.03202065\n",
            "Iteration 2143, loss = 8.02250175\n",
            "Iteration 2144, loss = 8.01113209\n",
            "Iteration 2145, loss = 7.99976130\n",
            "Iteration 2146, loss = 7.99067714\n",
            "Iteration 2147, loss = 7.97959478\n",
            "Iteration 2148, loss = 7.96952401\n",
            "Iteration 2149, loss = 7.96010277\n",
            "Iteration 2150, loss = 7.94975086\n",
            "Iteration 2151, loss = 7.93681706\n",
            "Iteration 2152, loss = 7.93010667\n",
            "Iteration 2153, loss = 7.91931843\n",
            "Iteration 2154, loss = 7.90673427\n",
            "Iteration 2155, loss = 7.89785661\n",
            "Iteration 2156, loss = 7.88923573\n",
            "Iteration 2157, loss = 7.87661002\n",
            "Iteration 2158, loss = 7.86540826\n",
            "Iteration 2159, loss = 7.85584153\n",
            "Iteration 2160, loss = 7.84466329\n",
            "Iteration 2161, loss = 7.83501784\n",
            "Iteration 2162, loss = 7.82655895\n",
            "Iteration 2163, loss = 7.81525851\n",
            "Iteration 2164, loss = 7.80561337\n",
            "Iteration 2165, loss = 7.79465659\n",
            "Iteration 2166, loss = 7.78456185\n",
            "Iteration 2167, loss = 7.77394335\n",
            "Iteration 2168, loss = 7.76355508\n",
            "Iteration 2169, loss = 7.75328495\n",
            "Iteration 2170, loss = 7.74542878\n",
            "Iteration 2171, loss = 7.73370260\n",
            "Iteration 2172, loss = 7.72435450\n",
            "Iteration 2173, loss = 7.71517852\n",
            "Iteration 2174, loss = 7.70485162\n",
            "Iteration 2175, loss = 7.69361370\n",
            "Iteration 2176, loss = 7.68260660\n",
            "Iteration 2177, loss = 7.67445031\n",
            "Iteration 2178, loss = 7.66714096\n",
            "Iteration 2179, loss = 7.65456507\n",
            "Iteration 2180, loss = 7.64560615\n",
            "Iteration 2181, loss = 7.63339109\n",
            "Iteration 2182, loss = 7.62525444\n",
            "Iteration 2183, loss = 7.61558762\n",
            "Iteration 2184, loss = 7.60421436\n",
            "Iteration 2185, loss = 7.59388695\n",
            "Iteration 2186, loss = 7.58637150\n",
            "Iteration 2187, loss = 7.57720514\n",
            "Iteration 2188, loss = 7.56721970\n",
            "Iteration 2189, loss = 7.55945680\n",
            "Iteration 2190, loss = 7.54814239\n",
            "Iteration 2191, loss = 7.53832083\n",
            "Iteration 2192, loss = 7.52775615\n",
            "Iteration 2193, loss = 7.51855216\n",
            "Iteration 2194, loss = 7.51091506\n",
            "Iteration 2195, loss = 7.50007716\n",
            "Iteration 2196, loss = 7.49019755\n",
            "Iteration 2197, loss = 7.47970595\n",
            "Iteration 2198, loss = 7.47157874\n",
            "Iteration 2199, loss = 7.46174803\n",
            "Iteration 2200, loss = 7.45304944\n",
            "Iteration 2201, loss = 7.44204319\n",
            "Iteration 2202, loss = 7.43420394\n",
            "Iteration 2203, loss = 7.42559210\n",
            "Iteration 2204, loss = 7.41595333\n",
            "Iteration 2205, loss = 7.40398533\n",
            "Iteration 2206, loss = 7.39594858\n",
            "Iteration 2207, loss = 7.38701859\n",
            "Iteration 2208, loss = 7.37725252\n",
            "Iteration 2209, loss = 7.36798333\n",
            "Iteration 2210, loss = 7.36014789\n",
            "Iteration 2211, loss = 7.34989255\n",
            "Iteration 2212, loss = 7.34002031\n",
            "Iteration 2213, loss = 7.33002562\n",
            "Iteration 2214, loss = 7.32343978\n",
            "Iteration 2215, loss = 7.31415419\n",
            "Iteration 2216, loss = 7.30292527\n",
            "Iteration 2217, loss = 7.29373927\n",
            "Iteration 2218, loss = 7.28571704\n",
            "Iteration 2219, loss = 7.27594216\n",
            "Iteration 2220, loss = 7.26694220\n",
            "Iteration 2221, loss = 7.25949957\n",
            "Iteration 2222, loss = 7.24975600\n",
            "Iteration 2223, loss = 7.23933480\n",
            "Iteration 2224, loss = 7.23025216\n",
            "Iteration 2225, loss = 7.22128895\n",
            "Iteration 2226, loss = 7.21211828\n",
            "Iteration 2227, loss = 7.20416105\n",
            "Iteration 2228, loss = 7.19560660\n",
            "Iteration 2229, loss = 7.18565628\n",
            "Iteration 2230, loss = 7.17699426\n",
            "Iteration 2231, loss = 7.16854164\n",
            "Iteration 2232, loss = 7.16174059\n",
            "Iteration 2233, loss = 7.15109489\n",
            "Iteration 2234, loss = 7.14175617\n",
            "Iteration 2235, loss = 7.13511532\n",
            "Iteration 2236, loss = 7.12603408\n",
            "Iteration 2237, loss = 7.11626563\n",
            "Iteration 2238, loss = 7.10711783\n",
            "Iteration 2239, loss = 7.09865097\n",
            "Iteration 2240, loss = 7.08988235\n",
            "Iteration 2241, loss = 7.08080217\n",
            "Iteration 2242, loss = 7.07435912\n",
            "Iteration 2243, loss = 7.06490054\n",
            "Iteration 2244, loss = 7.05581513\n",
            "Iteration 2245, loss = 7.04727773\n",
            "Iteration 2246, loss = 7.03770430\n",
            "Iteration 2247, loss = 7.03062853\n",
            "Iteration 2248, loss = 7.02136901\n",
            "Iteration 2249, loss = 7.01545929\n",
            "Iteration 2250, loss = 7.00475898\n",
            "Iteration 2251, loss = 6.99554342\n",
            "Iteration 2252, loss = 6.98800227\n",
            "Iteration 2253, loss = 6.98237508\n",
            "Iteration 2254, loss = 6.97053731\n",
            "Iteration 2255, loss = 6.96286851\n",
            "Iteration 2256, loss = 6.95338233\n",
            "Iteration 2257, loss = 6.94543086\n",
            "Iteration 2258, loss = 6.93822554\n",
            "Iteration 2259, loss = 6.92938926\n",
            "Iteration 2260, loss = 6.92106795\n",
            "Iteration 2261, loss = 6.91055803\n",
            "Iteration 2262, loss = 6.90249276\n",
            "Iteration 2263, loss = 6.89690041\n",
            "Iteration 2264, loss = 6.88711075\n",
            "Iteration 2265, loss = 6.87850788\n",
            "Iteration 2266, loss = 6.86888483\n",
            "Iteration 2267, loss = 6.86253818\n",
            "Iteration 2268, loss = 6.85338155\n",
            "Iteration 2269, loss = 6.84503115\n",
            "Iteration 2270, loss = 6.83563598\n",
            "Iteration 2271, loss = 6.83092175\n",
            "Iteration 2272, loss = 6.82199030\n",
            "Iteration 2273, loss = 6.81361244\n",
            "Iteration 2274, loss = 6.80430578\n",
            "Iteration 2275, loss = 6.79776822\n",
            "Iteration 2276, loss = 6.78915460\n",
            "Iteration 2277, loss = 6.77938814\n",
            "Iteration 2278, loss = 6.77137898\n",
            "Iteration 2279, loss = 6.76413685\n",
            "Iteration 2280, loss = 6.75643625\n",
            "Iteration 2281, loss = 6.74774178\n",
            "Iteration 2282, loss = 6.73998178\n",
            "Iteration 2283, loss = 6.73490613\n",
            "Iteration 2284, loss = 6.72283471\n",
            "Iteration 2285, loss = 6.71646861\n",
            "Iteration 2286, loss = 6.71223278\n",
            "Iteration 2287, loss = 6.70098364\n",
            "Iteration 2288, loss = 6.69186799\n",
            "Iteration 2289, loss = 6.68303315\n",
            "Iteration 2290, loss = 6.67585267\n",
            "Iteration 2291, loss = 6.66754777\n",
            "Iteration 2292, loss = 6.65916720\n",
            "Iteration 2293, loss = 6.65142772\n",
            "Iteration 2294, loss = 6.64400427\n",
            "Iteration 2295, loss = 6.63833669\n",
            "Iteration 2296, loss = 6.62865565\n",
            "Iteration 2297, loss = 6.61990412\n",
            "Iteration 2298, loss = 6.61270563\n",
            "Iteration 2299, loss = 6.60361384\n",
            "Iteration 2300, loss = 6.59669770\n",
            "Iteration 2301, loss = 6.58861448\n",
            "Iteration 2302, loss = 6.57984400\n",
            "Iteration 2303, loss = 6.57446691\n",
            "Iteration 2304, loss = 6.56451723\n",
            "Iteration 2305, loss = 6.55598904\n",
            "Iteration 2306, loss = 6.54883894\n",
            "Iteration 2307, loss = 6.54374619\n",
            "Iteration 2308, loss = 6.53518354\n",
            "Iteration 2309, loss = 6.52486323\n",
            "Iteration 2310, loss = 6.51870482\n",
            "Iteration 2311, loss = 6.50968100\n",
            "Iteration 2312, loss = 6.50268627\n",
            "Iteration 2313, loss = 6.49531814\n",
            "Iteration 2314, loss = 6.48784345\n",
            "Iteration 2315, loss = 6.47926783\n",
            "Iteration 2316, loss = 6.47111402\n",
            "Iteration 2317, loss = 6.46284626\n",
            "Iteration 2318, loss = 6.45675049\n",
            "Iteration 2319, loss = 6.44931602\n",
            "Iteration 2320, loss = 6.44033152\n",
            "Iteration 2321, loss = 6.43316209\n",
            "Iteration 2322, loss = 6.42445010\n",
            "Iteration 2323, loss = 6.41767965\n",
            "Iteration 2324, loss = 6.41054116\n",
            "Iteration 2325, loss = 6.40189898\n",
            "Iteration 2326, loss = 6.39417495\n",
            "Iteration 2327, loss = 6.38709192\n",
            "Iteration 2328, loss = 6.37985066\n",
            "Iteration 2329, loss = 6.37148091\n",
            "Iteration 2330, loss = 6.36333015\n",
            "Iteration 2331, loss = 6.35525911\n",
            "Iteration 2332, loss = 6.34931474\n",
            "Iteration 2333, loss = 6.34439255\n",
            "Iteration 2334, loss = 6.33453334\n",
            "Iteration 2335, loss = 6.32576785\n",
            "Iteration 2336, loss = 6.31848635\n",
            "Iteration 2337, loss = 6.31136890\n",
            "Iteration 2338, loss = 6.30350715\n",
            "Iteration 2339, loss = 6.29564018\n",
            "Iteration 2340, loss = 6.28920906\n",
            "Iteration 2341, loss = 6.28429820\n",
            "Iteration 2342, loss = 6.27438207\n",
            "Iteration 2343, loss = 6.26557827\n",
            "Iteration 2344, loss = 6.25945401\n",
            "Iteration 2345, loss = 6.25196910\n",
            "Iteration 2346, loss = 6.24577028\n",
            "Iteration 2347, loss = 6.23640396\n",
            "Iteration 2348, loss = 6.22981354\n",
            "Iteration 2349, loss = 6.22389296\n",
            "Iteration 2350, loss = 6.21546061\n",
            "Iteration 2351, loss = 6.20757234\n",
            "Iteration 2352, loss = 6.19986265\n",
            "Iteration 2353, loss = 6.19261757\n",
            "Iteration 2354, loss = 6.18579396\n",
            "Iteration 2355, loss = 6.17870945\n",
            "Iteration 2356, loss = 6.17605233\n",
            "Iteration 2357, loss = 6.16542098\n",
            "Iteration 2358, loss = 6.15610882\n",
            "Iteration 2359, loss = 6.15129995\n",
            "Iteration 2360, loss = 6.14174417\n",
            "Iteration 2361, loss = 6.13453335\n",
            "Iteration 2362, loss = 6.12805238\n",
            "Iteration 2363, loss = 6.12139082\n",
            "Iteration 2364, loss = 6.11303265\n",
            "Iteration 2365, loss = 6.10601595\n",
            "Iteration 2366, loss = 6.09918171\n",
            "Iteration 2367, loss = 6.09125822\n",
            "Iteration 2368, loss = 6.08403398\n",
            "Iteration 2369, loss = 6.07614846\n",
            "Iteration 2370, loss = 6.07323698\n",
            "Iteration 2371, loss = 6.06179935\n",
            "Iteration 2372, loss = 6.05578182\n",
            "Iteration 2373, loss = 6.04890620\n",
            "Iteration 2374, loss = 6.04283644\n",
            "Iteration 2375, loss = 6.03450258\n",
            "Iteration 2376, loss = 6.02769023\n",
            "Iteration 2377, loss = 6.01995999\n",
            "Iteration 2378, loss = 6.01171657\n",
            "Iteration 2379, loss = 6.00915573\n",
            "Iteration 2380, loss = 5.99804244\n",
            "Iteration 2381, loss = 5.99196204\n",
            "Iteration 2382, loss = 5.98427675\n",
            "Iteration 2383, loss = 5.97775034\n",
            "Iteration 2384, loss = 5.97204865\n",
            "Iteration 2385, loss = 5.96406415\n",
            "Iteration 2386, loss = 5.95426804\n",
            "Iteration 2387, loss = 5.94757553\n",
            "Iteration 2388, loss = 5.94016214\n",
            "Iteration 2389, loss = 5.93364059\n",
            "Iteration 2390, loss = 5.92789593\n",
            "Iteration 2391, loss = 5.92352489\n",
            "Iteration 2392, loss = 5.91241754\n",
            "Iteration 2393, loss = 5.90606940\n",
            "Iteration 2394, loss = 5.89877835\n",
            "Iteration 2395, loss = 5.89122534\n",
            "Iteration 2396, loss = 5.88399262\n",
            "Iteration 2397, loss = 5.87680837\n",
            "Iteration 2398, loss = 5.87036049\n",
            "Iteration 2399, loss = 5.86243519\n",
            "Iteration 2400, loss = 5.85562094\n",
            "Iteration 2401, loss = 5.84904098\n",
            "Iteration 2402, loss = 5.84209727\n",
            "Iteration 2403, loss = 5.83465994\n",
            "Iteration 2404, loss = 5.82873003\n",
            "Iteration 2405, loss = 5.82170699\n",
            "Iteration 2406, loss = 5.81464517\n",
            "Iteration 2407, loss = 5.80679385\n",
            "Iteration 2408, loss = 5.79994097\n",
            "Iteration 2409, loss = 5.79441877\n",
            "Iteration 2410, loss = 5.78635908\n",
            "Iteration 2411, loss = 5.77940942\n",
            "Iteration 2412, loss = 5.77345809\n",
            "Iteration 2413, loss = 5.76667727\n",
            "Iteration 2414, loss = 5.76039962\n",
            "Iteration 2415, loss = 5.75329179\n",
            "Iteration 2416, loss = 5.74545079\n",
            "Iteration 2417, loss = 5.74055307\n",
            "Iteration 2418, loss = 5.73345000\n",
            "Iteration 2419, loss = 5.72589992\n",
            "Iteration 2420, loss = 5.71887990\n",
            "Iteration 2421, loss = 5.71225979\n",
            "Iteration 2422, loss = 5.70727185\n",
            "Iteration 2423, loss = 5.69893543\n",
            "Iteration 2424, loss = 5.69265456\n",
            "Iteration 2425, loss = 5.68464994\n",
            "Iteration 2426, loss = 5.67891401\n",
            "Iteration 2427, loss = 5.67079002\n",
            "Iteration 2428, loss = 5.66463969\n",
            "Iteration 2429, loss = 5.65723678\n",
            "Iteration 2430, loss = 5.65076565\n",
            "Iteration 2431, loss = 5.64391599\n",
            "Iteration 2432, loss = 5.63652803\n",
            "Iteration 2433, loss = 5.63019662\n",
            "Iteration 2434, loss = 5.62389673\n",
            "Iteration 2435, loss = 5.61710277\n",
            "Iteration 2436, loss = 5.61053251\n",
            "Iteration 2437, loss = 5.60328803\n",
            "Iteration 2438, loss = 5.59660937\n",
            "Iteration 2439, loss = 5.59026063\n",
            "Iteration 2440, loss = 5.58331694\n",
            "Iteration 2441, loss = 5.57571280\n",
            "Iteration 2442, loss = 5.57115215\n",
            "Iteration 2443, loss = 5.56370774\n",
            "Iteration 2444, loss = 5.55630835\n",
            "Iteration 2445, loss = 5.55164895\n",
            "Iteration 2446, loss = 5.54380074\n",
            "Iteration 2447, loss = 5.53597427\n",
            "Iteration 2448, loss = 5.53040338\n",
            "Iteration 2449, loss = 5.52257138\n",
            "Iteration 2450, loss = 5.51665299\n",
            "Iteration 2451, loss = 5.51028650\n",
            "Iteration 2452, loss = 5.50273520\n",
            "Iteration 2453, loss = 5.49533043\n",
            "Iteration 2454, loss = 5.48974812\n",
            "Iteration 2455, loss = 5.48418261\n",
            "Iteration 2456, loss = 5.47799661\n",
            "Iteration 2457, loss = 5.47060023\n",
            "Iteration 2458, loss = 5.46318842\n",
            "Iteration 2459, loss = 5.45632780\n",
            "Iteration 2460, loss = 5.44924172\n",
            "Iteration 2461, loss = 5.44397316\n",
            "Iteration 2462, loss = 5.43659425\n",
            "Iteration 2463, loss = 5.43025677\n",
            "Iteration 2464, loss = 5.42307989\n",
            "Iteration 2465, loss = 5.41694030\n",
            "Iteration 2466, loss = 5.41106606\n",
            "Iteration 2467, loss = 5.40299606\n",
            "Iteration 2468, loss = 5.39636172\n",
            "Iteration 2469, loss = 5.39090294\n",
            "Iteration 2470, loss = 5.38387675\n",
            "Iteration 2471, loss = 5.37786745\n",
            "Iteration 2472, loss = 5.37241854\n",
            "Iteration 2473, loss = 5.36492134\n",
            "Iteration 2474, loss = 5.35735052\n",
            "Iteration 2475, loss = 5.35214469\n",
            "Iteration 2476, loss = 5.34450731\n",
            "Iteration 2477, loss = 5.33814972\n",
            "Iteration 2478, loss = 5.33356664\n",
            "Iteration 2479, loss = 5.32531785\n",
            "Iteration 2480, loss = 5.31921541\n",
            "Iteration 2481, loss = 5.31319296\n",
            "Iteration 2482, loss = 5.30682378\n",
            "Iteration 2483, loss = 5.29859273\n",
            "Iteration 2484, loss = 5.29284858\n",
            "Iteration 2485, loss = 5.28715455\n",
            "Iteration 2486, loss = 5.28117680\n",
            "Iteration 2487, loss = 5.27252624\n",
            "Iteration 2488, loss = 5.26737708\n",
            "Iteration 2489, loss = 5.26101881\n",
            "Iteration 2490, loss = 5.25373004\n",
            "Iteration 2491, loss = 5.24807288\n",
            "Iteration 2492, loss = 5.24204117\n",
            "Iteration 2493, loss = 5.23493155\n",
            "Iteration 2494, loss = 5.23118261\n",
            "Iteration 2495, loss = 5.22113135\n",
            "Iteration 2496, loss = 5.21622274\n",
            "Iteration 2497, loss = 5.20956784\n",
            "Iteration 2498, loss = 5.20349792\n",
            "Iteration 2499, loss = 5.19563744\n",
            "Iteration 2500, loss = 5.18982509\n",
            "Iteration 2501, loss = 5.18466333\n",
            "Iteration 2502, loss = 5.17812749\n",
            "Iteration 2503, loss = 5.17220717\n",
            "Iteration 2504, loss = 5.16520447\n",
            "Iteration 2505, loss = 5.15806639\n",
            "Iteration 2506, loss = 5.15578475\n",
            "Iteration 2507, loss = 5.14511526\n",
            "Iteration 2508, loss = 5.13888009\n",
            "Iteration 2509, loss = 5.13542786\n",
            "Iteration 2510, loss = 5.12687598\n",
            "Iteration 2511, loss = 5.12139025\n",
            "Iteration 2512, loss = 5.11568016\n",
            "Iteration 2513, loss = 5.10846669\n",
            "Iteration 2514, loss = 5.10130189\n",
            "Iteration 2515, loss = 5.09584095\n",
            "Iteration 2516, loss = 5.09011534\n",
            "Iteration 2517, loss = 5.08183752\n",
            "Iteration 2518, loss = 5.07612199\n",
            "Iteration 2519, loss = 5.07184026\n",
            "Iteration 2520, loss = 5.06552506\n",
            "Iteration 2521, loss = 5.05709809\n",
            "Iteration 2522, loss = 5.05095245\n",
            "Iteration 2523, loss = 5.04535245\n",
            "Iteration 2524, loss = 5.03818797\n",
            "Iteration 2525, loss = 5.03189582\n",
            "Iteration 2526, loss = 5.02575783\n",
            "Iteration 2527, loss = 5.02079221\n",
            "Iteration 2528, loss = 5.01316852\n",
            "Iteration 2529, loss = 5.00698704\n",
            "Iteration 2530, loss = 5.00160501\n",
            "Iteration 2531, loss = 4.99518359\n",
            "Iteration 2532, loss = 4.98922450\n",
            "Iteration 2533, loss = 4.98161277\n",
            "Iteration 2534, loss = 4.97664829\n",
            "Iteration 2535, loss = 4.97158479\n",
            "Iteration 2536, loss = 4.96302105\n",
            "Iteration 2537, loss = 4.95749106\n",
            "Iteration 2538, loss = 4.95217384\n",
            "Iteration 2539, loss = 4.94616498\n",
            "Iteration 2540, loss = 4.93857024\n",
            "Iteration 2541, loss = 4.93415758\n",
            "Iteration 2542, loss = 4.92753282\n",
            "Iteration 2543, loss = 4.92243518\n",
            "Iteration 2544, loss = 4.91795811\n",
            "Iteration 2545, loss = 4.91122117\n",
            "Iteration 2546, loss = 4.90425340\n",
            "Iteration 2547, loss = 4.89648391\n",
            "Iteration 2548, loss = 4.89293109\n",
            "Iteration 2549, loss = 4.88693489\n",
            "Iteration 2550, loss = 4.88143624\n",
            "Iteration 2551, loss = 4.87489321\n",
            "Iteration 2552, loss = 4.86852531\n",
            "Iteration 2553, loss = 4.86120404\n",
            "Iteration 2554, loss = 4.85539077\n",
            "Iteration 2555, loss = 4.85000986\n",
            "Iteration 2556, loss = 4.84422241\n",
            "Iteration 2557, loss = 4.83707414\n",
            "Iteration 2558, loss = 4.83356902\n",
            "Iteration 2559, loss = 4.82561054\n",
            "Iteration 2560, loss = 4.82018800\n",
            "Iteration 2561, loss = 4.81518811\n",
            "Iteration 2562, loss = 4.80790629\n",
            "Iteration 2563, loss = 4.80182141\n",
            "Iteration 2564, loss = 4.79727810\n",
            "Iteration 2565, loss = 4.79166510\n",
            "Iteration 2566, loss = 4.78471639\n",
            "Iteration 2567, loss = 4.77799984\n",
            "Iteration 2568, loss = 4.77213422\n",
            "Iteration 2569, loss = 4.77099137\n",
            "Iteration 2570, loss = 4.76123977\n",
            "Iteration 2571, loss = 4.75444153\n",
            "Iteration 2572, loss = 4.75016664\n",
            "Iteration 2573, loss = 4.75026496\n",
            "Iteration 2574, loss = 4.73611867\n",
            "Iteration 2575, loss = 4.73098377\n",
            "Iteration 2576, loss = 4.72462235\n",
            "Iteration 2577, loss = 4.72163099\n",
            "Iteration 2578, loss = 4.71296653\n",
            "Iteration 2579, loss = 4.70739490\n",
            "Iteration 2580, loss = 4.70166100\n",
            "Iteration 2581, loss = 4.69449203\n",
            "Iteration 2582, loss = 4.68926374\n",
            "Iteration 2583, loss = 4.68459727\n",
            "Iteration 2584, loss = 4.67719764\n",
            "Iteration 2585, loss = 4.67041251\n",
            "Iteration 2586, loss = 4.66593214\n",
            "Iteration 2587, loss = 4.65945648\n",
            "Iteration 2588, loss = 4.65429824\n",
            "Iteration 2589, loss = 4.64765287\n",
            "Iteration 2590, loss = 4.64064798\n",
            "Iteration 2591, loss = 4.63524829\n",
            "Iteration 2592, loss = 4.62892706\n",
            "Iteration 2593, loss = 4.62267773\n",
            "Iteration 2594, loss = 4.61753849\n",
            "Iteration 2595, loss = 4.61053411\n",
            "Iteration 2596, loss = 4.60603917\n",
            "Iteration 2597, loss = 4.59984285\n",
            "Iteration 2598, loss = 4.59156908\n",
            "Iteration 2599, loss = 4.58724468\n",
            "Iteration 2600, loss = 4.57913444\n",
            "Iteration 2601, loss = 4.57269301\n",
            "Iteration 2602, loss = 4.56809166\n",
            "Iteration 2603, loss = 4.55980275\n",
            "Iteration 2604, loss = 4.55348705\n",
            "Iteration 2605, loss = 4.54752465\n",
            "Iteration 2606, loss = 4.53970089\n",
            "Iteration 2607, loss = 4.53418139\n",
            "Iteration 2608, loss = 4.52668115\n",
            "Iteration 2609, loss = 4.52035119\n",
            "Iteration 2610, loss = 4.51254847\n",
            "Iteration 2611, loss = 4.50701453\n",
            "Iteration 2612, loss = 4.49824783\n",
            "Iteration 2613, loss = 4.48971813\n",
            "Iteration 2614, loss = 4.48205747\n",
            "Iteration 2615, loss = 4.47401587\n",
            "Iteration 2616, loss = 4.46827442\n",
            "Iteration 2617, loss = 4.46076497\n",
            "Iteration 2618, loss = 4.45178670\n",
            "Iteration 2619, loss = 4.44306151\n",
            "Iteration 2620, loss = 4.43400500\n",
            "Iteration 2621, loss = 4.42995191\n",
            "Iteration 2622, loss = 4.41922138\n",
            "Iteration 2623, loss = 4.40959804\n",
            "Iteration 2624, loss = 4.40436708\n",
            "Iteration 2625, loss = 4.39620104\n",
            "Iteration 2626, loss = 4.38577407\n",
            "Iteration 2627, loss = 4.37976326\n",
            "Iteration 2628, loss = 4.37007866\n",
            "Iteration 2629, loss = 4.36580740\n",
            "Iteration 2630, loss = 4.35490864\n",
            "Iteration 2631, loss = 4.34617095\n",
            "Iteration 2632, loss = 4.33790681\n",
            "Iteration 2633, loss = 4.33188962\n",
            "Iteration 2634, loss = 4.32543268\n",
            "Iteration 2635, loss = 4.31603994\n",
            "Iteration 2636, loss = 4.30877010\n",
            "Iteration 2637, loss = 4.30340644\n",
            "Iteration 2638, loss = 4.29398461\n",
            "Iteration 2639, loss = 4.28695753\n",
            "Iteration 2640, loss = 4.27965694\n",
            "Iteration 2641, loss = 4.27191800\n",
            "Iteration 2642, loss = 4.26526710\n",
            "Iteration 2643, loss = 4.25880409\n",
            "Iteration 2644, loss = 4.25127406\n",
            "Iteration 2645, loss = 4.24362443\n",
            "Iteration 2646, loss = 4.23755058\n",
            "Iteration 2647, loss = 4.22864701\n",
            "Iteration 2648, loss = 4.22222224\n",
            "Iteration 2649, loss = 4.21562528\n",
            "Iteration 2650, loss = 4.21134093\n",
            "Iteration 2651, loss = 4.20161290\n",
            "Iteration 2652, loss = 4.19490289\n",
            "Iteration 2653, loss = 4.18759320\n",
            "Iteration 2654, loss = 4.18225922\n",
            "Iteration 2655, loss = 4.17445533\n",
            "Iteration 2656, loss = 4.16976038\n",
            "Iteration 2657, loss = 4.16120289\n",
            "Iteration 2658, loss = 4.15331823\n",
            "Iteration 2659, loss = 4.14884806\n",
            "Iteration 2660, loss = 4.14112030\n",
            "Iteration 2661, loss = 4.13492183\n",
            "Iteration 2662, loss = 4.12865398\n",
            "Iteration 2663, loss = 4.12181636\n",
            "Iteration 2664, loss = 4.11708200\n",
            "Iteration 2665, loss = 4.11104037\n",
            "Iteration 2666, loss = 4.10204040\n",
            "Iteration 2667, loss = 4.09437524\n",
            "Iteration 2668, loss = 4.08823999\n",
            "Iteration 2669, loss = 4.08223888\n",
            "Iteration 2670, loss = 4.07546993\n",
            "Iteration 2671, loss = 4.06879220\n",
            "Iteration 2672, loss = 4.06211119\n",
            "Iteration 2673, loss = 4.05607529\n",
            "Iteration 2674, loss = 4.04977490\n",
            "Iteration 2675, loss = 4.04364621\n",
            "Iteration 2676, loss = 4.03660265\n",
            "Iteration 2677, loss = 4.03055792\n",
            "Iteration 2678, loss = 4.02675425\n",
            "Iteration 2679, loss = 4.01748536\n",
            "Iteration 2680, loss = 4.01004076\n",
            "Iteration 2681, loss = 4.00827253\n",
            "Iteration 2682, loss = 3.99880109\n",
            "Iteration 2683, loss = 3.99689211\n",
            "Iteration 2684, loss = 3.98671112\n",
            "Iteration 2685, loss = 3.97804455\n",
            "Iteration 2686, loss = 3.97293722\n",
            "Iteration 2687, loss = 3.97027117\n",
            "Iteration 2688, loss = 3.96090028\n",
            "Iteration 2689, loss = 3.95258317\n",
            "Iteration 2690, loss = 3.95170033\n",
            "Iteration 2691, loss = 3.94104132\n",
            "Iteration 2692, loss = 3.93771225\n",
            "Iteration 2693, loss = 3.93061710\n",
            "Iteration 2694, loss = 3.92337499\n",
            "Iteration 2695, loss = 3.91607535\n",
            "Iteration 2696, loss = 3.90882925\n",
            "Iteration 2697, loss = 3.90333105\n",
            "Iteration 2698, loss = 3.89785730\n",
            "Iteration 2699, loss = 3.89225341\n",
            "Iteration 2700, loss = 3.88410264\n",
            "Iteration 2701, loss = 3.87877504\n",
            "Iteration 2702, loss = 3.87214088\n",
            "Iteration 2703, loss = 3.86565793\n",
            "Iteration 2704, loss = 3.86076204\n",
            "Iteration 2705, loss = 3.85322234\n",
            "Iteration 2706, loss = 3.84738637\n",
            "Iteration 2707, loss = 3.84293489\n",
            "Iteration 2708, loss = 3.83454658\n",
            "Iteration 2709, loss = 3.83101031\n",
            "Iteration 2710, loss = 3.82253569\n",
            "Iteration 2711, loss = 3.81656807\n",
            "Iteration 2712, loss = 3.81178335\n",
            "Iteration 2713, loss = 3.80490708\n",
            "Iteration 2714, loss = 3.79938841\n",
            "Iteration 2715, loss = 3.79596677\n",
            "Iteration 2716, loss = 3.78596636\n",
            "Iteration 2717, loss = 3.78225801\n",
            "Iteration 2718, loss = 3.77882061\n",
            "Iteration 2719, loss = 3.76726115\n",
            "Iteration 2720, loss = 3.76299955\n",
            "Iteration 2721, loss = 3.76032273\n",
            "Iteration 2722, loss = 3.75141091\n",
            "Iteration 2723, loss = 3.74499520\n",
            "Iteration 2724, loss = 3.74136350\n",
            "Iteration 2725, loss = 3.73203931\n",
            "Iteration 2726, loss = 3.72568433\n",
            "Iteration 2727, loss = 3.72139153\n",
            "Iteration 2728, loss = 3.71982849\n",
            "Iteration 2729, loss = 3.70928294\n",
            "Iteration 2730, loss = 3.70163246\n",
            "Iteration 2731, loss = 3.69638267\n",
            "Iteration 2732, loss = 3.69060170\n",
            "Iteration 2733, loss = 3.68609832\n",
            "Iteration 2734, loss = 3.67862318\n",
            "Iteration 2735, loss = 3.67386004\n",
            "Iteration 2736, loss = 3.66928022\n",
            "Iteration 2737, loss = 3.66228094\n",
            "Iteration 2738, loss = 3.65603379\n",
            "Iteration 2739, loss = 3.65154292\n",
            "Iteration 2740, loss = 3.64598718\n",
            "Iteration 2741, loss = 3.63734425\n",
            "Iteration 2742, loss = 3.63271470\n",
            "Iteration 2743, loss = 3.62750585\n",
            "Iteration 2744, loss = 3.61999678\n",
            "Iteration 2745, loss = 3.61648200\n",
            "Iteration 2746, loss = 3.60893357\n",
            "Iteration 2747, loss = 3.60265993\n",
            "Iteration 2748, loss = 3.59709442\n",
            "Iteration 2749, loss = 3.59161721\n",
            "Iteration 2750, loss = 3.58646799\n",
            "Iteration 2751, loss = 3.58047361\n",
            "Iteration 2752, loss = 3.57411458\n",
            "Iteration 2753, loss = 3.56918758\n",
            "Iteration 2754, loss = 3.56312542\n",
            "Iteration 2755, loss = 3.55713049\n",
            "Iteration 2756, loss = 3.55077265\n",
            "Iteration 2757, loss = 3.54548928\n",
            "Iteration 2758, loss = 3.54016109\n",
            "Iteration 2759, loss = 3.53416602\n",
            "Iteration 2760, loss = 3.52860172\n",
            "Iteration 2761, loss = 3.52257438\n",
            "Iteration 2762, loss = 3.51852024\n",
            "Iteration 2763, loss = 3.51066282\n",
            "Iteration 2764, loss = 3.50672781\n",
            "Iteration 2765, loss = 3.49942293\n",
            "Iteration 2766, loss = 3.49526315\n",
            "Iteration 2767, loss = 3.49019412\n",
            "Iteration 2768, loss = 3.48301460\n",
            "Iteration 2769, loss = 3.47684097\n",
            "Iteration 2770, loss = 3.47084348\n",
            "Iteration 2771, loss = 3.46552042\n",
            "Iteration 2772, loss = 3.46134836\n",
            "Iteration 2773, loss = 3.45503475\n",
            "Iteration 2774, loss = 3.45023881\n",
            "Iteration 2775, loss = 3.44319578\n",
            "Iteration 2776, loss = 3.43881394\n",
            "Iteration 2777, loss = 3.43165624\n",
            "Iteration 2778, loss = 3.42690754\n",
            "Iteration 2779, loss = 3.42189265\n",
            "Iteration 2780, loss = 3.41592003\n",
            "Iteration 2781, loss = 3.40970348\n",
            "Iteration 2782, loss = 3.40503174\n",
            "Iteration 2783, loss = 3.40047134\n",
            "Iteration 2784, loss = 3.39301041\n",
            "Iteration 2785, loss = 3.38813606\n",
            "Iteration 2786, loss = 3.38253788\n",
            "Iteration 2787, loss = 3.37724166\n",
            "Iteration 2788, loss = 3.37209227\n",
            "Iteration 2789, loss = 3.36644324\n",
            "Iteration 2790, loss = 3.36120642\n",
            "Iteration 2791, loss = 3.35542202\n",
            "Iteration 2792, loss = 3.35025809\n",
            "Iteration 2793, loss = 3.34445947\n",
            "Iteration 2794, loss = 3.33799223\n",
            "Iteration 2795, loss = 3.33410730\n",
            "Iteration 2796, loss = 3.32797148\n",
            "Iteration 2797, loss = 3.32210304\n",
            "Iteration 2798, loss = 3.31767128\n",
            "Iteration 2799, loss = 3.31211638\n",
            "Iteration 2800, loss = 3.30759409\n",
            "Iteration 2801, loss = 3.30099456\n",
            "Iteration 2802, loss = 3.29470478\n",
            "Iteration 2803, loss = 3.28957300\n",
            "Iteration 2804, loss = 3.28432118\n",
            "Iteration 2805, loss = 3.27883772\n",
            "Iteration 2806, loss = 3.27500011\n",
            "Iteration 2807, loss = 3.26869945\n",
            "Iteration 2808, loss = 3.26301715\n",
            "Iteration 2809, loss = 3.25729907\n",
            "Iteration 2810, loss = 3.25351718\n",
            "Iteration 2811, loss = 3.24929861\n",
            "Iteration 2812, loss = 3.24153361\n",
            "Iteration 2813, loss = 3.23751458\n",
            "Iteration 2814, loss = 3.23249115\n",
            "Iteration 2815, loss = 3.22577808\n",
            "Iteration 2816, loss = 3.22007838\n",
            "Iteration 2817, loss = 3.21892132\n",
            "Iteration 2818, loss = 3.21040403\n",
            "Iteration 2819, loss = 3.20534102\n",
            "Iteration 2820, loss = 3.20017284\n",
            "Iteration 2821, loss = 3.19292532\n",
            "Iteration 2822, loss = 3.18946262\n",
            "Iteration 2823, loss = 3.18519531\n",
            "Iteration 2824, loss = 3.17674319\n",
            "Iteration 2825, loss = 3.17955788\n",
            "Iteration 2826, loss = 3.16899862\n",
            "Iteration 2827, loss = 3.16178821\n",
            "Iteration 2828, loss = 3.15864172\n",
            "Iteration 2829, loss = 3.15400278\n",
            "Iteration 2830, loss = 3.14784487\n",
            "Iteration 2831, loss = 3.14359567\n",
            "Iteration 2832, loss = 3.13662516\n",
            "Iteration 2833, loss = 3.13571009\n",
            "Iteration 2834, loss = 3.12716927\n",
            "Iteration 2835, loss = 3.12358574\n",
            "Iteration 2836, loss = 3.11548732\n",
            "Iteration 2837, loss = 3.11106634\n",
            "Iteration 2838, loss = 3.10496455\n",
            "Iteration 2839, loss = 3.10026580\n",
            "Iteration 2840, loss = 3.09507367\n",
            "Iteration 2841, loss = 3.09089958\n",
            "Iteration 2842, loss = 3.08438541\n",
            "Iteration 2843, loss = 3.08121551\n",
            "Iteration 2844, loss = 3.07641327\n",
            "Iteration 2845, loss = 3.06988645\n",
            "Iteration 2846, loss = 3.06439147\n",
            "Iteration 2847, loss = 3.05945278\n",
            "Iteration 2848, loss = 3.05532433\n",
            "Iteration 2849, loss = 3.04892194\n",
            "Iteration 2850, loss = 3.04472026\n",
            "Iteration 2851, loss = 3.04125233\n",
            "Iteration 2852, loss = 3.03467769\n",
            "Iteration 2853, loss = 3.02976006\n",
            "Iteration 2854, loss = 3.02507696\n",
            "Iteration 2855, loss = 3.01877251\n",
            "Iteration 2856, loss = 3.01568931\n",
            "Iteration 2857, loss = 3.00972696\n",
            "Iteration 2858, loss = 3.00385803\n",
            "Iteration 2859, loss = 3.00088758\n",
            "Iteration 2860, loss = 2.99475238\n",
            "Iteration 2861, loss = 2.98921292\n",
            "Iteration 2862, loss = 2.98546976\n",
            "Iteration 2863, loss = 2.98068392\n",
            "Iteration 2864, loss = 2.97549169\n",
            "Iteration 2865, loss = 2.97180314\n",
            "Iteration 2866, loss = 2.96517925\n",
            "Iteration 2867, loss = 2.96035325\n",
            "Iteration 2868, loss = 2.95566712\n",
            "Iteration 2869, loss = 2.95181227\n",
            "Iteration 2870, loss = 2.94621866\n",
            "Iteration 2871, loss = 2.94060447\n",
            "Iteration 2872, loss = 2.93628786\n",
            "Iteration 2873, loss = 2.93155661\n",
            "Iteration 2874, loss = 2.92685859\n",
            "Iteration 2875, loss = 2.92055519\n",
            "Iteration 2876, loss = 2.91624696\n",
            "Iteration 2877, loss = 2.91201896\n",
            "Iteration 2878, loss = 2.90670867\n",
            "Iteration 2879, loss = 2.90214196\n",
            "Iteration 2880, loss = 2.89760736\n",
            "Iteration 2881, loss = 2.89316093\n",
            "Iteration 2882, loss = 2.88969076\n",
            "Iteration 2883, loss = 2.88281176\n",
            "Iteration 2884, loss = 2.88078824\n",
            "Iteration 2885, loss = 2.87287425\n",
            "Iteration 2886, loss = 2.86903636\n",
            "Iteration 2887, loss = 2.86449370\n",
            "Iteration 2888, loss = 2.85923592\n",
            "Iteration 2889, loss = 2.85415424\n",
            "Iteration 2890, loss = 2.85193103\n",
            "Iteration 2891, loss = 2.84509245\n",
            "Iteration 2892, loss = 2.84067900\n",
            "Iteration 2893, loss = 2.83660835\n",
            "Iteration 2894, loss = 2.83296249\n",
            "Iteration 2895, loss = 2.82869549\n",
            "Iteration 2896, loss = 2.82360231\n",
            "Iteration 2897, loss = 2.81855015\n",
            "Iteration 2898, loss = 2.81438369\n",
            "Iteration 2899, loss = 2.81035926\n",
            "Iteration 2900, loss = 2.80246712\n",
            "Iteration 2901, loss = 2.79931622\n",
            "Iteration 2902, loss = 2.79363748\n",
            "Iteration 2903, loss = 2.79160435\n",
            "Iteration 2904, loss = 2.78554121\n",
            "Iteration 2905, loss = 2.78125530\n",
            "Iteration 2906, loss = 2.77999396\n",
            "Iteration 2907, loss = 2.77173946\n",
            "Iteration 2908, loss = 2.76684909\n",
            "Iteration 2909, loss = 2.76326383\n",
            "Iteration 2910, loss = 2.75946923\n",
            "Iteration 2911, loss = 2.75454210\n",
            "Iteration 2912, loss = 2.75032678\n",
            "Iteration 2913, loss = 2.74342274\n",
            "Iteration 2914, loss = 2.73947916\n",
            "Iteration 2915, loss = 2.73568175\n",
            "Iteration 2916, loss = 2.73320428\n",
            "Iteration 2917, loss = 2.72419935\n",
            "Iteration 2918, loss = 2.72215257\n",
            "Iteration 2919, loss = 2.71968565\n",
            "Iteration 2920, loss = 2.71424596\n",
            "Iteration 2921, loss = 2.70793319\n",
            "Iteration 2922, loss = 2.70493021\n",
            "Iteration 2923, loss = 2.69902566\n",
            "Iteration 2924, loss = 2.69489781\n",
            "Iteration 2925, loss = 2.69316585\n",
            "Iteration 2926, loss = 2.68561758\n",
            "Iteration 2927, loss = 2.68064714\n",
            "Iteration 2928, loss = 2.67575777\n",
            "Iteration 2929, loss = 2.67145160\n",
            "Iteration 2930, loss = 2.66762088\n",
            "Iteration 2931, loss = 2.66320135\n",
            "Iteration 2932, loss = 2.65825470\n",
            "Iteration 2933, loss = 2.65548462\n",
            "Iteration 2934, loss = 2.64930361\n",
            "Iteration 2935, loss = 2.64500805\n",
            "Iteration 2936, loss = 2.64229763\n",
            "Iteration 2937, loss = 2.63739655\n",
            "Iteration 2938, loss = 2.63245055\n",
            "Iteration 2939, loss = 2.62851947\n",
            "Iteration 2940, loss = 2.62281948\n",
            "Iteration 2941, loss = 2.61851812\n",
            "Iteration 2942, loss = 2.61608371\n",
            "Iteration 2943, loss = 2.60969227\n",
            "Iteration 2944, loss = 2.60627056\n",
            "Iteration 2945, loss = 2.60176163\n",
            "Iteration 2946, loss = 2.59720117\n",
            "Iteration 2947, loss = 2.59306927\n",
            "Iteration 2948, loss = 2.58892772\n",
            "Iteration 2949, loss = 2.58492826\n",
            "Iteration 2950, loss = 2.58191028\n",
            "Iteration 2951, loss = 2.57612330\n",
            "Iteration 2952, loss = 2.57179802\n",
            "Iteration 2953, loss = 2.56859061\n",
            "Iteration 2954, loss = 2.56376687\n",
            "Iteration 2955, loss = 2.55873656\n",
            "Iteration 2956, loss = 2.55616601\n",
            "Iteration 2957, loss = 2.55203382\n",
            "Iteration 2958, loss = 2.54607485\n",
            "Iteration 2959, loss = 2.54248035\n",
            "Iteration 2960, loss = 2.53937800\n",
            "Iteration 2961, loss = 2.53468673\n",
            "Iteration 2962, loss = 2.53161481\n",
            "Iteration 2963, loss = 2.52545392\n",
            "Iteration 2964, loss = 2.52194690\n",
            "Iteration 2965, loss = 2.51735482\n",
            "Iteration 2966, loss = 2.51267169\n",
            "Iteration 2967, loss = 2.50909544\n",
            "Iteration 2968, loss = 2.50449733\n",
            "Iteration 2969, loss = 2.50003082\n",
            "Iteration 2970, loss = 2.49693886\n",
            "Iteration 2971, loss = 2.49300159\n",
            "Iteration 2972, loss = 2.48901161\n",
            "Iteration 2973, loss = 2.48529128\n",
            "Iteration 2974, loss = 2.48159347\n",
            "Iteration 2975, loss = 2.47594599\n",
            "Iteration 2976, loss = 2.47329088\n",
            "Iteration 2977, loss = 2.46954170\n",
            "Iteration 2978, loss = 2.46581507\n",
            "Iteration 2979, loss = 2.46079536\n",
            "Iteration 2980, loss = 2.45609334\n",
            "Iteration 2981, loss = 2.45226535\n",
            "Iteration 2982, loss = 2.44814194\n",
            "Iteration 2983, loss = 2.44449958\n",
            "Iteration 2984, loss = 2.44313101\n",
            "Iteration 2985, loss = 2.43568285\n",
            "Iteration 2986, loss = 2.43157331\n",
            "Iteration 2987, loss = 2.42983576\n",
            "Iteration 2988, loss = 2.42460512\n",
            "Iteration 2989, loss = 2.42122313\n",
            "Iteration 2990, loss = 2.41748835\n",
            "Iteration 2991, loss = 2.41120164\n",
            "Iteration 2992, loss = 2.40811059\n",
            "Iteration 2993, loss = 2.40357774\n",
            "Iteration 2994, loss = 2.39886141\n",
            "Iteration 2995, loss = 2.39643388\n",
            "Iteration 2996, loss = 2.39288975\n",
            "Iteration 2997, loss = 2.38797502\n",
            "Iteration 2998, loss = 2.38279466\n",
            "Iteration 2999, loss = 2.38052152\n",
            "Iteration 3000, loss = 2.37644343\n",
            "Iteration 3001, loss = 2.37270184\n",
            "Iteration 3002, loss = 2.36858484\n",
            "Iteration 3003, loss = 2.36320156\n",
            "Iteration 3004, loss = 2.35984416\n",
            "Iteration 3005, loss = 2.35612441\n",
            "Iteration 3006, loss = 2.35398669\n",
            "Iteration 3007, loss = 2.34730562\n",
            "Iteration 3008, loss = 2.34293905\n",
            "Iteration 3009, loss = 2.33948505\n",
            "Iteration 3010, loss = 2.33490237\n",
            "Iteration 3011, loss = 2.33209555\n",
            "Iteration 3012, loss = 2.32702028\n",
            "Iteration 3013, loss = 2.32274061\n",
            "Iteration 3014, loss = 2.32199872\n",
            "Iteration 3015, loss = 2.31606880\n",
            "Iteration 3016, loss = 2.31130904\n",
            "Iteration 3017, loss = 2.30881021\n",
            "Iteration 3018, loss = 2.30344860\n",
            "Iteration 3019, loss = 2.29914049\n",
            "Iteration 3020, loss = 2.29627433\n",
            "Iteration 3021, loss = 2.29279791\n",
            "Iteration 3022, loss = 2.28691644\n",
            "Iteration 3023, loss = 2.28337283\n",
            "Iteration 3024, loss = 2.27913077\n",
            "Iteration 3025, loss = 2.27527615\n",
            "Iteration 3026, loss = 2.27116899\n",
            "Iteration 3027, loss = 2.26828576\n",
            "Iteration 3028, loss = 2.26341743\n",
            "Iteration 3029, loss = 2.25945293\n",
            "Iteration 3030, loss = 2.25592936\n",
            "Iteration 3031, loss = 2.25242836\n",
            "Iteration 3032, loss = 2.25222063\n",
            "Iteration 3033, loss = 2.24514312\n",
            "Iteration 3034, loss = 2.23935255\n",
            "Iteration 3035, loss = 2.23760464\n",
            "Iteration 3036, loss = 2.23140629\n",
            "Iteration 3037, loss = 2.22775069\n",
            "Iteration 3038, loss = 2.22314426\n",
            "Iteration 3039, loss = 2.21967592\n",
            "Iteration 3040, loss = 2.21545600\n",
            "Iteration 3041, loss = 2.21208238\n",
            "Iteration 3042, loss = 2.20772908\n",
            "Iteration 3043, loss = 2.20466870\n",
            "Iteration 3044, loss = 2.19952107\n",
            "Iteration 3045, loss = 2.19428232\n",
            "Iteration 3046, loss = 2.19115348\n",
            "Iteration 3047, loss = 2.18716542\n",
            "Iteration 3048, loss = 2.18385108\n",
            "Iteration 3049, loss = 2.17777583\n",
            "Iteration 3050, loss = 2.17332909\n",
            "Iteration 3051, loss = 2.16853836\n",
            "Iteration 3052, loss = 2.16539529\n",
            "Iteration 3053, loss = 2.16006284\n",
            "Iteration 3054, loss = 2.15651098\n",
            "Iteration 3055, loss = 2.15135855\n",
            "Iteration 3056, loss = 2.14813153\n",
            "Iteration 3057, loss = 2.14430242\n",
            "Iteration 3058, loss = 2.13987728\n",
            "Iteration 3059, loss = 2.13409152\n",
            "Iteration 3060, loss = 2.13126252\n",
            "Iteration 3061, loss = 2.12577758\n",
            "Iteration 3062, loss = 2.12068271\n",
            "Iteration 3063, loss = 2.11704605\n",
            "Iteration 3064, loss = 2.11290427\n",
            "Iteration 3065, loss = 2.10927026\n",
            "Iteration 3066, loss = 2.10633419\n",
            "Iteration 3067, loss = 2.10166778\n",
            "Iteration 3068, loss = 2.09578771\n",
            "Iteration 3069, loss = 2.09285712\n",
            "Iteration 3070, loss = 2.08865380\n",
            "Iteration 3071, loss = 2.08479162\n",
            "Iteration 3072, loss = 2.07779974\n",
            "Iteration 3073, loss = 2.07386954\n",
            "Iteration 3074, loss = 2.07116534\n",
            "Iteration 3075, loss = 2.06703166\n",
            "Iteration 3076, loss = 2.06159696\n",
            "Iteration 3077, loss = 2.05935452\n",
            "Iteration 3078, loss = 2.05327338\n",
            "Iteration 3079, loss = 2.04991186\n",
            "Iteration 3080, loss = 2.04458430\n",
            "Iteration 3081, loss = 2.04126604\n",
            "Iteration 3082, loss = 2.03698826\n",
            "Iteration 3083, loss = 2.03269630\n",
            "Iteration 3084, loss = 2.02898551\n",
            "Iteration 3085, loss = 2.02357308\n",
            "Iteration 3086, loss = 2.01992717\n",
            "Iteration 3087, loss = 2.01611478\n",
            "Iteration 3088, loss = 2.01222942\n",
            "Iteration 3089, loss = 2.00780631\n",
            "Iteration 3090, loss = 2.00458788\n",
            "Iteration 3091, loss = 2.00054467\n",
            "Iteration 3092, loss = 1.99587466\n",
            "Iteration 3093, loss = 1.99175794\n",
            "Iteration 3094, loss = 1.98879288\n",
            "Iteration 3095, loss = 1.98488591\n",
            "Iteration 3096, loss = 1.97994473\n",
            "Iteration 3097, loss = 1.97667034\n",
            "Iteration 3098, loss = 1.97360677\n",
            "Iteration 3099, loss = 1.96895209\n",
            "Iteration 3100, loss = 1.96622010\n",
            "Iteration 3101, loss = 1.96098715\n",
            "Iteration 3102, loss = 1.95702856\n",
            "Iteration 3103, loss = 1.95312881\n",
            "Iteration 3104, loss = 1.94991112\n",
            "Iteration 3105, loss = 1.94489664\n",
            "Iteration 3106, loss = 1.94281455\n",
            "Iteration 3107, loss = 1.93850618\n",
            "Iteration 3108, loss = 1.93490866\n",
            "Iteration 3109, loss = 1.93251263\n",
            "Iteration 3110, loss = 1.92743068\n",
            "Iteration 3111, loss = 1.92423953\n",
            "Iteration 3112, loss = 1.91964895\n",
            "Iteration 3113, loss = 1.91675534\n",
            "Iteration 3114, loss = 1.91373130\n",
            "Iteration 3115, loss = 1.90870623\n",
            "Iteration 3116, loss = 1.90563737\n",
            "Iteration 3117, loss = 1.90411483\n",
            "Iteration 3118, loss = 1.89712513\n",
            "Iteration 3119, loss = 1.89415166\n",
            "Iteration 3120, loss = 1.88927283\n",
            "Iteration 3121, loss = 1.88739724\n",
            "Iteration 3122, loss = 1.88437723\n",
            "Iteration 3123, loss = 1.87830967\n",
            "Iteration 3124, loss = 1.87472852\n",
            "Iteration 3125, loss = 1.87347334\n",
            "Iteration 3126, loss = 1.86943618\n",
            "Iteration 3127, loss = 1.86557726\n",
            "Iteration 3128, loss = 1.86172361\n",
            "Iteration 3129, loss = 1.85733969\n",
            "Iteration 3130, loss = 1.85591046\n",
            "Iteration 3131, loss = 1.85235698\n",
            "Iteration 3132, loss = 1.84601208\n",
            "Iteration 3133, loss = 1.84371408\n",
            "Iteration 3134, loss = 1.84012709\n",
            "Iteration 3135, loss = 1.83522070\n",
            "Iteration 3136, loss = 1.83355150\n",
            "Iteration 3137, loss = 1.82869981\n",
            "Iteration 3138, loss = 1.82591481\n",
            "Iteration 3139, loss = 1.82144431\n",
            "Iteration 3140, loss = 1.81781388\n",
            "Iteration 3141, loss = 1.81590393\n",
            "Iteration 3142, loss = 1.81177340\n",
            "Iteration 3143, loss = 1.81040022\n",
            "Iteration 3144, loss = 1.80645279\n",
            "Iteration 3145, loss = 1.80218192\n",
            "Iteration 3146, loss = 1.79741186\n",
            "Iteration 3147, loss = 1.79384498\n",
            "Iteration 3148, loss = 1.79395419\n",
            "Iteration 3149, loss = 1.78780288\n",
            "Iteration 3150, loss = 1.78498384\n",
            "Iteration 3151, loss = 1.78351067\n",
            "Iteration 3152, loss = 1.77814599\n",
            "Iteration 3153, loss = 1.77490271\n",
            "Iteration 3154, loss = 1.77246652\n",
            "Iteration 3155, loss = 1.76815281\n",
            "Iteration 3156, loss = 1.76340545\n",
            "Iteration 3157, loss = 1.76042996\n",
            "Iteration 3158, loss = 1.75750601\n",
            "Iteration 3159, loss = 1.75364683\n",
            "Iteration 3160, loss = 1.75023365\n",
            "Iteration 3161, loss = 1.74713473\n",
            "Iteration 3162, loss = 1.74325234\n",
            "Iteration 3163, loss = 1.74022969\n",
            "Iteration 3164, loss = 1.73745184\n",
            "Iteration 3165, loss = 1.73609852\n",
            "Iteration 3166, loss = 1.73013926\n",
            "Iteration 3167, loss = 1.72889099\n",
            "Iteration 3168, loss = 1.72349141\n",
            "Iteration 3169, loss = 1.72221318\n",
            "Iteration 3170, loss = 1.71768076\n",
            "Iteration 3171, loss = 1.71444626\n",
            "Iteration 3172, loss = 1.71087932\n",
            "Iteration 3173, loss = 1.70791885\n",
            "Iteration 3174, loss = 1.70603861\n",
            "Iteration 3175, loss = 1.70270057\n",
            "Iteration 3176, loss = 1.69787745\n",
            "Iteration 3177, loss = 1.69752783\n",
            "Iteration 3178, loss = 1.69261831\n",
            "Iteration 3179, loss = 1.68912445\n",
            "Iteration 3180, loss = 1.68775345\n",
            "Iteration 3181, loss = 1.68244173\n",
            "Iteration 3182, loss = 1.68019020\n",
            "Iteration 3183, loss = 1.67603962\n",
            "Iteration 3184, loss = 1.67328877\n",
            "Iteration 3185, loss = 1.67086235\n",
            "Iteration 3186, loss = 1.66817668\n",
            "Iteration 3187, loss = 1.66363957\n",
            "Iteration 3188, loss = 1.66076309\n",
            "Iteration 3189, loss = 1.65714669\n",
            "Iteration 3190, loss = 1.65479723\n",
            "Iteration 3191, loss = 1.65105912\n",
            "Iteration 3192, loss = 1.64962155\n",
            "Iteration 3193, loss = 1.64517466\n",
            "Iteration 3194, loss = 1.64369262\n",
            "Iteration 3195, loss = 1.64023972\n",
            "Iteration 3196, loss = 1.63590549\n",
            "Iteration 3197, loss = 1.63571484\n",
            "Iteration 3198, loss = 1.63107043\n",
            "Iteration 3199, loss = 1.62806221\n",
            "Iteration 3200, loss = 1.62418302\n",
            "Iteration 3201, loss = 1.62133472\n",
            "Iteration 3202, loss = 1.61751249\n",
            "Iteration 3203, loss = 1.61470959\n",
            "Iteration 3204, loss = 1.61201105\n",
            "Iteration 3205, loss = 1.60833018\n",
            "Iteration 3206, loss = 1.60575121\n",
            "Iteration 3207, loss = 1.60310243\n",
            "Iteration 3208, loss = 1.60008072\n",
            "Iteration 3209, loss = 1.59665485\n",
            "Iteration 3210, loss = 1.59317741\n",
            "Iteration 3211, loss = 1.59086606\n",
            "Iteration 3212, loss = 1.58841654\n",
            "Iteration 3213, loss = 1.58510496\n",
            "Iteration 3214, loss = 1.58142501\n",
            "Iteration 3215, loss = 1.57991963\n",
            "Iteration 3216, loss = 1.57607223\n",
            "Iteration 3217, loss = 1.57393114\n",
            "Iteration 3218, loss = 1.57035914\n",
            "Iteration 3219, loss = 1.56679059\n",
            "Iteration 3220, loss = 1.56646034\n",
            "Iteration 3221, loss = 1.56277373\n",
            "Iteration 3222, loss = 1.56108633\n",
            "Iteration 3223, loss = 1.55604343\n",
            "Iteration 3224, loss = 1.55421911\n",
            "Iteration 3225, loss = 1.54975820\n",
            "Iteration 3226, loss = 1.54923636\n",
            "Iteration 3227, loss = 1.54621398\n",
            "Iteration 3228, loss = 1.54296302\n",
            "Iteration 3229, loss = 1.53865382\n",
            "Iteration 3230, loss = 1.53700678\n",
            "Iteration 3231, loss = 1.53332061\n",
            "Iteration 3232, loss = 1.53088201\n",
            "Iteration 3233, loss = 1.52831211\n",
            "Iteration 3234, loss = 1.52502174\n",
            "Iteration 3235, loss = 1.52535400\n",
            "Iteration 3236, loss = 1.52634325\n",
            "Iteration 3237, loss = 1.51766618\n",
            "Iteration 3238, loss = 1.51404023\n",
            "Iteration 3239, loss = 1.51320799\n",
            "Iteration 3240, loss = 1.50967733\n",
            "Iteration 3241, loss = 1.50725081\n",
            "Iteration 3242, loss = 1.50312216\n",
            "Iteration 3243, loss = 1.49979805\n",
            "Iteration 3244, loss = 1.49952503\n",
            "Iteration 3245, loss = 1.49574001\n",
            "Iteration 3246, loss = 1.49186899\n",
            "Iteration 3247, loss = 1.49103468\n",
            "Iteration 3248, loss = 1.48816639\n",
            "Iteration 3249, loss = 1.48442266\n",
            "Iteration 3250, loss = 1.48110294\n",
            "Iteration 3251, loss = 1.47880906\n",
            "Iteration 3252, loss = 1.47609901\n",
            "Iteration 3253, loss = 1.47221912\n",
            "Iteration 3254, loss = 1.47258495\n",
            "Iteration 3255, loss = 1.46687168\n",
            "Iteration 3256, loss = 1.46623538\n",
            "Iteration 3257, loss = 1.46361465\n",
            "Iteration 3258, loss = 1.45969309\n",
            "Iteration 3259, loss = 1.45826138\n",
            "Iteration 3260, loss = 1.45567047\n",
            "Iteration 3261, loss = 1.45101812\n",
            "Iteration 3262, loss = 1.45223769\n",
            "Iteration 3263, loss = 1.44849490\n",
            "Iteration 3264, loss = 1.44394420\n",
            "Iteration 3265, loss = 1.44200543\n",
            "Iteration 3266, loss = 1.44023939\n",
            "Iteration 3267, loss = 1.43579408\n",
            "Iteration 3268, loss = 1.43262234\n",
            "Iteration 3269, loss = 1.43070329\n",
            "Iteration 3270, loss = 1.42797453\n",
            "Iteration 3271, loss = 1.42532322\n",
            "Iteration 3272, loss = 1.42298070\n",
            "Iteration 3273, loss = 1.42226041\n",
            "Iteration 3274, loss = 1.42028154\n",
            "Iteration 3275, loss = 1.41627315\n",
            "Iteration 3276, loss = 1.41419314\n",
            "Iteration 3277, loss = 1.41269285\n",
            "Iteration 3278, loss = 1.40731996\n",
            "Iteration 3279, loss = 1.40800249\n",
            "Iteration 3280, loss = 1.40302157\n",
            "Iteration 3281, loss = 1.40119400\n",
            "Iteration 3282, loss = 1.39914481\n",
            "Iteration 3283, loss = 1.39714160\n",
            "Iteration 3284, loss = 1.39667110\n",
            "Iteration 3285, loss = 1.39026499\n",
            "Iteration 3286, loss = 1.38688429\n",
            "Iteration 3287, loss = 1.38548033\n",
            "Iteration 3288, loss = 1.38308009\n",
            "Iteration 3289, loss = 1.38030044\n",
            "Iteration 3290, loss = 1.37802129\n",
            "Iteration 3291, loss = 1.37632718\n",
            "Iteration 3292, loss = 1.37362788\n",
            "Iteration 3293, loss = 1.37100585\n",
            "Iteration 3294, loss = 1.36748033\n",
            "Iteration 3295, loss = 1.36557664\n",
            "Iteration 3296, loss = 1.36245630\n",
            "Iteration 3297, loss = 1.36131445\n",
            "Iteration 3298, loss = 1.35833474\n",
            "Iteration 3299, loss = 1.35514504\n",
            "Iteration 3300, loss = 1.35466971\n",
            "Iteration 3301, loss = 1.35148777\n",
            "Iteration 3302, loss = 1.34916536\n",
            "Iteration 3303, loss = 1.34938213\n",
            "Iteration 3304, loss = 1.34415474\n",
            "Iteration 3305, loss = 1.34089208\n",
            "Iteration 3306, loss = 1.33957404\n",
            "Iteration 3307, loss = 1.33623446\n",
            "Iteration 3308, loss = 1.33487455\n",
            "Iteration 3309, loss = 1.33165209\n",
            "Iteration 3310, loss = 1.32951382\n",
            "Iteration 3311, loss = 1.32996601\n",
            "Iteration 3312, loss = 1.32522526\n",
            "Iteration 3313, loss = 1.32264577\n",
            "Iteration 3314, loss = 1.32046148\n",
            "Iteration 3315, loss = 1.32046535\n",
            "Iteration 3316, loss = 1.31543854\n",
            "Iteration 3317, loss = 1.31522833\n",
            "Iteration 3318, loss = 1.31075183\n",
            "Iteration 3319, loss = 1.30859084\n",
            "Iteration 3320, loss = 1.30722630\n",
            "Iteration 3321, loss = 1.30448878\n",
            "Iteration 3322, loss = 1.30274948\n",
            "Iteration 3323, loss = 1.30064708\n",
            "Iteration 3324, loss = 1.29729750\n",
            "Iteration 3325, loss = 1.29636621\n",
            "Iteration 3326, loss = 1.29268779\n",
            "Iteration 3327, loss = 1.29048781\n",
            "Iteration 3328, loss = 1.28985003\n",
            "Iteration 3329, loss = 1.29168960\n",
            "Iteration 3330, loss = 1.28383071\n",
            "Iteration 3331, loss = 1.28181081\n",
            "Iteration 3332, loss = 1.27867807\n",
            "Iteration 3333, loss = 1.27730614\n",
            "Iteration 3334, loss = 1.27637970\n",
            "Iteration 3335, loss = 1.27203175\n",
            "Iteration 3336, loss = 1.27265877\n",
            "Iteration 3337, loss = 1.26880908\n",
            "Iteration 3338, loss = 1.26673677\n",
            "Iteration 3339, loss = 1.26428085\n",
            "Iteration 3340, loss = 1.26116294\n",
            "Iteration 3341, loss = 1.25941853\n",
            "Iteration 3342, loss = 1.25725445\n",
            "Iteration 3343, loss = 1.25422536\n",
            "Iteration 3344, loss = 1.25235898\n",
            "Iteration 3345, loss = 1.25143018\n",
            "Iteration 3346, loss = 1.24767387\n",
            "Iteration 3347, loss = 1.24667591\n",
            "Iteration 3348, loss = 1.24511894\n",
            "Iteration 3349, loss = 1.24119629\n",
            "Iteration 3350, loss = 1.23992342\n",
            "Iteration 3351, loss = 1.23760953\n",
            "Iteration 3352, loss = 1.23566624\n",
            "Iteration 3353, loss = 1.23308008\n",
            "Iteration 3354, loss = 1.23251517\n",
            "Iteration 3355, loss = 1.22886705\n",
            "Iteration 3356, loss = 1.22677491\n",
            "Iteration 3357, loss = 1.22577288\n",
            "Iteration 3358, loss = 1.22279504\n",
            "Iteration 3359, loss = 1.21968111\n",
            "Iteration 3360, loss = 1.21807551\n",
            "Iteration 3361, loss = 1.21590509\n",
            "Iteration 3362, loss = 1.21363509\n",
            "Iteration 3363, loss = 1.21219752\n",
            "Iteration 3364, loss = 1.20908753\n",
            "Iteration 3365, loss = 1.20783032\n",
            "Iteration 3366, loss = 1.20574551\n",
            "Iteration 3367, loss = 1.20654244\n",
            "Iteration 3368, loss = 1.20256084\n",
            "Iteration 3369, loss = 1.19931652\n",
            "Iteration 3370, loss = 1.19758384\n",
            "Iteration 3371, loss = 1.19451109\n",
            "Iteration 3372, loss = 1.19426145\n",
            "Iteration 3373, loss = 1.19333957\n",
            "Iteration 3374, loss = 1.18930369\n",
            "Iteration 3375, loss = 1.18705362\n",
            "Iteration 3376, loss = 1.18561762\n",
            "Iteration 3377, loss = 1.18275028\n",
            "Iteration 3378, loss = 1.18213715\n",
            "Iteration 3379, loss = 1.18330210\n",
            "Iteration 3380, loss = 1.17722304\n",
            "Iteration 3381, loss = 1.17459811\n",
            "Iteration 3382, loss = 1.17390159\n",
            "Iteration 3383, loss = 1.17048559\n",
            "Iteration 3384, loss = 1.17149805\n",
            "Iteration 3385, loss = 1.16702511\n",
            "Iteration 3386, loss = 1.16515232\n",
            "Iteration 3387, loss = 1.16429187\n",
            "Iteration 3388, loss = 1.16223959\n",
            "Iteration 3389, loss = 1.15930314\n",
            "Iteration 3390, loss = 1.15763804\n",
            "Iteration 3391, loss = 1.15712875\n",
            "Iteration 3392, loss = 1.15287900\n",
            "Iteration 3393, loss = 1.15137233\n",
            "Iteration 3394, loss = 1.14881907\n",
            "Iteration 3395, loss = 1.14714826\n",
            "Iteration 3396, loss = 1.14612891\n",
            "Iteration 3397, loss = 1.14317822\n",
            "Iteration 3398, loss = 1.14227793\n",
            "Iteration 3399, loss = 1.14044498\n",
            "Iteration 3400, loss = 1.13971867\n",
            "Iteration 3401, loss = 1.13458568\n",
            "Iteration 3402, loss = 1.13367132\n",
            "Iteration 3403, loss = 1.13356155\n",
            "Iteration 3404, loss = 1.13060944\n",
            "Iteration 3405, loss = 1.12956653\n",
            "Iteration 3406, loss = 1.12585142\n",
            "Iteration 3407, loss = 1.12938206\n",
            "Iteration 3408, loss = 1.12577856\n",
            "Iteration 3409, loss = 1.12281882\n",
            "Iteration 3410, loss = 1.12107978\n",
            "Iteration 3411, loss = 1.11527299\n",
            "Iteration 3412, loss = 1.11724476\n",
            "Iteration 3413, loss = 1.11523293\n",
            "Iteration 3414, loss = 1.11184349\n",
            "Iteration 3415, loss = 1.11012977\n",
            "Iteration 3416, loss = 1.10828813\n",
            "Iteration 3417, loss = 1.10536801\n",
            "Iteration 3418, loss = 1.10306780\n",
            "Iteration 3419, loss = 1.10133576\n",
            "Iteration 3420, loss = 1.09938780\n",
            "Iteration 3421, loss = 1.09918501\n",
            "Iteration 3422, loss = 1.09676202\n",
            "Iteration 3423, loss = 1.09508188\n",
            "Iteration 3424, loss = 1.09179656\n",
            "Iteration 3425, loss = 1.09179602\n",
            "Iteration 3426, loss = 1.08804464\n",
            "Iteration 3427, loss = 1.08876703\n",
            "Iteration 3428, loss = 1.08667717\n",
            "Iteration 3429, loss = 1.08334727\n",
            "Iteration 3430, loss = 1.08414892\n",
            "Iteration 3431, loss = 1.07825679\n",
            "Iteration 3432, loss = 1.07727861\n",
            "Iteration 3433, loss = 1.07507813\n",
            "Iteration 3434, loss = 1.07330710\n",
            "Iteration 3435, loss = 1.07392329\n",
            "Iteration 3436, loss = 1.06998725\n",
            "Iteration 3437, loss = 1.07108075\n",
            "Iteration 3438, loss = 1.06609853\n",
            "Iteration 3439, loss = 1.06497433\n",
            "Iteration 3440, loss = 1.06278401\n",
            "Iteration 3441, loss = 1.06132431\n",
            "Iteration 3442, loss = 1.05898389\n",
            "Iteration 3443, loss = 1.05707137\n",
            "Iteration 3444, loss = 1.05620315\n",
            "Iteration 3445, loss = 1.05353181\n",
            "Iteration 3446, loss = 1.05176000\n",
            "Iteration 3447, loss = 1.05031103\n",
            "Iteration 3448, loss = 1.04843914\n",
            "Iteration 3449, loss = 1.04733000\n",
            "Iteration 3450, loss = 1.04524810\n",
            "Iteration 3451, loss = 1.04251825\n",
            "Iteration 3452, loss = 1.04189721\n",
            "Iteration 3453, loss = 1.04104464\n",
            "Iteration 3454, loss = 1.03803146\n",
            "Iteration 3455, loss = 1.03631119\n",
            "Iteration 3456, loss = 1.03392572\n",
            "Iteration 3457, loss = 1.03602971\n",
            "Iteration 3458, loss = 1.03036222\n",
            "Iteration 3459, loss = 1.03078450\n",
            "Iteration 3460, loss = 1.03043810\n",
            "Iteration 3461, loss = 1.02470469\n",
            "Iteration 3462, loss = 1.02936368\n",
            "Iteration 3463, loss = 1.02603157\n",
            "Iteration 3464, loss = 1.02310423\n",
            "Iteration 3465, loss = 1.02735402\n",
            "Iteration 3466, loss = 1.01787322\n",
            "Iteration 3467, loss = 1.01476397\n",
            "Iteration 3468, loss = 1.01421055\n",
            "Iteration 3469, loss = 1.01117268\n",
            "Iteration 3470, loss = 1.01067570\n",
            "Iteration 3471, loss = 1.00801713\n",
            "Iteration 3472, loss = 1.00610393\n",
            "Iteration 3473, loss = 1.00744182\n",
            "Iteration 3474, loss = 1.00268757\n",
            "Iteration 3475, loss = 1.00395847\n",
            "Iteration 3476, loss = 1.00470085\n",
            "Iteration 3477, loss = 1.00050096\n",
            "Iteration 3478, loss = 0.99725946\n",
            "Iteration 3479, loss = 0.99466267\n",
            "Iteration 3480, loss = 0.99353636\n",
            "Iteration 3481, loss = 0.99254392\n",
            "Iteration 3482, loss = 0.98891387\n",
            "Iteration 3483, loss = 0.98960244\n",
            "Iteration 3484, loss = 0.98712716\n",
            "Iteration 3485, loss = 0.98416969\n",
            "Iteration 3486, loss = 0.98352922\n",
            "Iteration 3487, loss = 0.98208555\n",
            "Iteration 3488, loss = 0.98021641\n",
            "Iteration 3489, loss = 0.97887463\n",
            "Iteration 3490, loss = 0.97862232\n",
            "Iteration 3491, loss = 0.97518877\n",
            "Iteration 3492, loss = 0.97414512\n",
            "Iteration 3493, loss = 0.97147727\n",
            "Iteration 3494, loss = 0.97221871\n",
            "Iteration 3495, loss = 0.96863267\n",
            "Iteration 3496, loss = 0.96607445\n",
            "Iteration 3497, loss = 0.96576098\n",
            "Iteration 3498, loss = 0.96467788\n",
            "Iteration 3499, loss = 0.96247367\n",
            "Iteration 3500, loss = 0.96072683\n",
            "Iteration 3501, loss = 0.95861301\n",
            "Iteration 3502, loss = 0.95745456\n",
            "Iteration 3503, loss = 0.95498089\n",
            "Iteration 3504, loss = 0.95366469\n",
            "Iteration 3505, loss = 0.95211109\n",
            "Iteration 3506, loss = 0.95199797\n",
            "Iteration 3507, loss = 0.94992491\n",
            "Iteration 3508, loss = 0.94725416\n",
            "Iteration 3509, loss = 0.94828186\n",
            "Iteration 3510, loss = 0.94661771\n",
            "Iteration 3511, loss = 0.94495284\n",
            "Iteration 3512, loss = 0.94159809\n",
            "Iteration 3513, loss = 0.93994882\n",
            "Iteration 3514, loss = 0.93919115\n",
            "Iteration 3515, loss = 0.93737007\n",
            "Iteration 3516, loss = 0.93497238\n",
            "Iteration 3517, loss = 0.93424892\n",
            "Iteration 3518, loss = 0.93172521\n",
            "Iteration 3519, loss = 0.93314182\n",
            "Iteration 3520, loss = 0.92871194\n",
            "Iteration 3521, loss = 0.92917538\n",
            "Iteration 3522, loss = 0.92794650\n",
            "Iteration 3523, loss = 0.92584776\n",
            "Iteration 3524, loss = 0.92632949\n",
            "Iteration 3525, loss = 0.92231719\n",
            "Iteration 3526, loss = 0.92386271\n",
            "Iteration 3527, loss = 0.91963250\n",
            "Iteration 3528, loss = 0.91890472\n",
            "Iteration 3529, loss = 0.91674791\n",
            "Iteration 3530, loss = 0.91598319\n",
            "Iteration 3531, loss = 0.91557131\n",
            "Iteration 3532, loss = 0.91142574\n",
            "Iteration 3533, loss = 0.91176489\n",
            "Iteration 3534, loss = 0.90968393\n",
            "Iteration 3535, loss = 0.90705023\n",
            "Iteration 3536, loss = 0.90708361\n",
            "Iteration 3537, loss = 0.90420772\n",
            "Iteration 3538, loss = 0.90268350\n",
            "Iteration 3539, loss = 0.90165017\n",
            "Iteration 3540, loss = 0.89900843\n",
            "Iteration 3541, loss = 0.89899864\n",
            "Iteration 3542, loss = 0.89610658\n",
            "Iteration 3543, loss = 0.89489369\n",
            "Iteration 3544, loss = 0.89396361\n",
            "Iteration 3545, loss = 0.89159157\n",
            "Iteration 3546, loss = 0.89095471\n",
            "Iteration 3547, loss = 0.88885864\n",
            "Iteration 3548, loss = 0.88798297\n",
            "Iteration 3549, loss = 0.88671763\n",
            "Iteration 3550, loss = 0.88434101\n",
            "Iteration 3551, loss = 0.88389580\n",
            "Iteration 3552, loss = 0.88209903\n",
            "Iteration 3553, loss = 0.88192916\n",
            "Iteration 3554, loss = 0.87924951\n",
            "Iteration 3555, loss = 0.87743306\n",
            "Iteration 3556, loss = 0.87817264\n",
            "Iteration 3557, loss = 0.87630415\n",
            "Iteration 3558, loss = 0.87617762\n",
            "Iteration 3559, loss = 0.87175300\n",
            "Iteration 3560, loss = 0.87211909\n",
            "Iteration 3561, loss = 0.87010257\n",
            "Iteration 3562, loss = 0.86739454\n",
            "Iteration 3563, loss = 0.86856502\n",
            "Iteration 3564, loss = 0.86561036\n",
            "Iteration 3565, loss = 0.86353906\n",
            "Iteration 3566, loss = 0.86292844\n",
            "Iteration 3567, loss = 0.86153097\n",
            "Iteration 3568, loss = 0.85941030\n",
            "Iteration 3569, loss = 0.85832399\n",
            "Iteration 3570, loss = 0.85761325\n",
            "Iteration 3571, loss = 0.85567630\n",
            "Iteration 3572, loss = 0.85328917\n",
            "Iteration 3573, loss = 0.85228514\n",
            "Iteration 3574, loss = 0.85253559\n",
            "Iteration 3575, loss = 0.84910343\n",
            "Iteration 3576, loss = 0.84973131\n",
            "Iteration 3577, loss = 0.84909162\n",
            "Iteration 3578, loss = 0.84572488\n",
            "Iteration 3579, loss = 0.84944756\n",
            "Iteration 3580, loss = 0.84552578\n",
            "Iteration 3581, loss = 0.84502584\n",
            "Iteration 3582, loss = 0.84011500\n",
            "Iteration 3583, loss = 0.84208981\n",
            "Iteration 3584, loss = 0.83904106\n",
            "Iteration 3585, loss = 0.83680775\n",
            "Iteration 3586, loss = 0.83912826\n",
            "Iteration 3587, loss = 0.83337436\n",
            "Iteration 3588, loss = 0.83663705\n",
            "Iteration 3589, loss = 0.83318834\n",
            "Iteration 3590, loss = 0.83225659\n",
            "Iteration 3591, loss = 0.83046620\n",
            "Iteration 3592, loss = 0.82661615\n",
            "Iteration 3593, loss = 0.82675275\n",
            "Iteration 3594, loss = 0.82440505\n",
            "Iteration 3595, loss = 0.82424453\n",
            "Iteration 3596, loss = 0.82433570\n",
            "Iteration 3597, loss = 0.82177802\n",
            "Iteration 3598, loss = 0.82071686\n",
            "Iteration 3599, loss = 0.81933042\n",
            "Iteration 3600, loss = 0.81797453\n",
            "Iteration 3601, loss = 0.81560189\n",
            "Iteration 3602, loss = 0.81404585\n",
            "Iteration 3603, loss = 0.81266514\n",
            "Iteration 3604, loss = 0.81174226\n",
            "Iteration 3605, loss = 0.81143189\n",
            "Iteration 3606, loss = 0.80870072\n",
            "Iteration 3607, loss = 0.80740527\n",
            "Iteration 3608, loss = 0.80669510\n",
            "Iteration 3609, loss = 0.80463865\n",
            "Iteration 3610, loss = 0.80377167\n",
            "Iteration 3611, loss = 0.80212603\n",
            "Iteration 3612, loss = 0.80112903\n",
            "Iteration 3613, loss = 0.79983237\n",
            "Iteration 3614, loss = 0.79831625\n",
            "Iteration 3615, loss = 0.79682981\n",
            "Iteration 3616, loss = 0.79606113\n",
            "Iteration 3617, loss = 0.79467682\n",
            "Iteration 3618, loss = 0.79394916\n",
            "Iteration 3619, loss = 0.79206224\n",
            "Iteration 3620, loss = 0.79204878\n",
            "Iteration 3621, loss = 0.79019726\n",
            "Iteration 3622, loss = 0.78860029\n",
            "Iteration 3623, loss = 0.78833506\n",
            "Iteration 3624, loss = 0.78702132\n",
            "Iteration 3625, loss = 0.78502979\n",
            "Iteration 3626, loss = 0.78364893\n",
            "Iteration 3627, loss = 0.78344264\n",
            "Iteration 3628, loss = 0.78162757\n",
            "Iteration 3629, loss = 0.77971048\n",
            "Iteration 3630, loss = 0.77917612\n",
            "Iteration 3631, loss = 0.77798434\n",
            "Iteration 3632, loss = 0.77700870\n",
            "Iteration 3633, loss = 0.77481136\n",
            "Iteration 3634, loss = 0.77433483\n",
            "Iteration 3635, loss = 0.77326409\n",
            "Iteration 3636, loss = 0.77201534\n",
            "Iteration 3637, loss = 0.77109235\n",
            "Iteration 3638, loss = 0.76973251\n",
            "Iteration 3639, loss = 0.76970217\n",
            "Iteration 3640, loss = 0.76686724\n",
            "Iteration 3641, loss = 0.76729599\n",
            "Iteration 3642, loss = 0.76538310\n",
            "Iteration 3643, loss = 0.76204755\n",
            "Iteration 3644, loss = 0.76275146\n",
            "Iteration 3645, loss = 0.76153673\n",
            "Iteration 3646, loss = 0.76351989\n",
            "Iteration 3647, loss = 0.75903743\n",
            "Iteration 3648, loss = 0.75750585\n",
            "Iteration 3649, loss = 0.75658525\n",
            "Iteration 3650, loss = 0.75465458\n",
            "Iteration 3651, loss = 0.75440868\n",
            "Iteration 3652, loss = 0.75272593\n",
            "Iteration 3653, loss = 0.75207635\n",
            "Iteration 3654, loss = 0.75155900\n",
            "Iteration 3655, loss = 0.74917265\n",
            "Iteration 3656, loss = 0.74805487\n",
            "Iteration 3657, loss = 0.74709411\n",
            "Iteration 3658, loss = 0.74580404\n",
            "Iteration 3659, loss = 0.74478671\n",
            "Iteration 3660, loss = 0.74370325\n",
            "Iteration 3661, loss = 0.74241133\n",
            "Iteration 3662, loss = 0.74317500\n",
            "Iteration 3663, loss = 0.74082567\n",
            "Iteration 3664, loss = 0.73971814\n",
            "Iteration 3665, loss = 0.73812334\n",
            "Iteration 3666, loss = 0.73670213\n",
            "Iteration 3667, loss = 0.73668107\n",
            "Iteration 3668, loss = 0.73469843\n",
            "Iteration 3669, loss = 0.73307858\n",
            "Iteration 3670, loss = 0.73244132\n",
            "Iteration 3671, loss = 0.73117785\n",
            "Iteration 3672, loss = 0.73027590\n",
            "Iteration 3673, loss = 0.72898025\n",
            "Iteration 3674, loss = 0.72756077\n",
            "Iteration 3675, loss = 0.72720499\n",
            "Iteration 3676, loss = 0.72750549\n",
            "Iteration 3677, loss = 0.72581006\n",
            "Iteration 3678, loss = 0.72490849\n",
            "Iteration 3679, loss = 0.72341467\n",
            "Iteration 3680, loss = 0.72102256\n",
            "Iteration 3681, loss = 0.72149370\n",
            "Iteration 3682, loss = 0.71965943\n",
            "Iteration 3683, loss = 0.71904250\n",
            "Iteration 3684, loss = 0.71757738\n",
            "Iteration 3685, loss = 0.71958178\n",
            "Iteration 3686, loss = 0.71650776\n",
            "Iteration 3687, loss = 0.71542781\n",
            "Iteration 3688, loss = 0.71369700\n",
            "Iteration 3689, loss = 0.71295772\n",
            "Iteration 3690, loss = 0.71170089\n",
            "Iteration 3691, loss = 0.70892467\n",
            "Iteration 3692, loss = 0.71181333\n",
            "Iteration 3693, loss = 0.70822297\n",
            "Iteration 3694, loss = 0.70801738\n",
            "Iteration 3695, loss = 0.70575064\n",
            "Iteration 3696, loss = 0.70452191\n",
            "Iteration 3697, loss = 0.70472063\n",
            "Iteration 3698, loss = 0.70186299\n",
            "Iteration 3699, loss = 0.70201437\n",
            "Iteration 3700, loss = 0.70119162\n",
            "Iteration 3701, loss = 0.69874215\n",
            "Iteration 3702, loss = 0.69811251\n",
            "Iteration 3703, loss = 0.69778844\n",
            "Iteration 3704, loss = 0.69727487\n",
            "Iteration 3705, loss = 0.69620123\n",
            "Iteration 3706, loss = 0.69452300\n",
            "Iteration 3707, loss = 0.69313073\n",
            "Iteration 3708, loss = 0.69176611\n",
            "Iteration 3709, loss = 0.69169090\n",
            "Iteration 3710, loss = 0.68953901\n",
            "Iteration 3711, loss = 0.68862432\n",
            "Iteration 3712, loss = 0.68839373\n",
            "Iteration 3713, loss = 0.68738950\n",
            "Iteration 3714, loss = 0.68568857\n",
            "Iteration 3715, loss = 0.68544008\n",
            "Iteration 3716, loss = 0.68457763\n",
            "Iteration 3717, loss = 0.68315871\n",
            "Iteration 3718, loss = 0.68196320\n",
            "Iteration 3719, loss = 0.68133439\n",
            "Iteration 3720, loss = 0.67999139\n",
            "Iteration 3721, loss = 0.67866151\n",
            "Iteration 3722, loss = 0.67787740\n",
            "Iteration 3723, loss = 0.67691088\n",
            "Iteration 3724, loss = 0.67537137\n",
            "Iteration 3725, loss = 0.67442336\n",
            "Iteration 3726, loss = 0.67383829\n",
            "Iteration 3727, loss = 0.67267373\n",
            "Iteration 3728, loss = 0.67161808\n",
            "Iteration 3729, loss = 0.67083634\n",
            "Iteration 3730, loss = 0.67105900\n",
            "Iteration 3731, loss = 0.66903814\n",
            "Iteration 3732, loss = 0.66740481\n",
            "Iteration 3733, loss = 0.66647846\n",
            "Iteration 3734, loss = 0.66619456\n",
            "Iteration 3735, loss = 0.66478638\n",
            "Iteration 3736, loss = 0.66475359\n",
            "Iteration 3737, loss = 0.66370985\n",
            "Iteration 3738, loss = 0.66294608\n",
            "Iteration 3739, loss = 0.66145219\n",
            "Iteration 3740, loss = 0.65982580\n",
            "Iteration 3741, loss = 0.65982561\n",
            "Iteration 3742, loss = 0.65785741\n",
            "Iteration 3743, loss = 0.65913644\n",
            "Iteration 3744, loss = 0.65814932\n",
            "Iteration 3745, loss = 0.65561682\n",
            "Iteration 3746, loss = 0.65433033\n",
            "Iteration 3747, loss = 0.65276659\n",
            "Iteration 3748, loss = 0.65303625\n",
            "Iteration 3749, loss = 0.65100813\n",
            "Iteration 3750, loss = 0.65067899\n",
            "Iteration 3751, loss = 0.64929517\n",
            "Iteration 3752, loss = 0.64843173\n",
            "Iteration 3753, loss = 0.64863940\n",
            "Iteration 3754, loss = 0.64714579\n",
            "Iteration 3755, loss = 0.64640905\n",
            "Iteration 3756, loss = 0.64458722\n",
            "Iteration 3757, loss = 0.64604245\n",
            "Iteration 3758, loss = 0.64350840\n",
            "Iteration 3759, loss = 0.64218335\n",
            "Iteration 3760, loss = 0.64176331\n",
            "Iteration 3761, loss = 0.64081296\n",
            "Iteration 3762, loss = 0.64022541\n",
            "Iteration 3763, loss = 0.63866983\n",
            "Iteration 3764, loss = 0.63795325\n",
            "Iteration 3765, loss = 0.63669946\n",
            "Iteration 3766, loss = 0.63547666\n",
            "Iteration 3767, loss = 0.63496115\n",
            "Iteration 3768, loss = 0.63470736\n",
            "Iteration 3769, loss = 0.63437656\n",
            "Iteration 3770, loss = 0.63182239\n",
            "Iteration 3771, loss = 0.63190423\n",
            "Iteration 3772, loss = 0.63045410\n",
            "Iteration 3773, loss = 0.62921487\n",
            "Iteration 3774, loss = 0.63014974\n",
            "Iteration 3775, loss = 0.62940987\n",
            "Iteration 3776, loss = 0.62772370\n",
            "Iteration 3777, loss = 0.62605438\n",
            "Iteration 3778, loss = 0.62557087\n",
            "Iteration 3779, loss = 0.62490606\n",
            "Iteration 3780, loss = 0.62332383\n",
            "Iteration 3781, loss = 0.62286199\n",
            "Iteration 3782, loss = 0.62151040\n",
            "Iteration 3783, loss = 0.62207715\n",
            "Iteration 3784, loss = 0.62035137\n",
            "Iteration 3785, loss = 0.62010464\n",
            "Iteration 3786, loss = 0.61909046\n",
            "Iteration 3787, loss = 0.61823161\n",
            "Iteration 3788, loss = 0.61952609\n",
            "Iteration 3789, loss = 0.61597727\n",
            "Iteration 3790, loss = 0.61546486\n",
            "Iteration 3791, loss = 0.61396411\n",
            "Iteration 3792, loss = 0.61293292\n",
            "Iteration 3793, loss = 0.61259979\n",
            "Iteration 3794, loss = 0.61137311\n",
            "Iteration 3795, loss = 0.61129420\n",
            "Iteration 3796, loss = 0.61062460\n",
            "Iteration 3797, loss = 0.60924086\n",
            "Iteration 3798, loss = 0.60940639\n",
            "Iteration 3799, loss = 0.60814344\n",
            "Iteration 3800, loss = 0.60832658\n",
            "Iteration 3801, loss = 0.60553055\n",
            "Iteration 3802, loss = 0.60445188\n",
            "Iteration 3803, loss = 0.60377027\n",
            "Iteration 3804, loss = 0.60273492\n",
            "Iteration 3805, loss = 0.60208583\n",
            "Iteration 3806, loss = 0.60068517\n",
            "Iteration 3807, loss = 0.60021701\n",
            "Iteration 3808, loss = 0.59917438\n",
            "Iteration 3809, loss = 0.59846962\n",
            "Iteration 3810, loss = 0.59805085\n",
            "Iteration 3811, loss = 0.59713134\n",
            "Iteration 3812, loss = 0.59608964\n",
            "Iteration 3813, loss = 0.59636432\n",
            "Iteration 3814, loss = 0.59602529\n",
            "Iteration 3815, loss = 0.59427843\n",
            "Iteration 3816, loss = 0.59318051\n",
            "Iteration 3817, loss = 0.59409662\n",
            "Iteration 3818, loss = 0.59156706\n",
            "Iteration 3819, loss = 0.59088067\n",
            "Iteration 3820, loss = 0.58985267\n",
            "Iteration 3821, loss = 0.58935500\n",
            "Iteration 3822, loss = 0.58764584\n",
            "Iteration 3823, loss = 0.58711594\n",
            "Iteration 3824, loss = 0.58614373\n",
            "Iteration 3825, loss = 0.58529876\n",
            "Iteration 3826, loss = 0.58536305\n",
            "Iteration 3827, loss = 0.58460780\n",
            "Iteration 3828, loss = 0.58268418\n",
            "Iteration 3829, loss = 0.58189519\n",
            "Iteration 3830, loss = 0.58162510\n",
            "Iteration 3831, loss = 0.58061573\n",
            "Iteration 3832, loss = 0.58007884\n",
            "Iteration 3833, loss = 0.57891861\n",
            "Iteration 3834, loss = 0.57784558\n",
            "Iteration 3835, loss = 0.57771946\n",
            "Iteration 3836, loss = 0.57752885\n",
            "Iteration 3837, loss = 0.57637639\n",
            "Iteration 3838, loss = 0.57429198\n",
            "Iteration 3839, loss = 0.57471802\n",
            "Iteration 3840, loss = 0.57368393\n",
            "Iteration 3841, loss = 0.57238232\n",
            "Iteration 3842, loss = 0.57294511\n",
            "Iteration 3843, loss = 0.57087085\n",
            "Iteration 3844, loss = 0.57145376\n",
            "Iteration 3845, loss = 0.57023006\n",
            "Iteration 3846, loss = 0.56846303\n",
            "Iteration 3847, loss = 0.56777702\n",
            "Iteration 3848, loss = 0.56664487\n",
            "Iteration 3849, loss = 0.56721743\n",
            "Iteration 3850, loss = 0.56506800\n",
            "Iteration 3851, loss = 0.56436692\n",
            "Iteration 3852, loss = 0.56335514\n",
            "Iteration 3853, loss = 0.56264991\n",
            "Iteration 3854, loss = 0.56191094\n",
            "Iteration 3855, loss = 0.56030632\n",
            "Iteration 3856, loss = 0.56035541\n",
            "Iteration 3857, loss = 0.55914783\n",
            "Iteration 3858, loss = 0.55901681\n",
            "Iteration 3859, loss = 0.55708131\n",
            "Iteration 3860, loss = 0.55660923\n",
            "Iteration 3861, loss = 0.55892707\n",
            "Iteration 3862, loss = 0.55527205\n",
            "Iteration 3863, loss = 0.55579484\n",
            "Iteration 3864, loss = 0.55401858\n",
            "Iteration 3865, loss = 0.55441628\n",
            "Iteration 3866, loss = 0.55247118\n",
            "Iteration 3867, loss = 0.55297012\n",
            "Iteration 3868, loss = 0.55229927\n",
            "Iteration 3869, loss = 0.55016411\n",
            "Iteration 3870, loss = 0.55141436\n",
            "Iteration 3871, loss = 0.54725697\n",
            "Iteration 3872, loss = 0.55070245\n",
            "Iteration 3873, loss = 0.54975322\n",
            "Iteration 3874, loss = 0.54645311\n",
            "Iteration 3875, loss = 0.54549246\n",
            "Iteration 3876, loss = 0.54477247\n",
            "Iteration 3877, loss = 0.54313247\n",
            "Iteration 3878, loss = 0.54353327\n",
            "Iteration 3879, loss = 0.54264447\n",
            "Iteration 3880, loss = 0.54097194\n",
            "Iteration 3881, loss = 0.54125185\n",
            "Iteration 3882, loss = 0.53958845\n",
            "Iteration 3883, loss = 0.53889608\n",
            "Iteration 3884, loss = 0.53801681\n",
            "Iteration 3885, loss = 0.53724406\n",
            "Iteration 3886, loss = 0.53700644\n",
            "Iteration 3887, loss = 0.53610683\n",
            "Iteration 3888, loss = 0.53731564\n",
            "Iteration 3889, loss = 0.53383230\n",
            "Iteration 3890, loss = 0.53447659\n",
            "Iteration 3891, loss = 0.53497779\n",
            "Iteration 3892, loss = 0.53144356\n",
            "Iteration 3893, loss = 0.53133164\n",
            "Iteration 3894, loss = 0.52999181\n",
            "Iteration 3895, loss = 0.53064296\n",
            "Iteration 3896, loss = 0.52904366\n",
            "Iteration 3897, loss = 0.52853297\n",
            "Iteration 3898, loss = 0.53016314\n",
            "Iteration 3899, loss = 0.52733234\n",
            "Iteration 3900, loss = 0.52550659\n",
            "Iteration 3901, loss = 0.52722648\n",
            "Iteration 3902, loss = 0.52555176\n",
            "Iteration 3903, loss = 0.52457273\n",
            "Iteration 3904, loss = 0.52233680\n",
            "Iteration 3905, loss = 0.52387530\n",
            "Iteration 3906, loss = 0.52177639\n",
            "Iteration 3907, loss = 0.52280197\n",
            "Iteration 3908, loss = 0.52242536\n",
            "Iteration 3909, loss = 0.52076800\n",
            "Iteration 3910, loss = 0.52049614\n",
            "Iteration 3911, loss = 0.51855281\n",
            "Iteration 3912, loss = 0.51879442\n",
            "Iteration 3913, loss = 0.51722064\n",
            "Iteration 3914, loss = 0.51632899\n",
            "Iteration 3915, loss = 0.51764758\n",
            "Iteration 3916, loss = 0.51400791\n",
            "Iteration 3917, loss = 0.51459740\n",
            "Iteration 3918, loss = 0.51321956\n",
            "Iteration 3919, loss = 0.51330119\n",
            "Iteration 3920, loss = 0.51273616\n",
            "Iteration 3921, loss = 0.51038320\n",
            "Iteration 3922, loss = 0.51088014\n",
            "Iteration 3923, loss = 0.50883074\n",
            "Iteration 3924, loss = 0.51191179\n",
            "Iteration 3925, loss = 0.50804598\n",
            "Iteration 3926, loss = 0.50943929\n",
            "Iteration 3927, loss = 0.50935531\n",
            "Iteration 3928, loss = 0.50617662\n",
            "Iteration 3929, loss = 0.50680550\n",
            "Iteration 3930, loss = 0.50630557\n",
            "Iteration 3931, loss = 0.50357631\n",
            "Iteration 3932, loss = 0.50296476\n",
            "Iteration 3933, loss = 0.50359550\n",
            "Iteration 3934, loss = 0.50236239\n",
            "Iteration 3935, loss = 0.50287897\n",
            "Iteration 3936, loss = 0.50020012\n",
            "Iteration 3937, loss = 0.49974774\n",
            "Iteration 3938, loss = 0.49976478\n",
            "Iteration 3939, loss = 0.49822693\n",
            "Iteration 3940, loss = 0.49775818\n",
            "Iteration 3941, loss = 0.49745017\n",
            "Iteration 3942, loss = 0.49636956\n",
            "Iteration 3943, loss = 0.49534835\n",
            "Iteration 3944, loss = 0.49488122\n",
            "Iteration 3945, loss = 0.49412498\n",
            "Iteration 3946, loss = 0.49387572\n",
            "Iteration 3947, loss = 0.49273834\n",
            "Iteration 3948, loss = 0.49252632\n",
            "Iteration 3949, loss = 0.49350638\n",
            "Iteration 3950, loss = 0.49147797\n",
            "Iteration 3951, loss = 0.49107330\n",
            "Iteration 3952, loss = 0.49054476\n",
            "Iteration 3953, loss = 0.49054426\n",
            "Iteration 3954, loss = 0.48927411\n",
            "Iteration 3955, loss = 0.49009943\n",
            "Iteration 3956, loss = 0.48681631\n",
            "Iteration 3957, loss = 0.48822526\n",
            "Iteration 3958, loss = 0.48609105\n",
            "Iteration 3959, loss = 0.48644926\n",
            "Iteration 3960, loss = 0.48576732\n",
            "Iteration 3961, loss = 0.48446891\n",
            "Iteration 3962, loss = 0.48373026\n",
            "Iteration 3963, loss = 0.48296180\n",
            "Iteration 3964, loss = 0.48282230\n",
            "Iteration 3965, loss = 0.48173626\n",
            "Iteration 3966, loss = 0.48049560\n",
            "Iteration 3967, loss = 0.48025256\n",
            "Iteration 3968, loss = 0.48019690\n",
            "Iteration 3969, loss = 0.47925113\n",
            "Iteration 3970, loss = 0.47846669\n",
            "Iteration 3971, loss = 0.47761723\n",
            "Iteration 3972, loss = 0.47743734\n",
            "Iteration 3973, loss = 0.47689158\n",
            "Iteration 3974, loss = 0.47677223\n",
            "Iteration 3975, loss = 0.47515014\n",
            "Iteration 3976, loss = 0.47553127\n",
            "Iteration 3977, loss = 0.47427226\n",
            "Iteration 3978, loss = 0.47284898\n",
            "Iteration 3979, loss = 0.47241211\n",
            "Iteration 3980, loss = 0.47358747\n",
            "Iteration 3981, loss = 0.47223652\n",
            "Iteration 3982, loss = 0.47076210\n",
            "Iteration 3983, loss = 0.47180909\n",
            "Iteration 3984, loss = 0.46898342\n",
            "Iteration 3985, loss = 0.47150943\n",
            "Iteration 3986, loss = 0.46847682\n",
            "Iteration 3987, loss = 0.46913548\n",
            "Iteration 3988, loss = 0.47147335\n",
            "Iteration 3989, loss = 0.46515162\n",
            "Iteration 3990, loss = 0.47119817\n",
            "Iteration 3991, loss = 0.46589905\n",
            "Iteration 3992, loss = 0.47027503\n",
            "Iteration 3993, loss = 0.46490213\n",
            "Iteration 3994, loss = 0.47110135\n",
            "Iteration 3995, loss = 0.46577682\n",
            "Iteration 3996, loss = 0.46684092\n",
            "Iteration 3997, loss = 0.46445816\n",
            "Iteration 3998, loss = 0.46364336\n",
            "Iteration 3999, loss = 0.46391412\n",
            "Iteration 4000, loss = 0.46212770\n",
            "Iteration 4001, loss = 0.46632250\n",
            "Iteration 4002, loss = 0.45942611\n",
            "Iteration 4003, loss = 0.46345966\n",
            "Iteration 4004, loss = 0.46007490\n",
            "Iteration 4005, loss = 0.45890364\n",
            "Iteration 4006, loss = 0.45616332\n",
            "Iteration 4007, loss = 0.45605971\n",
            "Iteration 4008, loss = 0.45654362\n",
            "Iteration 4009, loss = 0.45461492\n",
            "Iteration 4010, loss = 0.45680807\n",
            "Iteration 4011, loss = 0.45364732\n",
            "Iteration 4012, loss = 0.45307971\n",
            "Iteration 4013, loss = 0.45269814\n",
            "Iteration 4014, loss = 0.45184909\n",
            "Iteration 4015, loss = 0.45141077\n",
            "Iteration 4016, loss = 0.45005667\n",
            "Iteration 4017, loss = 0.45019207\n",
            "Iteration 4018, loss = 0.44891229\n",
            "Iteration 4019, loss = 0.44949155\n",
            "Iteration 4020, loss = 0.45052840\n",
            "Iteration 4021, loss = 0.44778392\n",
            "Iteration 4022, loss = 0.44683688\n",
            "Iteration 4023, loss = 0.44650851\n",
            "Iteration 4024, loss = 0.44585967\n",
            "Iteration 4025, loss = 0.44573606\n",
            "Iteration 4026, loss = 0.44543913\n",
            "Iteration 4027, loss = 0.44390184\n",
            "Iteration 4028, loss = 0.44402814\n",
            "Iteration 4029, loss = 0.44299383\n",
            "Iteration 4030, loss = 0.44291933\n",
            "Iteration 4031, loss = 0.44243038\n",
            "Iteration 4032, loss = 0.44121840\n",
            "Iteration 4033, loss = 0.44201645\n",
            "Iteration 4034, loss = 0.44020321\n",
            "Iteration 4035, loss = 0.44019609\n",
            "Iteration 4036, loss = 0.43959725\n",
            "Iteration 4037, loss = 0.43884064\n",
            "Iteration 4038, loss = 0.43927868\n",
            "Iteration 4039, loss = 0.43743047\n",
            "Iteration 4040, loss = 0.43800359\n",
            "Iteration 4041, loss = 0.43721703\n",
            "Iteration 4042, loss = 0.43589287\n",
            "Iteration 4043, loss = 0.43705850\n",
            "Iteration 4044, loss = 0.43662824\n",
            "Iteration 4045, loss = 0.43559509\n",
            "Iteration 4046, loss = 0.43752762\n",
            "Iteration 4047, loss = 0.43383217\n",
            "Iteration 4048, loss = 0.43317018\n",
            "Iteration 4049, loss = 0.43416342\n",
            "Iteration 4050, loss = 0.43283814\n",
            "Iteration 4051, loss = 0.43131440\n",
            "Iteration 4052, loss = 0.43196885\n",
            "Iteration 4053, loss = 0.43141406\n",
            "Iteration 4054, loss = 0.42941438\n",
            "Iteration 4055, loss = 0.43045528\n",
            "Iteration 4056, loss = 0.43009842\n",
            "Iteration 4057, loss = 0.42768704\n",
            "Iteration 4058, loss = 0.42754324\n",
            "Iteration 4059, loss = 0.42703117\n",
            "Iteration 4060, loss = 0.42645642\n",
            "Iteration 4061, loss = 0.42668424\n",
            "Iteration 4062, loss = 0.42580943\n",
            "Iteration 4063, loss = 0.42575743\n",
            "Iteration 4064, loss = 0.42460711\n",
            "Iteration 4065, loss = 0.42429728\n",
            "Iteration 4066, loss = 0.42282902\n",
            "Iteration 4067, loss = 0.42345707\n",
            "Iteration 4068, loss = 0.42255166\n",
            "Iteration 4069, loss = 0.42161375\n",
            "Iteration 4070, loss = 0.42169079\n",
            "Iteration 4071, loss = 0.42073726\n",
            "Iteration 4072, loss = 0.42138118\n",
            "Iteration 4073, loss = 0.41944987\n",
            "Iteration 4074, loss = 0.42040600\n",
            "Iteration 4075, loss = 0.41941836\n",
            "Iteration 4076, loss = 0.41805652\n",
            "Iteration 4077, loss = 0.41816234\n",
            "Iteration 4078, loss = 0.41682601\n",
            "Iteration 4079, loss = 0.41731404\n",
            "Iteration 4080, loss = 0.41614665\n",
            "Iteration 4081, loss = 0.41730753\n",
            "Iteration 4082, loss = 0.41483313\n",
            "Iteration 4083, loss = 0.41489937\n",
            "Iteration 4084, loss = 0.41424585\n",
            "Iteration 4085, loss = 0.41322612\n",
            "Iteration 4086, loss = 0.41344018\n",
            "Iteration 4087, loss = 0.41207893\n",
            "Iteration 4088, loss = 0.41173181\n",
            "Iteration 4089, loss = 0.41184687\n",
            "Iteration 4090, loss = 0.41249842\n",
            "Iteration 4091, loss = 0.41125825\n",
            "Iteration 4092, loss = 0.40981672\n",
            "Iteration 4093, loss = 0.40995682\n",
            "Iteration 4094, loss = 0.40925971\n",
            "Iteration 4095, loss = 0.40809166\n",
            "Iteration 4096, loss = 0.40771223\n",
            "Iteration 4097, loss = 0.40735028\n",
            "Iteration 4098, loss = 0.40663062\n",
            "Iteration 4099, loss = 0.40605203\n",
            "Iteration 4100, loss = 0.40573014\n",
            "Iteration 4101, loss = 0.40547286\n",
            "Iteration 4102, loss = 0.40474953\n",
            "Iteration 4103, loss = 0.40448062\n",
            "Iteration 4104, loss = 0.40370671\n",
            "Iteration 4105, loss = 0.40311869\n",
            "Iteration 4106, loss = 0.40293964\n",
            "Iteration 4107, loss = 0.40274949\n",
            "Iteration 4108, loss = 0.40211381\n",
            "Iteration 4109, loss = 0.40239782\n",
            "Iteration 4110, loss = 0.40163812\n",
            "Iteration 4111, loss = 0.40149193\n",
            "Iteration 4112, loss = 0.40032577\n",
            "Iteration 4113, loss = 0.39983621\n",
            "Iteration 4114, loss = 0.39842741\n",
            "Iteration 4115, loss = 0.39850214\n",
            "Iteration 4116, loss = 0.39746367\n",
            "Iteration 4117, loss = 0.39712589\n",
            "Iteration 4118, loss = 0.39684097\n",
            "Iteration 4119, loss = 0.39634927\n",
            "Iteration 4120, loss = 0.39621966\n",
            "Iteration 4121, loss = 0.39566065\n",
            "Iteration 4122, loss = 0.39497697\n",
            "Iteration 4123, loss = 0.39526711\n",
            "Iteration 4124, loss = 0.39384444\n",
            "Iteration 4125, loss = 0.39390867\n",
            "Iteration 4126, loss = 0.39351007\n",
            "Iteration 4127, loss = 0.39246750\n",
            "Iteration 4128, loss = 0.39196847\n",
            "Iteration 4129, loss = 0.39176326\n",
            "Iteration 4130, loss = 0.39222861\n",
            "Iteration 4131, loss = 0.38990849\n",
            "Iteration 4132, loss = 0.39276134\n",
            "Iteration 4133, loss = 0.38971823\n",
            "Iteration 4134, loss = 0.39113218\n",
            "Iteration 4135, loss = 0.39162535\n",
            "Iteration 4136, loss = 0.39024572\n",
            "Iteration 4137, loss = 0.38816313\n",
            "Iteration 4138, loss = 0.38895932\n",
            "Iteration 4139, loss = 0.38643104\n",
            "Iteration 4140, loss = 0.38844424\n",
            "Iteration 4141, loss = 0.38532978\n",
            "Iteration 4142, loss = 0.38822355\n",
            "Iteration 4143, loss = 0.38494373\n",
            "Iteration 4144, loss = 0.38619377\n",
            "Iteration 4145, loss = 0.38602371\n",
            "Iteration 4146, loss = 0.38340999\n",
            "Iteration 4147, loss = 0.38282491\n",
            "Iteration 4148, loss = 0.38211634\n",
            "Iteration 4149, loss = 0.38190272\n",
            "Iteration 4150, loss = 0.38107695\n",
            "Iteration 4151, loss = 0.38182775\n",
            "Iteration 4152, loss = 0.38055410\n",
            "Iteration 4153, loss = 0.37965725\n",
            "Iteration 4154, loss = 0.38011265\n",
            "Iteration 4155, loss = 0.37877142\n",
            "Iteration 4156, loss = 0.37860172\n",
            "Iteration 4157, loss = 0.37759258\n",
            "Iteration 4158, loss = 0.37739599\n",
            "Iteration 4159, loss = 0.37718683\n",
            "Iteration 4160, loss = 0.37630241\n",
            "Iteration 4161, loss = 0.37819392\n",
            "Iteration 4162, loss = 0.37527614\n",
            "Iteration 4163, loss = 0.37598255\n",
            "Iteration 4164, loss = 0.37515716\n",
            "Iteration 4165, loss = 0.37483489\n",
            "Iteration 4166, loss = 0.37283541\n",
            "Iteration 4167, loss = 0.37466332\n",
            "Iteration 4168, loss = 0.37451057\n",
            "Iteration 4169, loss = 0.37289619\n",
            "Iteration 4170, loss = 0.37131528\n",
            "Iteration 4171, loss = 0.37270615\n",
            "Iteration 4172, loss = 0.37088183\n",
            "Iteration 4173, loss = 0.37238495\n",
            "Iteration 4174, loss = 0.37110012\n",
            "Iteration 4175, loss = 0.37158818\n",
            "Iteration 4176, loss = 0.37166777\n",
            "Iteration 4177, loss = 0.37191454\n",
            "Iteration 4178, loss = 0.36858833\n",
            "Iteration 4179, loss = 0.36718241\n",
            "Iteration 4180, loss = 0.36729270\n",
            "Iteration 4181, loss = 0.36637789\n",
            "Iteration 4182, loss = 0.36768856\n",
            "Iteration 4183, loss = 0.36507951\n",
            "Iteration 4184, loss = 0.36533415\n",
            "Iteration 4185, loss = 0.36572755\n",
            "Iteration 4186, loss = 0.36578766\n",
            "Iteration 4187, loss = 0.36672936\n",
            "Iteration 4188, loss = 0.36305194\n",
            "Iteration 4189, loss = 0.36251518\n",
            "Iteration 4190, loss = 0.36248829\n",
            "Iteration 4191, loss = 0.36129259\n",
            "Iteration 4192, loss = 0.36245056\n",
            "Iteration 4193, loss = 0.36108150\n",
            "Iteration 4194, loss = 0.36135853\n",
            "Iteration 4195, loss = 0.35989662\n",
            "Iteration 4196, loss = 0.35976356\n",
            "Iteration 4197, loss = 0.35883167\n",
            "Iteration 4198, loss = 0.35840198\n",
            "Iteration 4199, loss = 0.35893775\n",
            "Iteration 4200, loss = 0.35878665\n",
            "Iteration 4201, loss = 0.35869651\n",
            "Iteration 4202, loss = 0.35919679\n",
            "Iteration 4203, loss = 0.35591299\n",
            "Iteration 4204, loss = 0.35541226\n",
            "Iteration 4205, loss = 0.35474723\n",
            "Iteration 4206, loss = 0.35469623\n",
            "Iteration 4207, loss = 0.35426195\n",
            "Iteration 4208, loss = 0.35314306\n",
            "Iteration 4209, loss = 0.35279273\n",
            "Iteration 4210, loss = 0.35234046\n",
            "Iteration 4211, loss = 0.35161017\n",
            "Iteration 4212, loss = 0.35325320\n",
            "Iteration 4213, loss = 0.35163494\n",
            "Iteration 4214, loss = 0.35189430\n",
            "Iteration 4215, loss = 0.34978995\n",
            "Iteration 4216, loss = 0.35115075\n",
            "Iteration 4217, loss = 0.34950175\n",
            "Iteration 4218, loss = 0.34953238\n",
            "Iteration 4219, loss = 0.34900974\n",
            "Iteration 4220, loss = 0.34779302\n",
            "Iteration 4221, loss = 0.34694671\n",
            "Iteration 4222, loss = 0.34768746\n",
            "Iteration 4223, loss = 0.34633498\n",
            "Iteration 4224, loss = 0.34707399\n",
            "Iteration 4225, loss = 0.34622466\n",
            "Iteration 4226, loss = 0.34519829\n",
            "Iteration 4227, loss = 0.34589186\n",
            "Iteration 4228, loss = 0.34482130\n",
            "Iteration 4229, loss = 0.34484593\n",
            "Iteration 4230, loss = 0.34496236\n",
            "Iteration 4231, loss = 0.34376663\n",
            "Iteration 4232, loss = 0.34543590\n",
            "Iteration 4233, loss = 0.34221470\n",
            "Iteration 4234, loss = 0.34181191\n",
            "Iteration 4235, loss = 0.34069168\n",
            "Iteration 4236, loss = 0.34175635\n",
            "Iteration 4237, loss = 0.34038929\n",
            "Iteration 4238, loss = 0.33961733\n",
            "Iteration 4239, loss = 0.33934684\n",
            "Iteration 4240, loss = 0.33924455\n",
            "Iteration 4241, loss = 0.33983097\n",
            "Iteration 4242, loss = 0.33912657\n",
            "Iteration 4243, loss = 0.33988350\n",
            "Iteration 4244, loss = 0.33910245\n",
            "Iteration 4245, loss = 0.33652693\n",
            "Iteration 4246, loss = 0.33869899\n",
            "Iteration 4247, loss = 0.33609780\n",
            "Iteration 4248, loss = 0.33577676\n",
            "Iteration 4249, loss = 0.33530734\n",
            "Iteration 4250, loss = 0.33494997\n",
            "Iteration 4251, loss = 0.33408991\n",
            "Iteration 4252, loss = 0.33499196\n",
            "Iteration 4253, loss = 0.33327610\n",
            "Iteration 4254, loss = 0.33281443\n",
            "Iteration 4255, loss = 0.33263106\n",
            "Iteration 4256, loss = 0.33234604\n",
            "Iteration 4257, loss = 0.33254544\n",
            "Iteration 4258, loss = 0.33446794\n",
            "Iteration 4259, loss = 0.33205051\n",
            "Iteration 4260, loss = 0.33085096\n",
            "Iteration 4261, loss = 0.33210992\n",
            "Iteration 4262, loss = 0.33296138\n",
            "Iteration 4263, loss = 0.33281784\n",
            "Iteration 4264, loss = 0.32866028\n",
            "Iteration 4265, loss = 0.32831115\n",
            "Iteration 4266, loss = 0.32673233\n",
            "Iteration 4267, loss = 0.32768432\n",
            "Iteration 4268, loss = 0.32668828\n",
            "Iteration 4269, loss = 0.32612618\n",
            "Iteration 4270, loss = 0.32543164\n",
            "Iteration 4271, loss = 0.32488888\n",
            "Iteration 4272, loss = 0.32493912\n",
            "Iteration 4273, loss = 0.32413039\n",
            "Iteration 4274, loss = 0.32439900\n",
            "Iteration 4275, loss = 0.32334145\n",
            "Iteration 4276, loss = 0.32352708\n",
            "Iteration 4277, loss = 0.32358868\n",
            "Iteration 4278, loss = 0.32250270\n",
            "Iteration 4279, loss = 0.32139699\n",
            "Iteration 4280, loss = 0.32219447\n",
            "Iteration 4281, loss = 0.32255951\n",
            "Iteration 4282, loss = 0.32072220\n",
            "Iteration 4283, loss = 0.32088352\n",
            "Iteration 4284, loss = 0.32066585\n",
            "Iteration 4285, loss = 0.31913798\n",
            "Iteration 4286, loss = 0.31908789\n",
            "Iteration 4287, loss = 0.31822435\n",
            "Iteration 4288, loss = 0.31772286\n",
            "Iteration 4289, loss = 0.31753620\n",
            "Iteration 4290, loss = 0.31684050\n",
            "Iteration 4291, loss = 0.31752014\n",
            "Iteration 4292, loss = 0.31771378\n",
            "Iteration 4293, loss = 0.31681783\n",
            "Iteration 4294, loss = 0.31604314\n",
            "Iteration 4295, loss = 0.31533075\n",
            "Iteration 4296, loss = 0.31472887\n",
            "Iteration 4297, loss = 0.31396819\n",
            "Iteration 4298, loss = 0.31420564\n",
            "Iteration 4299, loss = 0.31322041\n",
            "Iteration 4300, loss = 0.31439645\n",
            "Iteration 4301, loss = 0.31374533\n",
            "Iteration 4302, loss = 0.31225201\n",
            "Iteration 4303, loss = 0.31321070\n",
            "Iteration 4304, loss = 0.31290575\n",
            "Iteration 4305, loss = 0.31239303\n",
            "Iteration 4306, loss = 0.30989517\n",
            "Iteration 4307, loss = 0.31157704\n",
            "Iteration 4308, loss = 0.31037023\n",
            "Iteration 4309, loss = 0.30992569\n",
            "Iteration 4310, loss = 0.30898677\n",
            "Iteration 4311, loss = 0.30908655\n",
            "Iteration 4312, loss = 0.30935385\n",
            "Iteration 4313, loss = 0.31199847\n",
            "Iteration 4314, loss = 0.30979823\n",
            "Iteration 4315, loss = 0.30817718\n",
            "Iteration 4316, loss = 0.30849614\n",
            "Iteration 4317, loss = 0.30746169\n",
            "Iteration 4318, loss = 0.30649963\n",
            "Iteration 4319, loss = 0.30562181\n",
            "Iteration 4320, loss = 0.30504357\n",
            "Iteration 4321, loss = 0.30468812\n",
            "Iteration 4322, loss = 0.30497355\n",
            "Iteration 4323, loss = 0.30368840\n",
            "Iteration 4324, loss = 0.30419566\n",
            "Iteration 4325, loss = 0.30340577\n",
            "Iteration 4326, loss = 0.30359095\n",
            "Iteration 4327, loss = 0.30329125\n",
            "Iteration 4328, loss = 0.30234149\n",
            "Iteration 4329, loss = 0.30167671\n",
            "Iteration 4330, loss = 0.30151828\n",
            "Iteration 4331, loss = 0.30153033\n",
            "Iteration 4332, loss = 0.30026069\n",
            "Iteration 4333, loss = 0.30041600\n",
            "Iteration 4334, loss = 0.30003970\n",
            "Iteration 4335, loss = 0.30065720\n",
            "Iteration 4336, loss = 0.29952681\n",
            "Iteration 4337, loss = 0.29967287\n",
            "Iteration 4338, loss = 0.29836110\n",
            "Iteration 4339, loss = 0.29969323\n",
            "Iteration 4340, loss = 0.29796183\n",
            "Iteration 4341, loss = 0.29739835\n",
            "Iteration 4342, loss = 0.29934355\n",
            "Iteration 4343, loss = 0.29676012\n",
            "Iteration 4344, loss = 0.29749444\n",
            "Iteration 4345, loss = 0.29655518\n",
            "Iteration 4346, loss = 0.29658847\n",
            "Iteration 4347, loss = 0.29545068\n",
            "Iteration 4348, loss = 0.29610209\n",
            "Iteration 4349, loss = 0.29680414\n",
            "Iteration 4350, loss = 0.29420184\n",
            "Iteration 4351, loss = 0.29598748\n",
            "Iteration 4352, loss = 0.29527625\n",
            "Iteration 4353, loss = 0.29385123\n",
            "Iteration 4354, loss = 0.29522927\n",
            "Iteration 4355, loss = 0.29382577\n",
            "Iteration 4356, loss = 0.29245354\n",
            "Iteration 4357, loss = 0.29207407\n",
            "Iteration 4358, loss = 0.29169533\n",
            "Iteration 4359, loss = 0.29125710\n",
            "Iteration 4360, loss = 0.29156036\n",
            "Iteration 4361, loss = 0.29313847\n",
            "Iteration 4362, loss = 0.29023774\n",
            "Iteration 4363, loss = 0.29132046\n",
            "Iteration 4364, loss = 0.28978273\n",
            "Iteration 4365, loss = 0.29169551\n",
            "Iteration 4366, loss = 0.28927831\n",
            "Iteration 4367, loss = 0.28865629\n",
            "Iteration 4368, loss = 0.28864485\n",
            "Iteration 4369, loss = 0.28800382\n",
            "Iteration 4370, loss = 0.28800294\n",
            "Iteration 4371, loss = 0.28727957\n",
            "Iteration 4372, loss = 0.28751898\n",
            "Iteration 4373, loss = 0.28671769\n",
            "Iteration 4374, loss = 0.28675105\n",
            "Iteration 4375, loss = 0.28670217\n",
            "Iteration 4376, loss = 0.28734378\n",
            "Iteration 4377, loss = 0.28652063\n",
            "Iteration 4378, loss = 0.28544800\n",
            "Iteration 4379, loss = 0.28494429\n",
            "Iteration 4380, loss = 0.28478047\n",
            "Iteration 4381, loss = 0.28427553\n",
            "Iteration 4382, loss = 0.28445541\n",
            "Iteration 4383, loss = 0.28348122\n",
            "Iteration 4384, loss = 0.28400424\n",
            "Iteration 4385, loss = 0.28271119\n",
            "Iteration 4386, loss = 0.28288011\n",
            "Iteration 4387, loss = 0.28289646\n",
            "Iteration 4388, loss = 0.28191047\n",
            "Iteration 4389, loss = 0.28182390\n",
            "Iteration 4390, loss = 0.28168028\n",
            "Iteration 4391, loss = 0.28310839\n",
            "Iteration 4392, loss = 0.28073564\n",
            "Iteration 4393, loss = 0.28173399\n",
            "Iteration 4394, loss = 0.28216124\n",
            "Iteration 4395, loss = 0.28270351\n",
            "Iteration 4396, loss = 0.28016478\n",
            "Iteration 4397, loss = 0.28023663\n",
            "Iteration 4398, loss = 0.27915832\n",
            "Iteration 4399, loss = 0.28043256\n",
            "Iteration 4400, loss = 0.28067102\n",
            "Iteration 4401, loss = 0.27908837\n",
            "Iteration 4402, loss = 0.27760187\n",
            "Iteration 4403, loss = 0.28011886\n",
            "Iteration 4404, loss = 0.27847536\n",
            "Iteration 4405, loss = 0.27744642\n",
            "Iteration 4406, loss = 0.27748944\n",
            "Iteration 4407, loss = 0.27660798\n",
            "Iteration 4408, loss = 0.27652576\n",
            "Iteration 4409, loss = 0.27700797\n",
            "Iteration 4410, loss = 0.27619967\n",
            "Iteration 4411, loss = 0.27605415\n",
            "Iteration 4412, loss = 0.27525916\n",
            "Iteration 4413, loss = 0.27486819\n",
            "Iteration 4414, loss = 0.27428572\n",
            "Iteration 4415, loss = 0.27565913\n",
            "Iteration 4416, loss = 0.27683350\n",
            "Iteration 4417, loss = 0.27352940\n",
            "Iteration 4418, loss = 0.27385002\n",
            "Iteration 4419, loss = 0.27295985\n",
            "Iteration 4420, loss = 0.27289579\n",
            "Iteration 4421, loss = 0.27333325\n",
            "Iteration 4422, loss = 0.27371406\n",
            "Iteration 4423, loss = 0.27226874\n",
            "Iteration 4424, loss = 0.27237672\n",
            "Iteration 4425, loss = 0.27187972\n",
            "Iteration 4426, loss = 0.27178617\n",
            "Iteration 4427, loss = 0.27166319\n",
            "Iteration 4428, loss = 0.27067933\n",
            "Iteration 4429, loss = 0.27047280\n",
            "Iteration 4430, loss = 0.27285227\n",
            "Iteration 4431, loss = 0.26973059\n",
            "Iteration 4432, loss = 0.27236446\n",
            "Iteration 4433, loss = 0.26915045\n",
            "Iteration 4434, loss = 0.27387204\n",
            "Iteration 4435, loss = 0.26991778\n",
            "Iteration 4436, loss = 0.27232208\n",
            "Iteration 4437, loss = 0.26869764\n",
            "Iteration 4438, loss = 0.27353101\n",
            "Iteration 4439, loss = 0.26766273\n",
            "Iteration 4440, loss = 0.27128105\n",
            "Iteration 4441, loss = 0.26773899\n",
            "Iteration 4442, loss = 0.26858059\n",
            "Iteration 4443, loss = 0.26708287\n",
            "Iteration 4444, loss = 0.26661707\n",
            "Iteration 4445, loss = 0.26611272\n",
            "Iteration 4446, loss = 0.26637313\n",
            "Iteration 4447, loss = 0.26683354\n",
            "Iteration 4448, loss = 0.26642914\n",
            "Iteration 4449, loss = 0.26741378\n",
            "Iteration 4450, loss = 0.26482087\n",
            "Iteration 4451, loss = 0.26487860\n",
            "Iteration 4452, loss = 0.26439634\n",
            "Iteration 4453, loss = 0.26574148\n",
            "Iteration 4454, loss = 0.26503894\n",
            "Iteration 4455, loss = 0.26530625\n",
            "Iteration 4456, loss = 0.26424155\n",
            "Iteration 4457, loss = 0.26610981\n",
            "Iteration 4458, loss = 0.26585920\n",
            "Iteration 4459, loss = 0.26371164\n",
            "Iteration 4460, loss = 0.26583476\n",
            "Iteration 4461, loss = 0.26218726\n",
            "Iteration 4462, loss = 0.26207666\n",
            "Iteration 4463, loss = 0.26216443\n",
            "Iteration 4464, loss = 0.26270291\n",
            "Iteration 4465, loss = 0.26165996\n",
            "Iteration 4466, loss = 0.26145520\n",
            "Iteration 4467, loss = 0.26121672\n",
            "Iteration 4468, loss = 0.26059641\n",
            "Iteration 4469, loss = 0.26077497\n",
            "Iteration 4470, loss = 0.26061310\n",
            "Iteration 4471, loss = 0.26053187\n",
            "Iteration 4472, loss = 0.26174682\n",
            "Iteration 4473, loss = 0.25965523\n",
            "Iteration 4474, loss = 0.25995856\n",
            "Iteration 4475, loss = 0.26005187\n",
            "Iteration 4476, loss = 0.25814685\n",
            "Iteration 4477, loss = 0.26049019\n",
            "Iteration 4478, loss = 0.25938875\n",
            "Iteration 4479, loss = 0.25964310\n",
            "Iteration 4480, loss = 0.25710575\n",
            "Iteration 4481, loss = 0.25830429\n",
            "Iteration 4482, loss = 0.25749904\n",
            "Iteration 4483, loss = 0.25714828\n",
            "Iteration 4484, loss = 0.25646365\n",
            "Iteration 4485, loss = 0.25686045\n",
            "Iteration 4486, loss = 0.25703158\n",
            "Iteration 4487, loss = 0.25725681\n",
            "Iteration 4488, loss = 0.25588475\n",
            "Iteration 4489, loss = 0.25691963\n",
            "Iteration 4490, loss = 0.25548459\n",
            "Iteration 4491, loss = 0.25597064\n",
            "Iteration 4492, loss = 0.25452123\n",
            "Iteration 4493, loss = 0.25639257\n",
            "Iteration 4494, loss = 0.25693370\n",
            "Iteration 4495, loss = 0.25554924\n",
            "Iteration 4496, loss = 0.25429322\n",
            "Iteration 4497, loss = 0.25480703\n",
            "Iteration 4498, loss = 0.25463495\n",
            "Iteration 4499, loss = 0.25632900\n",
            "Iteration 4500, loss = 0.25337627\n",
            "Iteration 4501, loss = 0.25562237\n",
            "Iteration 4502, loss = 0.25342897\n",
            "Iteration 4503, loss = 0.25268560\n",
            "Iteration 4504, loss = 0.25326936\n",
            "Iteration 4505, loss = 0.25264089\n",
            "Iteration 4506, loss = 0.25139961\n",
            "Iteration 4507, loss = 0.25355671\n",
            "Iteration 4508, loss = 0.25140565\n",
            "Iteration 4509, loss = 0.25182854\n",
            "Iteration 4510, loss = 0.25105358\n",
            "Iteration 4511, loss = 0.25151349\n",
            "Iteration 4512, loss = 0.25238042\n",
            "Iteration 4513, loss = 0.25202831\n",
            "Iteration 4514, loss = 0.25002678\n",
            "Iteration 4515, loss = 0.25244410\n",
            "Iteration 4516, loss = 0.25212811\n",
            "Iteration 4517, loss = 0.25183794\n",
            "Iteration 4518, loss = 0.25412869\n",
            "Iteration 4519, loss = 0.25540415\n",
            "Iteration 4520, loss = 0.24998166\n",
            "Iteration 4521, loss = 0.24813605\n",
            "Iteration 4522, loss = 0.25059641\n",
            "Iteration 4523, loss = 0.24815533\n",
            "Iteration 4524, loss = 0.25269164\n",
            "Iteration 4525, loss = 0.25002489\n",
            "Iteration 4526, loss = 0.25006616\n",
            "Iteration 4527, loss = 0.24774595\n",
            "Iteration 4528, loss = 0.24752059\n",
            "Iteration 4529, loss = 0.24693817\n",
            "Iteration 4530, loss = 0.24711832\n",
            "Iteration 4531, loss = 0.24667351\n",
            "Iteration 4532, loss = 0.24743567\n",
            "Iteration 4533, loss = 0.24679769\n",
            "Iteration 4534, loss = 0.24558229\n",
            "Iteration 4535, loss = 0.24578174\n",
            "Iteration 4536, loss = 0.24523720\n",
            "Iteration 4537, loss = 0.24745702\n",
            "Iteration 4538, loss = 0.24478128\n",
            "Iteration 4539, loss = 0.24549845\n",
            "Iteration 4540, loss = 0.24437991\n",
            "Iteration 4541, loss = 0.24445333\n",
            "Iteration 4542, loss = 0.24376023\n",
            "Iteration 4543, loss = 0.24416431\n",
            "Iteration 4544, loss = 0.24361557\n",
            "Iteration 4545, loss = 0.24497510\n",
            "Iteration 4546, loss = 0.24419941\n",
            "Iteration 4547, loss = 0.24388335\n",
            "Iteration 4548, loss = 0.24309519\n",
            "Iteration 4549, loss = 0.24270035\n",
            "Iteration 4550, loss = 0.24245866\n",
            "Iteration 4551, loss = 0.24264383\n",
            "Iteration 4552, loss = 0.24217675\n",
            "Iteration 4553, loss = 0.24199986\n",
            "Iteration 4554, loss = 0.24172808\n",
            "Iteration 4555, loss = 0.24196074\n",
            "Iteration 4556, loss = 0.24072961\n",
            "Iteration 4557, loss = 0.24143158\n",
            "Iteration 4558, loss = 0.24207637\n",
            "Iteration 4559, loss = 0.24276391\n",
            "Iteration 4560, loss = 0.24036680\n",
            "Iteration 4561, loss = 0.24015147\n",
            "Iteration 4562, loss = 0.24106122\n",
            "Iteration 4563, loss = 0.24036325\n",
            "Iteration 4564, loss = 0.24016372\n",
            "Iteration 4565, loss = 0.24129396\n",
            "Iteration 4566, loss = 0.23973120\n",
            "Iteration 4567, loss = 0.24027799\n",
            "Iteration 4568, loss = 0.24021152\n",
            "Iteration 4569, loss = 0.23905545\n",
            "Iteration 4570, loss = 0.23895575\n",
            "Iteration 4571, loss = 0.24048966\n",
            "Iteration 4572, loss = 0.23820608\n",
            "Iteration 4573, loss = 0.23905717\n",
            "Iteration 4574, loss = 0.23799054\n",
            "Iteration 4575, loss = 0.23740309\n",
            "Iteration 4576, loss = 0.23927622\n",
            "Iteration 4577, loss = 0.23728035\n",
            "Iteration 4578, loss = 0.23846628\n",
            "Iteration 4579, loss = 0.23737641\n",
            "Iteration 4580, loss = 0.23688301\n",
            "Iteration 4581, loss = 0.23755611\n",
            "Iteration 4582, loss = 0.23707493\n",
            "Iteration 4583, loss = 0.23652785\n",
            "Iteration 4584, loss = 0.23655846\n",
            "Iteration 4585, loss = 0.23558167\n",
            "Iteration 4586, loss = 0.23561374\n",
            "Iteration 4587, loss = 0.23537886\n",
            "Iteration 4588, loss = 0.23510538\n",
            "Iteration 4589, loss = 0.23650892\n",
            "Iteration 4590, loss = 0.23532631\n",
            "Iteration 4591, loss = 0.23590640\n",
            "Iteration 4592, loss = 0.23578438\n",
            "Iteration 4593, loss = 0.23656283\n",
            "Iteration 4594, loss = 0.23771919\n",
            "Iteration 4595, loss = 0.23484921\n",
            "Iteration 4596, loss = 0.23440150\n",
            "Iteration 4597, loss = 0.23415918\n",
            "Iteration 4598, loss = 0.23423243\n",
            "Iteration 4599, loss = 0.23437167\n",
            "Iteration 4600, loss = 0.23334457\n",
            "Iteration 4601, loss = 0.23305817\n",
            "Iteration 4602, loss = 0.23503602\n",
            "Iteration 4603, loss = 0.23288962\n",
            "Iteration 4604, loss = 0.23293747\n",
            "Iteration 4605, loss = 0.23256797\n",
            "Iteration 4606, loss = 0.23243278\n",
            "Iteration 4607, loss = 0.23153903\n",
            "Iteration 4608, loss = 0.23204501\n",
            "Iteration 4609, loss = 0.23102622\n",
            "Iteration 4610, loss = 0.23090199\n",
            "Iteration 4611, loss = 0.23123316\n",
            "Iteration 4612, loss = 0.23068889\n",
            "Iteration 4613, loss = 0.23068432\n",
            "Iteration 4614, loss = 0.23045343\n",
            "Iteration 4615, loss = 0.23029438\n",
            "Iteration 4616, loss = 0.22984972\n",
            "Iteration 4617, loss = 0.22997138\n",
            "Iteration 4618, loss = 0.22954997\n",
            "Iteration 4619, loss = 0.22955740\n",
            "Iteration 4620, loss = 0.23009369\n",
            "Iteration 4621, loss = 0.22940014\n",
            "Iteration 4622, loss = 0.22955596\n",
            "Iteration 4623, loss = 0.22913401\n",
            "Iteration 4624, loss = 0.22874269\n",
            "Iteration 4625, loss = 0.22888633\n",
            "Iteration 4626, loss = 0.22874693\n",
            "Iteration 4627, loss = 0.22823660\n",
            "Iteration 4628, loss = 0.22855086\n",
            "Iteration 4629, loss = 0.22918077\n",
            "Iteration 4630, loss = 0.22910965\n",
            "Iteration 4631, loss = 0.22961159\n",
            "Iteration 4632, loss = 0.22779398\n",
            "Iteration 4633, loss = 0.22714613\n",
            "Iteration 4634, loss = 0.23136039\n",
            "Iteration 4635, loss = 0.22674342\n",
            "Iteration 4636, loss = 0.23025652\n",
            "Iteration 4637, loss = 0.22725029\n",
            "Iteration 4638, loss = 0.22847108\n",
            "Iteration 4639, loss = 0.22582980\n",
            "Iteration 4640, loss = 0.22751364\n",
            "Iteration 4641, loss = 0.22609049\n",
            "Iteration 4642, loss = 0.22730190\n",
            "Iteration 4643, loss = 0.22522902\n",
            "Iteration 4644, loss = 0.22714710\n",
            "Iteration 4645, loss = 0.22663802\n",
            "Iteration 4646, loss = 0.22590570\n",
            "Iteration 4647, loss = 0.22461737\n",
            "Iteration 4648, loss = 0.22598021\n",
            "Iteration 4649, loss = 0.22432452\n",
            "Iteration 4650, loss = 0.22522775\n",
            "Iteration 4651, loss = 0.22438340\n",
            "Iteration 4652, loss = 0.22502675\n",
            "Iteration 4653, loss = 0.22422348\n",
            "Iteration 4654, loss = 0.22412967\n",
            "Iteration 4655, loss = 0.22438280\n",
            "Iteration 4656, loss = 0.22304802\n",
            "Iteration 4657, loss = 0.22361867\n",
            "Iteration 4658, loss = 0.22358934\n",
            "Iteration 4659, loss = 0.22281724\n",
            "Iteration 4660, loss = 0.22271128\n",
            "Iteration 4661, loss = 0.22279583\n",
            "Iteration 4662, loss = 0.22348372\n",
            "Iteration 4663, loss = 0.22352681\n",
            "Iteration 4664, loss = 0.22215148\n",
            "Iteration 4665, loss = 0.22182386\n",
            "Iteration 4666, loss = 0.22167288\n",
            "Iteration 4667, loss = 0.22149084\n",
            "Iteration 4668, loss = 0.22115299\n",
            "Iteration 4669, loss = 0.22157393\n",
            "Iteration 4670, loss = 0.22142583\n",
            "Iteration 4671, loss = 0.22102675\n",
            "Iteration 4672, loss = 0.22090867\n",
            "Iteration 4673, loss = 0.22075537\n",
            "Iteration 4674, loss = 0.22035855\n",
            "Iteration 4675, loss = 0.22040634\n",
            "Iteration 4676, loss = 0.22009983\n",
            "Iteration 4677, loss = 0.21963358\n",
            "Iteration 4678, loss = 0.22057610\n",
            "Iteration 4679, loss = 0.21969440\n",
            "Iteration 4680, loss = 0.22122564\n",
            "Iteration 4681, loss = 0.21892279\n",
            "Iteration 4682, loss = 0.21996226\n",
            "Iteration 4683, loss = 0.21891257\n",
            "Iteration 4684, loss = 0.21943758\n",
            "Iteration 4685, loss = 0.21873534\n",
            "Iteration 4686, loss = 0.22108604\n",
            "Iteration 4687, loss = 0.21856778\n",
            "Iteration 4688, loss = 0.22018860\n",
            "Iteration 4689, loss = 0.21865397\n",
            "Iteration 4690, loss = 0.21964455\n",
            "Iteration 4691, loss = 0.21717409\n",
            "Iteration 4692, loss = 0.21936990\n",
            "Iteration 4693, loss = 0.21841761\n",
            "Iteration 4694, loss = 0.21832299\n",
            "Iteration 4695, loss = 0.21782159\n",
            "Iteration 4696, loss = 0.21768523\n",
            "Iteration 4697, loss = 0.21702647\n",
            "Iteration 4698, loss = 0.21720773\n",
            "Iteration 4699, loss = 0.21671897\n",
            "Iteration 4700, loss = 0.21734540\n",
            "Iteration 4701, loss = 0.21613936\n",
            "Iteration 4702, loss = 0.21620155\n",
            "Iteration 4703, loss = 0.21602376\n",
            "Iteration 4704, loss = 0.21589206\n",
            "Iteration 4705, loss = 0.21592431\n",
            "Iteration 4706, loss = 0.21561543\n",
            "Iteration 4707, loss = 0.21669391\n",
            "Iteration 4708, loss = 0.21522501\n",
            "Iteration 4709, loss = 0.21634857\n",
            "Iteration 4710, loss = 0.21539655\n",
            "Iteration 4711, loss = 0.21606987\n",
            "Iteration 4712, loss = 0.21594781\n",
            "Iteration 4713, loss = 0.21638372\n",
            "Iteration 4714, loss = 0.21432765\n",
            "Iteration 4715, loss = 0.21682256\n",
            "Iteration 4716, loss = 0.21600116\n",
            "Iteration 4717, loss = 0.21574863\n",
            "Iteration 4718, loss = 0.21354660\n",
            "Iteration 4719, loss = 0.21551959\n",
            "Iteration 4720, loss = 0.21292822\n",
            "Iteration 4721, loss = 0.21632148\n",
            "Iteration 4722, loss = 0.21441046\n",
            "Iteration 4723, loss = 0.21608673\n",
            "Iteration 4724, loss = 0.21559883\n",
            "Iteration 4725, loss = 0.21577513\n",
            "Iteration 4726, loss = 0.21679613\n",
            "Iteration 4727, loss = 0.21404475\n",
            "Iteration 4728, loss = 0.21758705\n",
            "Iteration 4729, loss = 0.21290322\n",
            "Iteration 4730, loss = 0.21649750\n",
            "Iteration 4731, loss = 0.21195235\n",
            "Iteration 4732, loss = 0.21630990\n",
            "Iteration 4733, loss = 0.21266710\n",
            "Iteration 4734, loss = 0.21501736\n",
            "Iteration 4735, loss = 0.21165947\n",
            "Iteration 4736, loss = 0.21444049\n",
            "Iteration 4737, loss = 0.21071226\n",
            "Iteration 4738, loss = 0.21088633\n",
            "Iteration 4739, loss = 0.21030188\n",
            "Iteration 4740, loss = 0.21043873\n",
            "Iteration 4741, loss = 0.21014958\n",
            "Iteration 4742, loss = 0.21001331\n",
            "Iteration 4743, loss = 0.20983982\n",
            "Iteration 4744, loss = 0.21133880\n",
            "Iteration 4745, loss = 0.20973126\n",
            "Iteration 4746, loss = 0.21237043\n",
            "Iteration 4747, loss = 0.21139765\n",
            "Iteration 4748, loss = 0.21165873\n",
            "Iteration 4749, loss = 0.21047214\n",
            "Iteration 4750, loss = 0.21098200\n",
            "Iteration 4751, loss = 0.20885069\n",
            "Iteration 4752, loss = 0.21007361\n",
            "Iteration 4753, loss = 0.20876337\n",
            "Iteration 4754, loss = 0.21098102\n",
            "Iteration 4755, loss = 0.20769959\n",
            "Iteration 4756, loss = 0.21005691\n",
            "Iteration 4757, loss = 0.20792158\n",
            "Iteration 4758, loss = 0.20975204\n",
            "Iteration 4759, loss = 0.20894827\n",
            "Iteration 4760, loss = 0.20872135\n",
            "Iteration 4761, loss = 0.20881921\n",
            "Iteration 4762, loss = 0.20833807\n",
            "Iteration 4763, loss = 0.20906596\n",
            "Iteration 4764, loss = 0.20730955\n",
            "Iteration 4765, loss = 0.20862075\n",
            "Iteration 4766, loss = 0.20747934\n",
            "Iteration 4767, loss = 0.20783296\n",
            "Iteration 4768, loss = 0.20724803\n",
            "Iteration 4769, loss = 0.20790669\n",
            "Iteration 4770, loss = 0.20754371\n",
            "Iteration 4771, loss = 0.20741345\n",
            "Iteration 4772, loss = 0.20556977\n",
            "Iteration 4773, loss = 0.20693104\n",
            "Iteration 4774, loss = 0.20657173\n",
            "Iteration 4775, loss = 0.20681929\n",
            "Iteration 4776, loss = 0.20622074\n",
            "Iteration 4777, loss = 0.20595178\n",
            "Iteration 4778, loss = 0.20517965\n",
            "Iteration 4779, loss = 0.20603880\n",
            "Iteration 4780, loss = 0.20492197\n",
            "Iteration 4781, loss = 0.20586185\n",
            "Iteration 4782, loss = 0.20440040\n",
            "Iteration 4783, loss = 0.20560234\n",
            "Iteration 4784, loss = 0.20445115\n",
            "Iteration 4785, loss = 0.20456709\n",
            "Iteration 4786, loss = 0.20381968\n",
            "Iteration 4787, loss = 0.20426074\n",
            "Iteration 4788, loss = 0.20405648\n",
            "Iteration 4789, loss = 0.20466039\n",
            "Iteration 4790, loss = 0.20438105\n",
            "Iteration 4791, loss = 0.20360725\n",
            "Iteration 4792, loss = 0.20497103\n",
            "Iteration 4793, loss = 0.20291093\n",
            "Iteration 4794, loss = 0.20360382\n",
            "Iteration 4795, loss = 0.20383453\n",
            "Iteration 4796, loss = 0.20335081\n",
            "Iteration 4797, loss = 0.20237351\n",
            "Iteration 4798, loss = 0.20366457\n",
            "Iteration 4799, loss = 0.20302482\n",
            "Iteration 4800, loss = 0.20257134\n",
            "Iteration 4801, loss = 0.20285994\n",
            "Iteration 4802, loss = 0.20222142\n",
            "Iteration 4803, loss = 0.20198209\n",
            "Iteration 4804, loss = 0.20175634\n",
            "Iteration 4805, loss = 0.20139519\n",
            "Iteration 4806, loss = 0.20272037\n",
            "Iteration 4807, loss = 0.20103080\n",
            "Iteration 4808, loss = 0.20301094\n",
            "Iteration 4809, loss = 0.20156613\n",
            "Iteration 4810, loss = 0.20254538\n",
            "Iteration 4811, loss = 0.20066199\n",
            "Iteration 4812, loss = 0.20209945\n",
            "Iteration 4813, loss = 0.20127234\n",
            "Iteration 4814, loss = 0.20013994\n",
            "Iteration 4815, loss = 0.20132939\n",
            "Iteration 4816, loss = 0.20134853\n",
            "Iteration 4817, loss = 0.20059089\n",
            "Iteration 4818, loss = 0.20036579\n",
            "Iteration 4819, loss = 0.20045751\n",
            "Iteration 4820, loss = 0.19942411\n",
            "Iteration 4821, loss = 0.20027004\n",
            "Iteration 4822, loss = 0.19984584\n",
            "Iteration 4823, loss = 0.19981797\n",
            "Iteration 4824, loss = 0.19931278\n",
            "Iteration 4825, loss = 0.19976316\n",
            "Iteration 4826, loss = 0.19929681\n",
            "Iteration 4827, loss = 0.19951965\n",
            "Iteration 4828, loss = 0.19854472\n",
            "Iteration 4829, loss = 0.19972853\n",
            "Iteration 4830, loss = 0.19858132\n",
            "Iteration 4831, loss = 0.19842613\n",
            "Iteration 4832, loss = 0.19795013\n",
            "Iteration 4833, loss = 0.19783482\n",
            "Iteration 4834, loss = 0.19789922\n",
            "Iteration 4835, loss = 0.19788312\n",
            "Iteration 4836, loss = 0.19811937\n",
            "Iteration 4837, loss = 0.19758545\n",
            "Iteration 4838, loss = 0.19830077\n",
            "Iteration 4839, loss = 0.19725089\n",
            "Iteration 4840, loss = 0.19788461\n",
            "Iteration 4841, loss = 0.19798438\n",
            "Iteration 4842, loss = 0.19773149\n",
            "Iteration 4843, loss = 0.19659695\n",
            "Iteration 4844, loss = 0.19740123\n",
            "Iteration 4845, loss = 0.19616782\n",
            "Iteration 4846, loss = 0.19717693\n",
            "Iteration 4847, loss = 0.19624735\n",
            "Iteration 4848, loss = 0.19703473\n",
            "Iteration 4849, loss = 0.19644756\n",
            "Iteration 4850, loss = 0.19604620\n",
            "Iteration 4851, loss = 0.19706907\n",
            "Iteration 4852, loss = 0.19570883\n",
            "Iteration 4853, loss = 0.19659541\n",
            "Iteration 4854, loss = 0.19583475\n",
            "Iteration 4855, loss = 0.19706169\n",
            "Iteration 4856, loss = 0.19510409\n",
            "Iteration 4857, loss = 0.20035827\n",
            "Iteration 4858, loss = 0.19780393\n",
            "Iteration 4859, loss = 0.19726336\n",
            "Iteration 4860, loss = 0.19647993\n",
            "Iteration 4861, loss = 0.19499002\n",
            "Iteration 4862, loss = 0.19626379\n",
            "Iteration 4863, loss = 0.19638872\n",
            "Iteration 4864, loss = 0.19533999\n",
            "Iteration 4865, loss = 0.19499111\n",
            "Iteration 4866, loss = 0.19476618\n",
            "Iteration 4867, loss = 0.19414476\n",
            "Iteration 4868, loss = 0.19410673\n",
            "Iteration 4869, loss = 0.19400967\n",
            "Iteration 4870, loss = 0.19346282\n",
            "Iteration 4871, loss = 0.19451332\n",
            "Iteration 4872, loss = 0.19359997\n",
            "Iteration 4873, loss = 0.19436334\n",
            "Iteration 4874, loss = 0.19264785\n",
            "Iteration 4875, loss = 0.19368712\n",
            "Iteration 4876, loss = 0.19252966\n",
            "Iteration 4877, loss = 0.19352047\n",
            "Iteration 4878, loss = 0.19210801\n",
            "Iteration 4879, loss = 0.19322419\n",
            "Iteration 4880, loss = 0.19344501\n",
            "Iteration 4881, loss = 0.19361962\n",
            "Iteration 4882, loss = 0.19148035\n",
            "Iteration 4883, loss = 0.19320962\n",
            "Iteration 4884, loss = 0.19170428\n",
            "Iteration 4885, loss = 0.19198092\n",
            "Iteration 4886, loss = 0.19215570\n",
            "Iteration 4887, loss = 0.19158417\n",
            "Iteration 4888, loss = 0.19117663\n",
            "Iteration 4889, loss = 0.19103488\n",
            "Iteration 4890, loss = 0.19099252\n",
            "Iteration 4891, loss = 0.19102835\n",
            "Iteration 4892, loss = 0.19144171\n",
            "Iteration 4893, loss = 0.19079987\n",
            "Iteration 4894, loss = 0.19091893\n",
            "Iteration 4895, loss = 0.19078254\n",
            "Iteration 4896, loss = 0.19066602\n",
            "Iteration 4897, loss = 0.19100135\n",
            "Iteration 4898, loss = 0.19075998\n",
            "Iteration 4899, loss = 0.18985199\n",
            "Iteration 4900, loss = 0.19065860\n",
            "Iteration 4901, loss = 0.18962918\n",
            "Iteration 4902, loss = 0.18997873\n",
            "Iteration 4903, loss = 0.19011313\n",
            "Iteration 4904, loss = 0.18904304\n",
            "Iteration 4905, loss = 0.18987275\n",
            "Iteration 4906, loss = 0.18951681\n",
            "Iteration 4907, loss = 0.19152623\n",
            "Iteration 4908, loss = 0.19111359\n",
            "Iteration 4909, loss = 0.19033011\n",
            "Iteration 4910, loss = 0.18956298\n",
            "Iteration 4911, loss = 0.18906229\n",
            "Iteration 4912, loss = 0.18988566\n",
            "Iteration 4913, loss = 0.18899389\n",
            "Iteration 4914, loss = 0.18987581\n",
            "Iteration 4915, loss = 0.18833706\n",
            "Iteration 4916, loss = 0.19090864\n",
            "Iteration 4917, loss = 0.18947223\n",
            "Iteration 4918, loss = 0.19065618\n",
            "Iteration 4919, loss = 0.18788976\n",
            "Iteration 4920, loss = 0.19024674\n",
            "Iteration 4921, loss = 0.18727022\n",
            "Iteration 4922, loss = 0.18967269\n",
            "Iteration 4923, loss = 0.18739250\n",
            "Iteration 4924, loss = 0.18988908\n",
            "Iteration 4925, loss = 0.18867915\n",
            "Iteration 4926, loss = 0.18964291\n",
            "Iteration 4927, loss = 0.18940587\n",
            "Iteration 4928, loss = 0.18719678\n",
            "Iteration 4929, loss = 0.19197776\n",
            "Iteration 4930, loss = 0.19180880\n",
            "Iteration 4931, loss = 0.18955296\n",
            "Iteration 4932, loss = 0.18721955\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPRegressor(activation='logistic', hidden_layer_sizes=(150, 20), max_iter=5000,\n",
              "             random_state=1, verbose=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-3 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-3 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-3 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-3 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-3 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-3 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-3 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-3 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-3 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-3 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-3 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-3 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPRegressor(activation=&#x27;logistic&#x27;, hidden_layer_sizes=(150, 20), max_iter=5000,\n",
              "             random_state=1, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>MLPRegressor</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.neural_network.MLPRegressor.html\">?<span>Documentation for MLPRegressor</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>MLPRegressor(activation=&#x27;logistic&#x27;, hidden_layer_sizes=(150, 20), max_iter=5000,\n",
              "             random_state=1, verbose=True)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yhat=model.predict(x.values)\n",
        "yhat_train=model.predict(x_train)\n",
        "yhat_test=model.predict(x_test)\n"
      ],
      "metadata": {
        "id": "P8Ve3Va9dww1"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(y_test,yhat_test)\n",
        "plt.plot(y_test,y_test,'r--')\n",
        "plt.xlabel('prices')\n",
        "plt.ylabel('predicted_price')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "5iXAsGNyepQl",
        "outputId": "e3833db9-549c-4ec7-dba4-c596c76fd54e"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'predicted_price')"
            ]
          },
          "metadata": {},
          "execution_count": 68
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "met.mean_squared_error(y_test,yhat_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IP9lZHiiA7NT",
        "outputId": "68943e40-f486-46b2-b557-e47d7a2393cd"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12.308773402691015"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(x_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnJy_RIiDGRL",
        "outputId": "34c364e9-8ef6-41b3-8f0b-d4e9d0e15f0c"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9957567514215183"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(x_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3uOZtAxDb_4",
        "outputId": "a088e22d-3ea9-4d59-e714-0e1cd5074144"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8348106019635153"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "scratchpad",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}