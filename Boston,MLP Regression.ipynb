{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lIYdn1woOS1n"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "boston = fetch_openml(name='boston', version=1, as_frame=True)\n",
        "x= boston.data\n",
        "y = boston.target\n",
        "\n",
        "print(X.head())\n"
      ],
      "metadata": {
        "id": "iiVoybHANMYi",
        "outputId": "57ac9d1c-a3b4-4beb-c1db-80b2e1bfe22b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      CRIM    ZN  INDUS CHAS    NOX  ...  RAD    TAX  PTRATIO       B  LSTAT\n",
            "0  0.00632  18.0   2.31    0  0.538  ...    1  296.0     15.3  396.90   4.98\n",
            "1  0.02731   0.0   7.07    0  0.469  ...    2  242.0     17.8  396.90   9.14\n",
            "2  0.02729   0.0   7.07    0  0.469  ...    2  242.0     17.8  392.83   4.03\n",
            "3  0.03237   0.0   2.18    0  0.458  ...    3  222.0     18.7  394.63   2.94\n",
            "4  0.06905   0.0   2.18    0  0.458  ...    3  222.0     18.7  396.90   5.33\n",
            "\n",
            "[5 rows x 13 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "dVQTePM6Nb0M"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.metrics as met\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "oZHEDe5_Q3iz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.DataFrame(x, columns=boston.feature_names)\n",
        "df"
      ],
      "metadata": {
        "id": "2uBUicU0R2H7",
        "outputId": "68812b46-b14c-428e-c1bb-aa10e3f51e3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        CRIM    ZN  INDUS CHAS    NOX  ...  RAD    TAX  PTRATIO       B  LSTAT\n",
              "0    0.00632  18.0   2.31    0  0.538  ...    1  296.0     15.3  396.90   4.98\n",
              "1    0.02731   0.0   7.07    0  0.469  ...    2  242.0     17.8  396.90   9.14\n",
              "2    0.02729   0.0   7.07    0  0.469  ...    2  242.0     17.8  392.83   4.03\n",
              "3    0.03237   0.0   2.18    0  0.458  ...    3  222.0     18.7  394.63   2.94\n",
              "4    0.06905   0.0   2.18    0  0.458  ...    3  222.0     18.7  396.90   5.33\n",
              "..       ...   ...    ...  ...    ...  ...  ...    ...      ...     ...    ...\n",
              "501  0.06263   0.0  11.93    0  0.573  ...    1  273.0     21.0  391.99   9.67\n",
              "502  0.04527   0.0  11.93    0  0.573  ...    1  273.0     21.0  396.90   9.08\n",
              "503  0.06076   0.0  11.93    0  0.573  ...    1  273.0     21.0  396.90   5.64\n",
              "504  0.10959   0.0  11.93    0  0.573  ...    1  273.0     21.0  393.45   6.48\n",
              "505  0.04741   0.0  11.93    0  0.573  ...    1  273.0     21.0  396.90   7.88\n",
              "\n",
              "[506 rows x 13 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-99145302-4571-45bb-99cb-a62be37fab22\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1</td>\n",
              "      <td>296.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.8</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.63</td>\n",
              "      <td>2.94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.2</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>501</th>\n",
              "      <td>0.06263</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.593</td>\n",
              "      <td>69.1</td>\n",
              "      <td>2.4786</td>\n",
              "      <td>1</td>\n",
              "      <td>273.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>391.99</td>\n",
              "      <td>9.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>502</th>\n",
              "      <td>0.04527</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.120</td>\n",
              "      <td>76.7</td>\n",
              "      <td>2.2875</td>\n",
              "      <td>1</td>\n",
              "      <td>273.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>503</th>\n",
              "      <td>0.06076</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.976</td>\n",
              "      <td>91.0</td>\n",
              "      <td>2.1675</td>\n",
              "      <td>1</td>\n",
              "      <td>273.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>504</th>\n",
              "      <td>0.10959</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.794</td>\n",
              "      <td>89.3</td>\n",
              "      <td>2.3889</td>\n",
              "      <td>1</td>\n",
              "      <td>273.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>393.45</td>\n",
              "      <td>6.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>505</th>\n",
              "      <td>0.04741</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.030</td>\n",
              "      <td>80.8</td>\n",
              "      <td>2.5050</td>\n",
              "      <td>1</td>\n",
              "      <td>273.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>396.90</td>\n",
              "      <td>7.88</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>506 rows × 13 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-99145302-4571-45bb-99cb-a62be37fab22')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-99145302-4571-45bb-99cb-a62be37fab22 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-99145302-4571-45bb-99cb-a62be37fab22');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-07bbd2c3-6bb3-4438-871e-6816f5cfb90b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-07bbd2c3-6bb3-4438-871e-6816f5cfb90b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-07bbd2c3-6bb3-4438-871e-6816f5cfb90b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 506,\n  \"fields\": [\n    {\n      \"column\": \"CRIM\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.601545105332487,\n        \"min\": 0.00632,\n        \"max\": 88.9762,\n        \"num_unique_values\": 504,\n        \"samples\": [\n          0.09178,\n          0.05644,\n          0.10574\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ZN\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 23.322452994515036,\n        \"min\": 0.0,\n        \"max\": 100.0,\n        \"num_unique_values\": 26,\n        \"samples\": [\n          25.0,\n          30.0,\n          18.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"INDUS\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.8603529408975845,\n        \"min\": 0.46,\n        \"max\": 27.74,\n        \"num_unique_values\": 76,\n        \"samples\": [\n          8.14,\n          1.47,\n          1.22\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CHAS\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"1\",\n          \"0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NOX\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11587767566755611,\n        \"min\": 0.385,\n        \"max\": 0.871,\n        \"num_unique_values\": 81,\n        \"samples\": [\n          0.401,\n          0.538\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RM\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7026171434153237,\n        \"min\": 3.561,\n        \"max\": 8.78,\n        \"num_unique_values\": 446,\n        \"samples\": [\n          6.849,\n          4.88\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AGE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 28.148861406903638,\n        \"min\": 2.9,\n        \"max\": 100.0,\n        \"num_unique_values\": 356,\n        \"samples\": [\n          51.8,\n          33.8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DIS\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.1057101266276104,\n        \"min\": 1.1296,\n        \"max\": 12.1265,\n        \"num_unique_values\": 412,\n        \"samples\": [\n          2.2955,\n          4.2515\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RAD\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"7\",\n          \"2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TAX\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 168.53711605495926,\n        \"min\": 187.0,\n        \"max\": 711.0,\n        \"num_unique_values\": 66,\n        \"samples\": [\n          370.0,\n          666.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PTRATIO\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.164945523714446,\n        \"min\": 12.6,\n        \"max\": 22.0,\n        \"num_unique_values\": 46,\n        \"samples\": [\n          19.6,\n          15.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 91.29486438415779,\n        \"min\": 0.32,\n        \"max\": 396.9,\n        \"num_unique_values\": 357,\n        \"samples\": [\n          396.24,\n          395.11\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LSTAT\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.141061511348571,\n        \"min\": 1.73,\n        \"max\": 37.97,\n        \"num_unique_values\": 455,\n        \"samples\": [\n          6.15,\n          4.32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['price']=y"
      ],
      "metadata": {
        "id": "drAY_jzXSxfc"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test,y_train,y_test=train_test_split(x,y,test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "3o-Hs0fZTGQy"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "id": "yaC1dcOoVE4r",
        "outputId": "e2f5dcc2-ed8a-4387-c28f-1665fc440bad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(354, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.shape"
      ],
      "metadata": {
        "id": "c-_WPRGNVILK",
        "outputId": "602a9685-f9f2-4538-dcd5-37d918171f92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(152, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "id": "uyekNfKIXkll",
        "outputId": "b17e81a7-37d3-43d4-870e-1b20a47cce80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(354,)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x_train.iloc[:,12],y_train)\n",
        "plt.scatter(x_test.iloc[:,12],y_test)"
      ],
      "metadata": {
        "id": "q3vUS9I0YMqr",
        "outputId": "d4383187-a2af-40ad-ad66-a53ec4891a8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x78082260dd10>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=MLPRegressor(hidden_layer_sizes=(150,20), activation='logistic', solver='adam', random_state=1, max_iter=5000,verbose=True)\n",
        "model.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "fyO0USURa3U_",
        "outputId": "9cdecc1c-a036-44d7-d499-e819a3ddebdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 304.12753092\n",
            "Iteration 2, loss = 300.64363321\n",
            "Iteration 3, loss = 297.26335939\n",
            "Iteration 4, loss = 293.78760130\n",
            "Iteration 5, loss = 290.49999071\n",
            "Iteration 6, loss = 287.37623281\n",
            "Iteration 7, loss = 284.38355732\n",
            "Iteration 8, loss = 281.64650355\n",
            "Iteration 9, loss = 279.15640324\n",
            "Iteration 10, loss = 276.89152398\n",
            "Iteration 11, loss = 274.78709971\n",
            "Iteration 12, loss = 272.85464639\n",
            "Iteration 13, loss = 271.11156200\n",
            "Iteration 14, loss = 269.46720256\n",
            "Iteration 15, loss = 267.96759226\n",
            "Iteration 16, loss = 266.56572842\n",
            "Iteration 17, loss = 265.26814860\n",
            "Iteration 18, loss = 264.10780816\n",
            "Iteration 19, loss = 263.03949613\n",
            "Iteration 20, loss = 262.06345092\n",
            "Iteration 21, loss = 261.12160319\n",
            "Iteration 22, loss = 260.22706862\n",
            "Iteration 23, loss = 259.37179102\n",
            "Iteration 24, loss = 258.54096689\n",
            "Iteration 25, loss = 257.72839806\n",
            "Iteration 26, loss = 256.96161111\n",
            "Iteration 27, loss = 256.19206453\n",
            "Iteration 28, loss = 255.45338509\n",
            "Iteration 29, loss = 254.71456262\n",
            "Iteration 30, loss = 253.98437152\n",
            "Iteration 31, loss = 253.25442064\n",
            "Iteration 32, loss = 252.53978910\n",
            "Iteration 33, loss = 251.81440821\n",
            "Iteration 34, loss = 251.11723501\n",
            "Iteration 35, loss = 250.43406412\n",
            "Iteration 36, loss = 249.76223292\n",
            "Iteration 37, loss = 249.08384438\n",
            "Iteration 38, loss = 248.40861818\n",
            "Iteration 39, loss = 247.74794932\n",
            "Iteration 40, loss = 247.08762261\n",
            "Iteration 41, loss = 246.43564988\n",
            "Iteration 42, loss = 245.77811478\n",
            "Iteration 43, loss = 245.13091165\n",
            "Iteration 44, loss = 244.48784390\n",
            "Iteration 45, loss = 243.84606473\n",
            "Iteration 46, loss = 243.21053357\n",
            "Iteration 47, loss = 242.57740457\n",
            "Iteration 48, loss = 241.94240704\n",
            "Iteration 49, loss = 241.32000269\n",
            "Iteration 50, loss = 240.69370168\n",
            "Iteration 51, loss = 240.07522539\n",
            "Iteration 52, loss = 239.44752214\n",
            "Iteration 53, loss = 238.83714876\n",
            "Iteration 54, loss = 238.22629924\n",
            "Iteration 55, loss = 237.61702802\n",
            "Iteration 56, loss = 237.00990366\n",
            "Iteration 57, loss = 236.40402552\n",
            "Iteration 58, loss = 235.80764522\n",
            "Iteration 59, loss = 235.19353945\n",
            "Iteration 60, loss = 234.60055858\n",
            "Iteration 61, loss = 234.00433650\n",
            "Iteration 62, loss = 233.40085944\n",
            "Iteration 63, loss = 232.81614960\n",
            "Iteration 64, loss = 232.21614112\n",
            "Iteration 65, loss = 231.62512999\n",
            "Iteration 66, loss = 231.03836303\n",
            "Iteration 67, loss = 230.44830615\n",
            "Iteration 68, loss = 229.85995816\n",
            "Iteration 69, loss = 229.28366978\n",
            "Iteration 70, loss = 228.70085880\n",
            "Iteration 71, loss = 228.11992745\n",
            "Iteration 72, loss = 227.54914578\n",
            "Iteration 73, loss = 226.97646068\n",
            "Iteration 74, loss = 226.39255792\n",
            "Iteration 75, loss = 225.83260895\n",
            "Iteration 76, loss = 225.26248476\n",
            "Iteration 77, loss = 224.68733580\n",
            "Iteration 78, loss = 224.12697417\n",
            "Iteration 79, loss = 223.56183156\n",
            "Iteration 80, loss = 222.99830372\n",
            "Iteration 81, loss = 222.43831447\n",
            "Iteration 82, loss = 221.87867432\n",
            "Iteration 83, loss = 221.32807497\n",
            "Iteration 84, loss = 220.76276228\n",
            "Iteration 85, loss = 220.20932606\n",
            "Iteration 86, loss = 219.66492596\n",
            "Iteration 87, loss = 219.10981941\n",
            "Iteration 88, loss = 218.56248188\n",
            "Iteration 89, loss = 218.01272465\n",
            "Iteration 90, loss = 217.46280010\n",
            "Iteration 91, loss = 216.91521954\n",
            "Iteration 92, loss = 216.38405696\n",
            "Iteration 93, loss = 215.83916481\n",
            "Iteration 94, loss = 215.30587501\n",
            "Iteration 95, loss = 214.75523224\n",
            "Iteration 96, loss = 214.22484813\n",
            "Iteration 97, loss = 213.68523272\n",
            "Iteration 98, loss = 213.15307929\n",
            "Iteration 99, loss = 212.62153533\n",
            "Iteration 100, loss = 212.09226021\n",
            "Iteration 101, loss = 211.56322813\n",
            "Iteration 102, loss = 211.02861510\n",
            "Iteration 103, loss = 210.50818552\n",
            "Iteration 104, loss = 209.98165954\n",
            "Iteration 105, loss = 209.45725850\n",
            "Iteration 106, loss = 208.94041303\n",
            "Iteration 107, loss = 208.41328708\n",
            "Iteration 108, loss = 207.89606128\n",
            "Iteration 109, loss = 207.36878343\n",
            "Iteration 110, loss = 206.86427689\n",
            "Iteration 111, loss = 206.34674063\n",
            "Iteration 112, loss = 205.83313616\n",
            "Iteration 113, loss = 205.31599918\n",
            "Iteration 114, loss = 204.80420871\n",
            "Iteration 115, loss = 204.29933271\n",
            "Iteration 116, loss = 203.79168822\n",
            "Iteration 117, loss = 203.27971450\n",
            "Iteration 118, loss = 202.77842996\n",
            "Iteration 119, loss = 202.26964004\n",
            "Iteration 120, loss = 201.76537529\n",
            "Iteration 121, loss = 201.26619209\n",
            "Iteration 122, loss = 200.76248842\n",
            "Iteration 123, loss = 200.26509083\n",
            "Iteration 124, loss = 199.77304270\n",
            "Iteration 125, loss = 199.27049379\n",
            "Iteration 126, loss = 198.78437589\n",
            "Iteration 127, loss = 198.27909222\n",
            "Iteration 128, loss = 197.78562384\n",
            "Iteration 129, loss = 197.29696929\n",
            "Iteration 130, loss = 196.80876868\n",
            "Iteration 131, loss = 196.32069527\n",
            "Iteration 132, loss = 195.83038689\n",
            "Iteration 133, loss = 195.34105968\n",
            "Iteration 134, loss = 194.86264471\n",
            "Iteration 135, loss = 194.37292864\n",
            "Iteration 136, loss = 193.89826320\n",
            "Iteration 137, loss = 193.40964567\n",
            "Iteration 138, loss = 192.92708690\n",
            "Iteration 139, loss = 192.44420724\n",
            "Iteration 140, loss = 191.97590577\n",
            "Iteration 141, loss = 191.49086654\n",
            "Iteration 142, loss = 191.01949845\n",
            "Iteration 143, loss = 190.54478226\n",
            "Iteration 144, loss = 190.07405969\n",
            "Iteration 145, loss = 189.59448323\n",
            "Iteration 146, loss = 189.12961453\n",
            "Iteration 147, loss = 188.65202376\n",
            "Iteration 148, loss = 188.19407500\n",
            "Iteration 149, loss = 187.71523974\n",
            "Iteration 150, loss = 187.25305571\n",
            "Iteration 151, loss = 186.79470127\n",
            "Iteration 152, loss = 186.32547286\n",
            "Iteration 153, loss = 185.86123640\n",
            "Iteration 154, loss = 185.39854711\n",
            "Iteration 155, loss = 184.94858871\n",
            "Iteration 156, loss = 184.48733045\n",
            "Iteration 157, loss = 184.02272338\n",
            "Iteration 158, loss = 183.57153552\n",
            "Iteration 159, loss = 183.11777788\n",
            "Iteration 160, loss = 182.65731250\n",
            "Iteration 161, loss = 182.21167409\n",
            "Iteration 162, loss = 181.75916673\n",
            "Iteration 163, loss = 181.30607528\n",
            "Iteration 164, loss = 180.85948166\n",
            "Iteration 165, loss = 180.40957200\n",
            "Iteration 166, loss = 179.96486148\n",
            "Iteration 167, loss = 179.52278787\n",
            "Iteration 168, loss = 179.07138551\n",
            "Iteration 169, loss = 178.62991254\n",
            "Iteration 170, loss = 178.18908523\n",
            "Iteration 171, loss = 177.74572921\n",
            "Iteration 172, loss = 177.31191973\n",
            "Iteration 173, loss = 176.86846525\n",
            "Iteration 174, loss = 176.43070234\n",
            "Iteration 175, loss = 175.99010468\n",
            "Iteration 176, loss = 175.55632524\n",
            "Iteration 177, loss = 175.12644413\n",
            "Iteration 178, loss = 174.69265511\n",
            "Iteration 179, loss = 174.26186999\n",
            "Iteration 180, loss = 173.82480340\n",
            "Iteration 181, loss = 173.39604585\n",
            "Iteration 182, loss = 172.96225647\n",
            "Iteration 183, loss = 172.54351175\n",
            "Iteration 184, loss = 172.11450105\n",
            "Iteration 185, loss = 171.69087845\n",
            "Iteration 186, loss = 171.26373235\n",
            "Iteration 187, loss = 170.83948923\n",
            "Iteration 188, loss = 170.41546498\n",
            "Iteration 189, loss = 169.99664363\n",
            "Iteration 190, loss = 169.57842323\n",
            "Iteration 191, loss = 169.15855553\n",
            "Iteration 192, loss = 168.73680857\n",
            "Iteration 193, loss = 168.31932016\n",
            "Iteration 194, loss = 167.91084615\n",
            "Iteration 195, loss = 167.49208305\n",
            "Iteration 196, loss = 167.07780838\n",
            "Iteration 197, loss = 166.66934303\n",
            "Iteration 198, loss = 166.25371790\n",
            "Iteration 199, loss = 165.84392763\n",
            "Iteration 200, loss = 165.43580563\n",
            "Iteration 201, loss = 165.02910587\n",
            "Iteration 202, loss = 164.62568342\n",
            "Iteration 203, loss = 164.21430969\n",
            "Iteration 204, loss = 163.80966343\n",
            "Iteration 205, loss = 163.40290740\n",
            "Iteration 206, loss = 163.00475366\n",
            "Iteration 207, loss = 162.60093192\n",
            "Iteration 208, loss = 162.20569492\n",
            "Iteration 209, loss = 161.80423124\n",
            "Iteration 210, loss = 161.40427484\n",
            "Iteration 211, loss = 161.00830170\n",
            "Iteration 212, loss = 160.60820289\n",
            "Iteration 213, loss = 160.21836233\n",
            "Iteration 214, loss = 159.82242985\n",
            "Iteration 215, loss = 159.42737215\n",
            "Iteration 216, loss = 159.03721351\n",
            "Iteration 217, loss = 158.63844246\n",
            "Iteration 218, loss = 158.25398176\n",
            "Iteration 219, loss = 157.86985253\n",
            "Iteration 220, loss = 157.47663064\n",
            "Iteration 221, loss = 157.09300263\n",
            "Iteration 222, loss = 156.70755505\n",
            "Iteration 223, loss = 156.31712271\n",
            "Iteration 224, loss = 155.93422272\n",
            "Iteration 225, loss = 155.55573758\n",
            "Iteration 226, loss = 155.17253236\n",
            "Iteration 227, loss = 154.79129397\n",
            "Iteration 228, loss = 154.40324894\n",
            "Iteration 229, loss = 154.03079907\n",
            "Iteration 230, loss = 153.65495475\n",
            "Iteration 231, loss = 153.27624329\n",
            "Iteration 232, loss = 152.89781718\n",
            "Iteration 233, loss = 152.52700506\n",
            "Iteration 234, loss = 152.15444205\n",
            "Iteration 235, loss = 151.77337011\n",
            "Iteration 236, loss = 151.40727550\n",
            "Iteration 237, loss = 151.03350365\n",
            "Iteration 238, loss = 150.66348533\n",
            "Iteration 239, loss = 150.29768424\n",
            "Iteration 240, loss = 149.93130470\n",
            "Iteration 241, loss = 149.55779309\n",
            "Iteration 242, loss = 149.19623950\n",
            "Iteration 243, loss = 148.83149093\n",
            "Iteration 244, loss = 148.46631749\n",
            "Iteration 245, loss = 148.10139211\n",
            "Iteration 246, loss = 147.74677041\n",
            "Iteration 247, loss = 147.37922168\n",
            "Iteration 248, loss = 147.02202974\n",
            "Iteration 249, loss = 146.66596003\n",
            "Iteration 250, loss = 146.30428649\n",
            "Iteration 251, loss = 145.94927417\n",
            "Iteration 252, loss = 145.59047864\n",
            "Iteration 253, loss = 145.23675971\n",
            "Iteration 254, loss = 144.87703652\n",
            "Iteration 255, loss = 144.52890418\n",
            "Iteration 256, loss = 144.17513518\n",
            "Iteration 257, loss = 143.82943555\n",
            "Iteration 258, loss = 143.46966914\n",
            "Iteration 259, loss = 143.12298826\n",
            "Iteration 260, loss = 142.77659197\n",
            "Iteration 261, loss = 142.42906847\n",
            "Iteration 262, loss = 142.07857874\n",
            "Iteration 263, loss = 141.73418343\n",
            "Iteration 264, loss = 141.38837094\n",
            "Iteration 265, loss = 141.04115935\n",
            "Iteration 266, loss = 140.70562616\n",
            "Iteration 267, loss = 140.35501888\n",
            "Iteration 268, loss = 140.01450892\n",
            "Iteration 269, loss = 139.68509259\n",
            "Iteration 270, loss = 139.33305617\n",
            "Iteration 271, loss = 138.99538990\n",
            "Iteration 272, loss = 138.66508703\n",
            "Iteration 273, loss = 138.32794198\n",
            "Iteration 274, loss = 137.98810890\n",
            "Iteration 275, loss = 137.65770180\n",
            "Iteration 276, loss = 137.31826263\n",
            "Iteration 277, loss = 136.98887259\n",
            "Iteration 278, loss = 136.65529140\n",
            "Iteration 279, loss = 136.32738132\n",
            "Iteration 280, loss = 135.99385083\n",
            "Iteration 281, loss = 135.66327580\n",
            "Iteration 282, loss = 135.33091772\n",
            "Iteration 283, loss = 135.00927477\n",
            "Iteration 284, loss = 134.68426525\n",
            "Iteration 285, loss = 134.35672808\n",
            "Iteration 286, loss = 134.03110938\n",
            "Iteration 287, loss = 133.70408874\n",
            "Iteration 288, loss = 133.38053795\n",
            "Iteration 289, loss = 133.05720352\n",
            "Iteration 290, loss = 132.73806947\n",
            "Iteration 291, loss = 132.42011127\n",
            "Iteration 292, loss = 132.09658759\n",
            "Iteration 293, loss = 131.78087535\n",
            "Iteration 294, loss = 131.45640699\n",
            "Iteration 295, loss = 131.14056752\n",
            "Iteration 296, loss = 130.82611564\n",
            "Iteration 297, loss = 130.51279157\n",
            "Iteration 298, loss = 130.19543372\n",
            "Iteration 299, loss = 129.88417203\n",
            "Iteration 300, loss = 129.57088183\n",
            "Iteration 301, loss = 129.25767654\n",
            "Iteration 302, loss = 128.94983473\n",
            "Iteration 303, loss = 128.63513563\n",
            "Iteration 304, loss = 128.32839529\n",
            "Iteration 305, loss = 128.01919677\n",
            "Iteration 306, loss = 127.71698221\n",
            "Iteration 307, loss = 127.40725906\n",
            "Iteration 308, loss = 127.10222794\n",
            "Iteration 309, loss = 126.79449628\n",
            "Iteration 310, loss = 126.49324013\n",
            "Iteration 311, loss = 126.19226005\n",
            "Iteration 312, loss = 125.88996476\n",
            "Iteration 313, loss = 125.58570857\n",
            "Iteration 314, loss = 125.28721506\n",
            "Iteration 315, loss = 124.98434945\n",
            "Iteration 316, loss = 124.68790300\n",
            "Iteration 317, loss = 124.39315096\n",
            "Iteration 318, loss = 124.09100724\n",
            "Iteration 319, loss = 123.79466244\n",
            "Iteration 320, loss = 123.50012815\n",
            "Iteration 321, loss = 123.20649290\n",
            "Iteration 322, loss = 122.91819657\n",
            "Iteration 323, loss = 122.61614782\n",
            "Iteration 324, loss = 122.32900198\n",
            "Iteration 325, loss = 122.03438915\n",
            "Iteration 326, loss = 121.74946471\n",
            "Iteration 327, loss = 121.45411815\n",
            "Iteration 328, loss = 121.16878296\n",
            "Iteration 329, loss = 120.87655961\n",
            "Iteration 330, loss = 120.59512873\n",
            "Iteration 331, loss = 120.30650147\n",
            "Iteration 332, loss = 120.02103142\n",
            "Iteration 333, loss = 119.74043508\n",
            "Iteration 334, loss = 119.45238971\n",
            "Iteration 335, loss = 119.17230010\n",
            "Iteration 336, loss = 118.89004887\n",
            "Iteration 337, loss = 118.60402905\n",
            "Iteration 338, loss = 118.32806112\n",
            "Iteration 339, loss = 118.05378536\n",
            "Iteration 340, loss = 117.77034545\n",
            "Iteration 341, loss = 117.49327849\n",
            "Iteration 342, loss = 117.21610991\n",
            "Iteration 343, loss = 116.93685677\n",
            "Iteration 344, loss = 116.66603607\n",
            "Iteration 345, loss = 116.39532174\n",
            "Iteration 346, loss = 116.11966128\n",
            "Iteration 347, loss = 115.84166271\n",
            "Iteration 348, loss = 115.56937269\n",
            "Iteration 349, loss = 115.30301201\n",
            "Iteration 350, loss = 115.03683122\n",
            "Iteration 351, loss = 114.76493242\n",
            "Iteration 352, loss = 114.49335565\n",
            "Iteration 353, loss = 114.22472423\n",
            "Iteration 354, loss = 113.95920216\n",
            "Iteration 355, loss = 113.69041157\n",
            "Iteration 356, loss = 113.42972684\n",
            "Iteration 357, loss = 113.16103571\n",
            "Iteration 358, loss = 112.90120473\n",
            "Iteration 359, loss = 112.63433771\n",
            "Iteration 360, loss = 112.37321118\n",
            "Iteration 361, loss = 112.10922078\n",
            "Iteration 362, loss = 111.84385121\n",
            "Iteration 363, loss = 111.59105049\n",
            "Iteration 364, loss = 111.33177085\n",
            "Iteration 365, loss = 111.07133571\n",
            "Iteration 366, loss = 110.81153746\n",
            "Iteration 367, loss = 110.54941582\n",
            "Iteration 368, loss = 110.30345411\n",
            "Iteration 369, loss = 110.04142210\n",
            "Iteration 370, loss = 109.78751641\n",
            "Iteration 371, loss = 109.53310309\n",
            "Iteration 372, loss = 109.27844035\n",
            "Iteration 373, loss = 109.02789126\n",
            "Iteration 374, loss = 108.77413015\n",
            "Iteration 375, loss = 108.51944075\n",
            "Iteration 376, loss = 108.27093875\n",
            "Iteration 377, loss = 108.02179695\n",
            "Iteration 378, loss = 107.77492211\n",
            "Iteration 379, loss = 107.52285917\n",
            "Iteration 380, loss = 107.27275658\n",
            "Iteration 381, loss = 107.03349647\n",
            "Iteration 382, loss = 106.78033685\n",
            "Iteration 383, loss = 106.53419582\n",
            "Iteration 384, loss = 106.29000404\n",
            "Iteration 385, loss = 106.04613525\n",
            "Iteration 386, loss = 105.79850582\n",
            "Iteration 387, loss = 105.56428557\n",
            "Iteration 388, loss = 105.31394773\n",
            "Iteration 389, loss = 105.07605124\n",
            "Iteration 390, loss = 104.83380149\n",
            "Iteration 391, loss = 104.59291191\n",
            "Iteration 392, loss = 104.35354498\n",
            "Iteration 393, loss = 104.11619609\n",
            "Iteration 394, loss = 103.87739969\n",
            "Iteration 395, loss = 103.64209739\n",
            "Iteration 396, loss = 103.40214280\n",
            "Iteration 397, loss = 103.16436504\n",
            "Iteration 398, loss = 102.93695814\n",
            "Iteration 399, loss = 102.70175468\n",
            "Iteration 400, loss = 102.46553639\n",
            "Iteration 401, loss = 102.23083142\n",
            "Iteration 402, loss = 102.00069403\n",
            "Iteration 403, loss = 101.77186538\n",
            "Iteration 404, loss = 101.53736297\n",
            "Iteration 405, loss = 101.31471507\n",
            "Iteration 406, loss = 101.08162314\n",
            "Iteration 407, loss = 100.85326754\n",
            "Iteration 408, loss = 100.62515389\n",
            "Iteration 409, loss = 100.39920600\n",
            "Iteration 410, loss = 100.17609354\n",
            "Iteration 411, loss = 99.94648123\n",
            "Iteration 412, loss = 99.72063787\n",
            "Iteration 413, loss = 99.50217811\n",
            "Iteration 414, loss = 99.27796921\n",
            "Iteration 415, loss = 99.05041295\n",
            "Iteration 416, loss = 98.83246800\n",
            "Iteration 417, loss = 98.60820423\n",
            "Iteration 418, loss = 98.38942311\n",
            "Iteration 419, loss = 98.17160371\n",
            "Iteration 420, loss = 97.95021219\n",
            "Iteration 421, loss = 97.72746577\n",
            "Iteration 422, loss = 97.51495916\n",
            "Iteration 423, loss = 97.29804866\n",
            "Iteration 424, loss = 97.07769333\n",
            "Iteration 425, loss = 96.86226554\n",
            "Iteration 426, loss = 96.64867185\n",
            "Iteration 427, loss = 96.43214811\n",
            "Iteration 428, loss = 96.21928948\n",
            "Iteration 429, loss = 96.00610149\n",
            "Iteration 430, loss = 95.79210846\n",
            "Iteration 431, loss = 95.58499727\n",
            "Iteration 432, loss = 95.36909085\n",
            "Iteration 433, loss = 95.15821213\n",
            "Iteration 434, loss = 94.95074685\n",
            "Iteration 435, loss = 94.74608100\n",
            "Iteration 436, loss = 94.53066905\n",
            "Iteration 437, loss = 94.32477232\n",
            "Iteration 438, loss = 94.11802282\n",
            "Iteration 439, loss = 93.91415312\n",
            "Iteration 440, loss = 93.70569838\n",
            "Iteration 441, loss = 93.50529979\n",
            "Iteration 442, loss = 93.29567881\n",
            "Iteration 443, loss = 93.09257515\n",
            "Iteration 444, loss = 92.89702838\n",
            "Iteration 445, loss = 92.68803388\n",
            "Iteration 446, loss = 92.48836776\n",
            "Iteration 447, loss = 92.28329394\n",
            "Iteration 448, loss = 92.08915619\n",
            "Iteration 449, loss = 91.88834264\n",
            "Iteration 450, loss = 91.68963781\n",
            "Iteration 451, loss = 91.48838571\n",
            "Iteration 452, loss = 91.29286123\n",
            "Iteration 453, loss = 91.09705597\n",
            "Iteration 454, loss = 90.89694010\n",
            "Iteration 455, loss = 90.70064836\n",
            "Iteration 456, loss = 90.50937200\n",
            "Iteration 457, loss = 90.30905078\n",
            "Iteration 458, loss = 90.11945436\n",
            "Iteration 459, loss = 89.92618272\n",
            "Iteration 460, loss = 89.73155672\n",
            "Iteration 461, loss = 89.53837974\n",
            "Iteration 462, loss = 89.34527115\n",
            "Iteration 463, loss = 89.15580239\n",
            "Iteration 464, loss = 88.96136375\n",
            "Iteration 465, loss = 88.77690744\n",
            "Iteration 466, loss = 88.58172448\n",
            "Iteration 467, loss = 88.39477548\n",
            "Iteration 468, loss = 88.20993268\n",
            "Iteration 469, loss = 88.01987294\n",
            "Iteration 470, loss = 87.83145098\n",
            "Iteration 471, loss = 87.64442812\n",
            "Iteration 472, loss = 87.46183345\n",
            "Iteration 473, loss = 87.27703765\n",
            "Iteration 474, loss = 87.08994971\n",
            "Iteration 475, loss = 86.90677701\n",
            "Iteration 476, loss = 86.72532082\n",
            "Iteration 477, loss = 86.54132097\n",
            "Iteration 478, loss = 86.35790259\n",
            "Iteration 479, loss = 86.17683756\n",
            "Iteration 480, loss = 85.99921009\n",
            "Iteration 481, loss = 85.81579838\n",
            "Iteration 482, loss = 85.63941924\n",
            "Iteration 483, loss = 85.45505613\n",
            "Iteration 484, loss = 85.27960636\n",
            "Iteration 485, loss = 85.10162583\n",
            "Iteration 486, loss = 84.92937315\n",
            "Iteration 487, loss = 84.75053082\n",
            "Iteration 488, loss = 84.57300985\n",
            "Iteration 489, loss = 84.39648448\n",
            "Iteration 490, loss = 84.22461680\n",
            "Iteration 491, loss = 84.04778589\n",
            "Iteration 492, loss = 83.87679084\n",
            "Iteration 493, loss = 83.70812179\n",
            "Iteration 494, loss = 83.52885723\n",
            "Iteration 495, loss = 83.35612065\n",
            "Iteration 496, loss = 83.18903680\n",
            "Iteration 497, loss = 83.01622852\n",
            "Iteration 498, loss = 82.85356349\n",
            "Iteration 499, loss = 82.68118444\n",
            "Iteration 500, loss = 82.51153377\n",
            "Iteration 501, loss = 82.34180622\n",
            "Iteration 502, loss = 82.17515559\n",
            "Iteration 503, loss = 82.00886516\n",
            "Iteration 504, loss = 81.84180432\n",
            "Iteration 505, loss = 81.67668490\n",
            "Iteration 506, loss = 81.51533882\n",
            "Iteration 507, loss = 81.34761962\n",
            "Iteration 508, loss = 81.18229828\n",
            "Iteration 509, loss = 81.01551192\n",
            "Iteration 510, loss = 80.85616021\n",
            "Iteration 511, loss = 80.69695176\n",
            "Iteration 512, loss = 80.53184610\n",
            "Iteration 513, loss = 80.36837162\n",
            "Iteration 514, loss = 80.20984948\n",
            "Iteration 515, loss = 80.04770716\n",
            "Iteration 516, loss = 79.88999435\n",
            "Iteration 517, loss = 79.73213754\n",
            "Iteration 518, loss = 79.56827897\n",
            "Iteration 519, loss = 79.41381626\n",
            "Iteration 520, loss = 79.25509038\n",
            "Iteration 521, loss = 79.10076177\n",
            "Iteration 522, loss = 78.94289443\n",
            "Iteration 523, loss = 78.78465091\n",
            "Iteration 524, loss = 78.62598592\n",
            "Iteration 525, loss = 78.47618934\n",
            "Iteration 526, loss = 78.32110036\n",
            "Iteration 527, loss = 78.16697334\n",
            "Iteration 528, loss = 78.01121487\n",
            "Iteration 529, loss = 77.86270096\n",
            "Iteration 530, loss = 77.70917678\n",
            "Iteration 531, loss = 77.55957292\n",
            "Iteration 532, loss = 77.40546412\n",
            "Iteration 533, loss = 77.25372496\n",
            "Iteration 534, loss = 77.10701181\n",
            "Iteration 535, loss = 76.95537860\n",
            "Iteration 536, loss = 76.80905536\n",
            "Iteration 537, loss = 76.65902675\n",
            "Iteration 538, loss = 76.50976388\n",
            "Iteration 539, loss = 76.36148463\n",
            "Iteration 540, loss = 76.21965963\n",
            "Iteration 541, loss = 76.07433116\n",
            "Iteration 542, loss = 75.92362337\n",
            "Iteration 543, loss = 75.78162407\n",
            "Iteration 544, loss = 75.63873358\n",
            "Iteration 545, loss = 75.49296976\n",
            "Iteration 546, loss = 75.34546353\n",
            "Iteration 547, loss = 75.20456871\n",
            "Iteration 548, loss = 75.06542959\n",
            "Iteration 549, loss = 74.91882545\n",
            "Iteration 550, loss = 74.78369313\n",
            "Iteration 551, loss = 74.63970906\n",
            "Iteration 552, loss = 74.50200670\n",
            "Iteration 553, loss = 74.35626579\n",
            "Iteration 554, loss = 74.21722415\n",
            "Iteration 555, loss = 74.07967305\n",
            "Iteration 556, loss = 73.93887026\n",
            "Iteration 557, loss = 73.80391984\n",
            "Iteration 558, loss = 73.66418065\n",
            "Iteration 559, loss = 73.53128368\n",
            "Iteration 560, loss = 73.38769657\n",
            "Iteration 561, loss = 73.25602619\n",
            "Iteration 562, loss = 73.12395919\n",
            "Iteration 563, loss = 72.98375011\n",
            "Iteration 564, loss = 72.85186089\n",
            "Iteration 565, loss = 72.71594318\n",
            "Iteration 566, loss = 72.58110959\n",
            "Iteration 567, loss = 72.44683936\n",
            "Iteration 568, loss = 72.31683061\n",
            "Iteration 569, loss = 72.18631598\n",
            "Iteration 570, loss = 72.05561135\n",
            "Iteration 571, loss = 71.92036727\n",
            "Iteration 572, loss = 71.79312442\n",
            "Iteration 573, loss = 71.66325045\n",
            "Iteration 574, loss = 71.53312506\n",
            "Iteration 575, loss = 71.40101354\n",
            "Iteration 576, loss = 71.27213455\n",
            "Iteration 577, loss = 71.14839049\n",
            "Iteration 578, loss = 71.01741489\n",
            "Iteration 579, loss = 70.89205617\n",
            "Iteration 580, loss = 70.76315117\n",
            "Iteration 581, loss = 70.63794324\n",
            "Iteration 582, loss = 70.51243656\n",
            "Iteration 583, loss = 70.38653968\n",
            "Iteration 584, loss = 70.25716303\n",
            "Iteration 585, loss = 70.13896515\n",
            "Iteration 586, loss = 70.01004223\n",
            "Iteration 587, loss = 69.89103874\n",
            "Iteration 588, loss = 69.76089574\n",
            "Iteration 589, loss = 69.64037061\n",
            "Iteration 590, loss = 69.51831825\n",
            "Iteration 591, loss = 69.39749964\n",
            "Iteration 592, loss = 69.27785579\n",
            "Iteration 593, loss = 69.15685614\n",
            "Iteration 594, loss = 69.03075163\n",
            "Iteration 595, loss = 68.91464666\n",
            "Iteration 596, loss = 68.79529618\n",
            "Iteration 597, loss = 68.67614597\n",
            "Iteration 598, loss = 68.55891198\n",
            "Iteration 599, loss = 68.44142879\n",
            "Iteration 600, loss = 68.32428157\n",
            "Iteration 601, loss = 68.20538969\n",
            "Iteration 602, loss = 68.09116232\n",
            "Iteration 603, loss = 67.97108658\n",
            "Iteration 604, loss = 67.85621539\n",
            "Iteration 605, loss = 67.74511514\n",
            "Iteration 606, loss = 67.62889183\n",
            "Iteration 607, loss = 67.51239704\n",
            "Iteration 608, loss = 67.40077160\n",
            "Iteration 609, loss = 67.28829794\n",
            "Iteration 610, loss = 67.17671666\n",
            "Iteration 611, loss = 67.06303672\n",
            "Iteration 612, loss = 66.95003810\n",
            "Iteration 613, loss = 66.83671707\n",
            "Iteration 614, loss = 66.72854826\n",
            "Iteration 615, loss = 66.61748452\n",
            "Iteration 616, loss = 66.50496735\n",
            "Iteration 617, loss = 66.40110588\n",
            "Iteration 618, loss = 66.28645653\n",
            "Iteration 619, loss = 66.17865330\n",
            "Iteration 620, loss = 66.06819288\n",
            "Iteration 621, loss = 65.96292295\n",
            "Iteration 622, loss = 65.85467554\n",
            "Iteration 623, loss = 65.74599188\n",
            "Iteration 624, loss = 65.63931982\n",
            "Iteration 625, loss = 65.53452893\n",
            "Iteration 626, loss = 65.42533137\n",
            "Iteration 627, loss = 65.31897797\n",
            "Iteration 628, loss = 65.21378951\n",
            "Iteration 629, loss = 65.10915051\n",
            "Iteration 630, loss = 65.00298183\n",
            "Iteration 631, loss = 64.89901323\n",
            "Iteration 632, loss = 64.79677072\n",
            "Iteration 633, loss = 64.69118913\n",
            "Iteration 634, loss = 64.58801858\n",
            "Iteration 635, loss = 64.48736165\n",
            "Iteration 636, loss = 64.38426309\n",
            "Iteration 637, loss = 64.27986387\n",
            "Iteration 638, loss = 64.17974688\n",
            "Iteration 639, loss = 64.07869741\n",
            "Iteration 640, loss = 63.97696099\n",
            "Iteration 641, loss = 63.87573529\n",
            "Iteration 642, loss = 63.77938929\n",
            "Iteration 643, loss = 63.67977956\n",
            "Iteration 644, loss = 63.58228857\n",
            "Iteration 645, loss = 63.48096017\n",
            "Iteration 646, loss = 63.38183742\n",
            "Iteration 647, loss = 63.28837176\n",
            "Iteration 648, loss = 63.18877662\n",
            "Iteration 649, loss = 63.09127982\n",
            "Iteration 650, loss = 62.99094718\n",
            "Iteration 651, loss = 62.90287925\n",
            "Iteration 652, loss = 62.80180946\n",
            "Iteration 653, loss = 62.70827341\n",
            "Iteration 654, loss = 62.61286884\n",
            "Iteration 655, loss = 62.51627224\n",
            "Iteration 656, loss = 62.42587316\n",
            "Iteration 657, loss = 62.32848875\n",
            "Iteration 658, loss = 62.23597441\n",
            "Iteration 659, loss = 62.14126712\n",
            "Iteration 660, loss = 62.04800611\n",
            "Iteration 661, loss = 61.96173731\n",
            "Iteration 662, loss = 61.86496720\n",
            "Iteration 663, loss = 61.77238604\n",
            "Iteration 664, loss = 61.68332713\n",
            "Iteration 665, loss = 61.58998178\n",
            "Iteration 666, loss = 61.49964149\n",
            "Iteration 667, loss = 61.40961041\n",
            "Iteration 668, loss = 61.32320988\n",
            "Iteration 669, loss = 61.23395478\n",
            "Iteration 670, loss = 61.14116901\n",
            "Iteration 671, loss = 61.05368234\n",
            "Iteration 672, loss = 60.96533826\n",
            "Iteration 673, loss = 60.87620930\n",
            "Iteration 674, loss = 60.78843899\n",
            "Iteration 675, loss = 60.70035652\n",
            "Iteration 676, loss = 60.61737007\n",
            "Iteration 677, loss = 60.52726235\n",
            "Iteration 678, loss = 60.44007136\n",
            "Iteration 679, loss = 60.35866450\n",
            "Iteration 680, loss = 60.26964675\n",
            "Iteration 681, loss = 60.18104432\n",
            "Iteration 682, loss = 60.10086910\n",
            "Iteration 683, loss = 60.01433244\n",
            "Iteration 684, loss = 59.93125789\n",
            "Iteration 685, loss = 59.84780081\n",
            "Iteration 686, loss = 59.76698565\n",
            "Iteration 687, loss = 59.67882454\n",
            "Iteration 688, loss = 59.59985623\n",
            "Iteration 689, loss = 59.51663544\n",
            "Iteration 690, loss = 59.43533463\n",
            "Iteration 691, loss = 59.35289744\n",
            "Iteration 692, loss = 59.26994524\n",
            "Iteration 693, loss = 59.18955733\n",
            "Iteration 694, loss = 59.11292896\n",
            "Iteration 695, loss = 59.03217826\n",
            "Iteration 696, loss = 58.95047895\n",
            "Iteration 697, loss = 58.87105227\n",
            "Iteration 698, loss = 58.78972775\n",
            "Iteration 699, loss = 58.71321882\n",
            "Iteration 700, loss = 58.63577694\n",
            "Iteration 701, loss = 58.55631860\n",
            "Iteration 702, loss = 58.47847736\n",
            "Iteration 703, loss = 58.39917963\n",
            "Iteration 704, loss = 58.32325037\n",
            "Iteration 705, loss = 58.24532998\n",
            "Iteration 706, loss = 58.17111479\n",
            "Iteration 707, loss = 58.09340954\n",
            "Iteration 708, loss = 58.01813141\n",
            "Iteration 709, loss = 57.94105289\n",
            "Iteration 710, loss = 57.86584390\n",
            "Iteration 711, loss = 57.79133250\n",
            "Iteration 712, loss = 57.71669018\n",
            "Iteration 713, loss = 57.64324920\n",
            "Iteration 714, loss = 57.56867592\n",
            "Iteration 715, loss = 57.49493728\n",
            "Iteration 716, loss = 57.42074471\n",
            "Iteration 717, loss = 57.34921175\n",
            "Iteration 718, loss = 57.27633706\n",
            "Iteration 719, loss = 57.20593788\n",
            "Iteration 720, loss = 57.13103746\n",
            "Iteration 721, loss = 57.06148357\n",
            "Iteration 722, loss = 56.98611560\n",
            "Iteration 723, loss = 56.91866514\n",
            "Iteration 724, loss = 56.84753164\n",
            "Iteration 725, loss = 56.77448567\n",
            "Iteration 726, loss = 56.70669647\n",
            "Iteration 727, loss = 56.63789398\n",
            "Iteration 728, loss = 56.56629297\n",
            "Iteration 729, loss = 56.49544476\n",
            "Iteration 730, loss = 56.42825390\n",
            "Iteration 731, loss = 56.36005097\n",
            "Iteration 732, loss = 56.29033472\n",
            "Iteration 733, loss = 56.22313519\n",
            "Iteration 734, loss = 56.15702247\n",
            "Iteration 735, loss = 56.08712354\n",
            "Iteration 736, loss = 56.02423129\n",
            "Iteration 737, loss = 55.95653753\n",
            "Iteration 738, loss = 55.88992199\n",
            "Iteration 739, loss = 55.82147889\n",
            "Iteration 740, loss = 55.75494428\n",
            "Iteration 741, loss = 55.68921568\n",
            "Iteration 742, loss = 55.62794815\n",
            "Iteration 743, loss = 55.56289717\n",
            "Iteration 744, loss = 55.49613917\n",
            "Iteration 745, loss = 55.43398598\n",
            "Iteration 746, loss = 55.37125129\n",
            "Iteration 747, loss = 55.30249828\n",
            "Iteration 748, loss = 55.24069481\n",
            "Iteration 749, loss = 55.17906626\n",
            "Iteration 750, loss = 55.11422762\n",
            "Iteration 751, loss = 55.05366994\n",
            "Iteration 752, loss = 54.99113055\n",
            "Iteration 753, loss = 54.93164047\n",
            "Iteration 754, loss = 54.87022027\n",
            "Iteration 755, loss = 54.80637525\n",
            "Iteration 756, loss = 54.74523759\n",
            "Iteration 757, loss = 54.68364936\n",
            "Iteration 758, loss = 54.62618995\n",
            "Iteration 759, loss = 54.56581775\n",
            "Iteration 760, loss = 54.50588180\n",
            "Iteration 761, loss = 54.44502117\n",
            "Iteration 762, loss = 54.38708390\n",
            "Iteration 763, loss = 54.32656044\n",
            "Iteration 764, loss = 54.26648300\n",
            "Iteration 765, loss = 54.21069964\n",
            "Iteration 766, loss = 54.15193279\n",
            "Iteration 767, loss = 54.09187665\n",
            "Iteration 768, loss = 54.03596034\n",
            "Iteration 769, loss = 53.97816063\n",
            "Iteration 770, loss = 53.92160420\n",
            "Iteration 771, loss = 53.86000535\n",
            "Iteration 772, loss = 53.80682107\n",
            "Iteration 773, loss = 53.75113903\n",
            "Iteration 774, loss = 53.69004688\n",
            "Iteration 775, loss = 53.63964129\n",
            "Iteration 776, loss = 53.57767033\n",
            "Iteration 777, loss = 53.52543447\n",
            "Iteration 778, loss = 53.47007723\n",
            "Iteration 779, loss = 53.41440086\n",
            "Iteration 780, loss = 53.35983237\n",
            "Iteration 781, loss = 53.30719847\n",
            "Iteration 782, loss = 53.25128601\n",
            "Iteration 783, loss = 53.19870101\n",
            "Iteration 784, loss = 53.14124612\n",
            "Iteration 785, loss = 53.09233811\n",
            "Iteration 786, loss = 53.03994342\n",
            "Iteration 787, loss = 52.98266420\n",
            "Iteration 788, loss = 52.93127163\n",
            "Iteration 789, loss = 52.88235821\n",
            "Iteration 790, loss = 52.82747425\n",
            "Iteration 791, loss = 52.77553781\n",
            "Iteration 792, loss = 52.72488692\n",
            "Iteration 793, loss = 52.67262124\n",
            "Iteration 794, loss = 52.62329799\n",
            "Iteration 795, loss = 52.56940708\n",
            "Iteration 796, loss = 52.52030762\n",
            "Iteration 797, loss = 52.46848253\n",
            "Iteration 798, loss = 52.41769441\n",
            "Iteration 799, loss = 52.37106867\n",
            "Iteration 800, loss = 52.31822378\n",
            "Iteration 801, loss = 52.26855942\n",
            "Iteration 802, loss = 52.21905459\n",
            "Iteration 803, loss = 52.17091978\n",
            "Iteration 804, loss = 52.12024653\n",
            "Iteration 805, loss = 52.07447889\n",
            "Iteration 806, loss = 52.02489942\n",
            "Iteration 807, loss = 51.97382206\n",
            "Iteration 808, loss = 51.92721480\n",
            "Iteration 809, loss = 51.87894082\n",
            "Iteration 810, loss = 51.83129407\n",
            "Iteration 811, loss = 51.78464642\n",
            "Iteration 812, loss = 51.73698861\n",
            "Iteration 813, loss = 51.69104253\n",
            "Iteration 814, loss = 51.64407546\n",
            "Iteration 815, loss = 51.59499947\n",
            "Iteration 816, loss = 51.55064571\n",
            "Iteration 817, loss = 51.50602070\n",
            "Iteration 818, loss = 51.45818284\n",
            "Iteration 819, loss = 51.41457826\n",
            "Iteration 820, loss = 51.36895677\n",
            "Iteration 821, loss = 51.32355959\n",
            "Iteration 822, loss = 51.28024985\n",
            "Iteration 823, loss = 51.23556523\n",
            "Iteration 824, loss = 51.19011196\n",
            "Iteration 825, loss = 51.14524820\n",
            "Iteration 826, loss = 51.10080267\n",
            "Iteration 827, loss = 51.06108993\n",
            "Iteration 828, loss = 51.01382114\n",
            "Iteration 829, loss = 50.97177462\n",
            "Iteration 830, loss = 50.93440586\n",
            "Iteration 831, loss = 50.88629680\n",
            "Iteration 832, loss = 50.84448970\n",
            "Iteration 833, loss = 50.80325619\n",
            "Iteration 834, loss = 50.75953011\n",
            "Iteration 835, loss = 50.71817860\n",
            "Iteration 836, loss = 50.67586752\n",
            "Iteration 837, loss = 50.63712355\n",
            "Iteration 838, loss = 50.59513814\n",
            "Iteration 839, loss = 50.55471504\n",
            "Iteration 840, loss = 50.51012095\n",
            "Iteration 841, loss = 50.47182444\n",
            "Iteration 842, loss = 50.43368664\n",
            "Iteration 843, loss = 50.39207249\n",
            "Iteration 844, loss = 50.35433960\n",
            "Iteration 845, loss = 50.31193936\n",
            "Iteration 846, loss = 50.27452272\n",
            "Iteration 847, loss = 50.23759456\n",
            "Iteration 848, loss = 50.19591531\n",
            "Iteration 849, loss = 50.15736395\n",
            "Iteration 850, loss = 50.11871065\n",
            "Iteration 851, loss = 50.08135121\n",
            "Iteration 852, loss = 50.04465962\n",
            "Iteration 853, loss = 50.00628725\n",
            "Iteration 854, loss = 49.96719027\n",
            "Iteration 855, loss = 49.93126338\n",
            "Iteration 856, loss = 49.89101431\n",
            "Iteration 857, loss = 49.85663474\n",
            "Iteration 858, loss = 49.82120662\n",
            "Iteration 859, loss = 49.78118981\n",
            "Iteration 860, loss = 49.74624263\n",
            "Iteration 861, loss = 49.71042451\n",
            "Iteration 862, loss = 49.67373162\n",
            "Iteration 863, loss = 49.63718288\n",
            "Iteration 864, loss = 49.60101712\n",
            "Iteration 865, loss = 49.56381629\n",
            "Iteration 866, loss = 49.52875733\n",
            "Iteration 867, loss = 49.49498785\n",
            "Iteration 868, loss = 49.45781582\n",
            "Iteration 869, loss = 49.42348926\n",
            "Iteration 870, loss = 49.38846894\n",
            "Iteration 871, loss = 49.35568066\n",
            "Iteration 872, loss = 49.31965235\n",
            "Iteration 873, loss = 49.28613360\n",
            "Iteration 874, loss = 49.25286234\n",
            "Iteration 875, loss = 49.21566160\n",
            "Iteration 876, loss = 49.18393373\n",
            "Iteration 877, loss = 49.15021262\n",
            "Iteration 878, loss = 49.11690653\n",
            "Iteration 879, loss = 49.08475837\n",
            "Iteration 880, loss = 49.05262643\n",
            "Iteration 881, loss = 49.01759267\n",
            "Iteration 882, loss = 48.98495567\n",
            "Iteration 883, loss = 48.95304675\n",
            "Iteration 884, loss = 48.91982783\n",
            "Iteration 885, loss = 48.88960805\n",
            "Iteration 886, loss = 48.85607164\n",
            "Iteration 887, loss = 48.82594159\n",
            "Iteration 888, loss = 48.79307871\n",
            "Iteration 889, loss = 48.76003994\n",
            "Iteration 890, loss = 48.73098793\n",
            "Iteration 891, loss = 48.70009583\n",
            "Iteration 892, loss = 48.66943835\n",
            "Iteration 893, loss = 48.63632061\n",
            "Iteration 894, loss = 48.60582161\n",
            "Iteration 895, loss = 48.57443089\n",
            "Iteration 896, loss = 48.54607977\n",
            "Iteration 897, loss = 48.51750329\n",
            "Iteration 898, loss = 48.48700196\n",
            "Iteration 899, loss = 48.45554152\n",
            "Iteration 900, loss = 48.42665623\n",
            "Iteration 901, loss = 48.39459304\n",
            "Iteration 902, loss = 48.36906411\n",
            "Iteration 903, loss = 48.33811314\n",
            "Iteration 904, loss = 48.30965344\n",
            "Iteration 905, loss = 48.27908850\n",
            "Iteration 906, loss = 48.25296251\n",
            "Iteration 907, loss = 48.22325718\n",
            "Iteration 908, loss = 48.19470680\n",
            "Iteration 909, loss = 48.16688926\n",
            "Iteration 910, loss = 48.13793391\n",
            "Iteration 911, loss = 48.11248010\n",
            "Iteration 912, loss = 48.08381659\n",
            "Iteration 913, loss = 48.05493337\n",
            "Iteration 914, loss = 48.02845385\n",
            "Iteration 915, loss = 48.00102350\n",
            "Iteration 916, loss = 47.97371154\n",
            "Iteration 917, loss = 47.94684460\n",
            "Iteration 918, loss = 47.91974925\n",
            "Iteration 919, loss = 47.89296484\n",
            "Iteration 920, loss = 47.86890866\n",
            "Iteration 921, loss = 47.84084735\n",
            "Iteration 922, loss = 47.81338353\n",
            "Iteration 923, loss = 47.79080927\n",
            "Iteration 924, loss = 47.76190194\n",
            "Iteration 925, loss = 47.73920620\n",
            "Iteration 926, loss = 47.71197203\n",
            "Iteration 927, loss = 47.68659129\n",
            "Iteration 928, loss = 47.66112920\n",
            "Iteration 929, loss = 47.63454601\n",
            "Iteration 930, loss = 47.61096419\n",
            "Iteration 931, loss = 47.58721689\n",
            "Iteration 932, loss = 47.56158204\n",
            "Iteration 933, loss = 47.53818641\n",
            "Iteration 934, loss = 47.51344422\n",
            "Iteration 935, loss = 47.48888109\n",
            "Iteration 936, loss = 47.46563273\n",
            "Iteration 937, loss = 47.43958301\n",
            "Iteration 938, loss = 47.41779586\n",
            "Iteration 939, loss = 47.39440291\n",
            "Iteration 940, loss = 47.36927525\n",
            "Iteration 941, loss = 47.34292010\n",
            "Iteration 942, loss = 47.32402745\n",
            "Iteration 943, loss = 47.30005572\n",
            "Iteration 944, loss = 47.27365008\n",
            "Iteration 945, loss = 47.25318274\n",
            "Iteration 946, loss = 47.23064739\n",
            "Iteration 947, loss = 47.20474620\n",
            "Iteration 948, loss = 47.18469808\n",
            "Iteration 949, loss = 47.16005715\n",
            "Iteration 950, loss = 47.13511226\n",
            "Iteration 951, loss = 47.10662304\n",
            "Iteration 952, loss = 47.08291114\n",
            "Iteration 953, loss = 47.05341346\n",
            "Iteration 954, loss = 47.01442980\n",
            "Iteration 955, loss = 46.96089884\n",
            "Iteration 956, loss = 46.87899568\n",
            "Iteration 957, loss = 46.71589945\n",
            "Iteration 958, loss = 46.39836644\n",
            "Iteration 959, loss = 46.04211435\n",
            "Iteration 960, loss = 45.90257730\n",
            "Iteration 961, loss = 45.74180624\n",
            "Iteration 962, loss = 45.52262647\n",
            "Iteration 963, loss = 45.39431637\n",
            "Iteration 964, loss = 45.26120076\n",
            "Iteration 965, loss = 45.17289548\n",
            "Iteration 966, loss = 45.04749810\n",
            "Iteration 967, loss = 44.99656959\n",
            "Iteration 968, loss = 44.88844778\n",
            "Iteration 969, loss = 44.83306914\n",
            "Iteration 970, loss = 44.78337761\n",
            "Iteration 971, loss = 44.70613231\n",
            "Iteration 972, loss = 44.66843733\n",
            "Iteration 973, loss = 44.61022577\n",
            "Iteration 974, loss = 44.57122961\n",
            "Iteration 975, loss = 44.51961838\n",
            "Iteration 976, loss = 44.48503180\n",
            "Iteration 977, loss = 44.45139879\n",
            "Iteration 978, loss = 44.40764381\n",
            "Iteration 979, loss = 44.38814318\n",
            "Iteration 980, loss = 44.33394419\n",
            "Iteration 981, loss = 44.31502228\n",
            "Iteration 982, loss = 44.27101249\n",
            "Iteration 983, loss = 44.23733079\n",
            "Iteration 984, loss = 44.19772527\n",
            "Iteration 985, loss = 44.17139070\n",
            "Iteration 986, loss = 44.14093826\n",
            "Iteration 987, loss = 44.10538153\n",
            "Iteration 988, loss = 44.06862998\n",
            "Iteration 989, loss = 44.05112319\n",
            "Iteration 990, loss = 44.00877641\n",
            "Iteration 991, loss = 43.98828143\n",
            "Iteration 992, loss = 43.96749597\n",
            "Iteration 993, loss = 43.92337993\n",
            "Iteration 994, loss = 43.89831132\n",
            "Iteration 995, loss = 43.86675323\n",
            "Iteration 996, loss = 43.82779202\n",
            "Iteration 997, loss = 43.80014044\n",
            "Iteration 998, loss = 43.76745137\n",
            "Iteration 999, loss = 43.73983939\n",
            "Iteration 1000, loss = 43.69967246\n",
            "Iteration 1001, loss = 43.66423991\n",
            "Iteration 1002, loss = 43.62088201\n",
            "Iteration 1003, loss = 43.57023908\n",
            "Iteration 1004, loss = 43.50199043\n",
            "Iteration 1005, loss = 43.40719366\n",
            "Iteration 1006, loss = 43.26885485\n",
            "Iteration 1007, loss = 43.02916920\n",
            "Iteration 1008, loss = 42.67143393\n",
            "Iteration 1009, loss = 42.26727435\n",
            "Iteration 1010, loss = 41.89178825\n",
            "Iteration 1011, loss = 41.68765218\n",
            "Iteration 1012, loss = 41.45086483\n",
            "Iteration 1013, loss = 41.12268967\n",
            "Iteration 1014, loss = 40.77243103\n",
            "Iteration 1015, loss = 40.47643442\n",
            "Iteration 1016, loss = 40.15646225\n",
            "Iteration 1017, loss = 39.93494005\n",
            "Iteration 1018, loss = 39.76456973\n",
            "Iteration 1019, loss = 39.53839666\n",
            "Iteration 1020, loss = 39.34102229\n",
            "Iteration 1021, loss = 39.12619230\n",
            "Iteration 1022, loss = 38.92308360\n",
            "Iteration 1023, loss = 38.80498711\n",
            "Iteration 1024, loss = 38.59992183\n",
            "Iteration 1025, loss = 38.59338927\n",
            "Iteration 1026, loss = 38.36076895\n",
            "Iteration 1027, loss = 38.25973588\n",
            "Iteration 1028, loss = 38.17113982\n",
            "Iteration 1029, loss = 38.03707211\n",
            "Iteration 1030, loss = 37.96655722\n",
            "Iteration 1031, loss = 37.92553967\n",
            "Iteration 1032, loss = 37.82531580\n",
            "Iteration 1033, loss = 37.79085358\n",
            "Iteration 1034, loss = 37.79012618\n",
            "Iteration 1035, loss = 37.66809484\n",
            "Iteration 1036, loss = 37.59415772\n",
            "Iteration 1037, loss = 37.47292642\n",
            "Iteration 1038, loss = 37.50568064\n",
            "Iteration 1039, loss = 37.31677401\n",
            "Iteration 1040, loss = 37.34676952\n",
            "Iteration 1041, loss = 37.21130481\n",
            "Iteration 1042, loss = 37.16094135\n",
            "Iteration 1043, loss = 37.11021288\n",
            "Iteration 1044, loss = 37.01321059\n",
            "Iteration 1045, loss = 37.02901989\n",
            "Iteration 1046, loss = 36.91756649\n",
            "Iteration 1047, loss = 36.90442869\n",
            "Iteration 1048, loss = 36.81880118\n",
            "Iteration 1049, loss = 36.77632869\n",
            "Iteration 1050, loss = 36.86338392\n",
            "Iteration 1051, loss = 36.66202348\n",
            "Iteration 1052, loss = 36.59465407\n",
            "Iteration 1053, loss = 36.52337194\n",
            "Iteration 1054, loss = 36.46680713\n",
            "Iteration 1055, loss = 36.39225419\n",
            "Iteration 1056, loss = 36.43774495\n",
            "Iteration 1057, loss = 36.28046558\n",
            "Iteration 1058, loss = 36.26437814\n",
            "Iteration 1059, loss = 36.11438482\n",
            "Iteration 1060, loss = 36.07355742\n",
            "Iteration 1061, loss = 35.94643241\n",
            "Iteration 1062, loss = 35.90517834\n",
            "Iteration 1063, loss = 35.80467370\n",
            "Iteration 1064, loss = 35.73606815\n",
            "Iteration 1065, loss = 35.68195997\n",
            "Iteration 1066, loss = 35.54714351\n",
            "Iteration 1067, loss = 35.47678845\n",
            "Iteration 1068, loss = 35.38760328\n",
            "Iteration 1069, loss = 35.30635501\n",
            "Iteration 1070, loss = 35.20537286\n",
            "Iteration 1071, loss = 35.13289771\n",
            "Iteration 1072, loss = 35.02396462\n",
            "Iteration 1073, loss = 34.97463316\n",
            "Iteration 1074, loss = 34.87289192\n",
            "Iteration 1075, loss = 34.80760861\n",
            "Iteration 1076, loss = 34.70010230\n",
            "Iteration 1077, loss = 34.63911244\n",
            "Iteration 1078, loss = 34.58089433\n",
            "Iteration 1079, loss = 34.46231960\n",
            "Iteration 1080, loss = 34.43017510\n",
            "Iteration 1081, loss = 34.35394860\n",
            "Iteration 1082, loss = 34.28617354\n",
            "Iteration 1083, loss = 34.22390460\n",
            "Iteration 1084, loss = 34.14742273\n",
            "Iteration 1085, loss = 34.07715532\n",
            "Iteration 1086, loss = 34.03392149\n",
            "Iteration 1087, loss = 33.93841312\n",
            "Iteration 1088, loss = 33.88349011\n",
            "Iteration 1089, loss = 33.81601493\n",
            "Iteration 1090, loss = 33.78897399\n",
            "Iteration 1091, loss = 33.71807823\n",
            "Iteration 1092, loss = 33.67218311\n",
            "Iteration 1093, loss = 33.59769913\n",
            "Iteration 1094, loss = 33.55120113\n",
            "Iteration 1095, loss = 33.52812699\n",
            "Iteration 1096, loss = 33.49877327\n",
            "Iteration 1097, loss = 33.38526462\n",
            "Iteration 1098, loss = 33.37076536\n",
            "Iteration 1099, loss = 33.29534733\n",
            "Iteration 1100, loss = 33.24735556\n",
            "Iteration 1101, loss = 33.20359866\n",
            "Iteration 1102, loss = 33.14984780\n",
            "Iteration 1103, loss = 33.16288383\n",
            "Iteration 1104, loss = 33.06217645\n",
            "Iteration 1105, loss = 33.05904169\n",
            "Iteration 1106, loss = 32.95438544\n",
            "Iteration 1107, loss = 32.94986459\n",
            "Iteration 1108, loss = 32.86011805\n",
            "Iteration 1109, loss = 32.88219598\n",
            "Iteration 1110, loss = 32.74678308\n",
            "Iteration 1111, loss = 32.71942596\n",
            "Iteration 1112, loss = 32.67327472\n",
            "Iteration 1113, loss = 32.62070258\n",
            "Iteration 1114, loss = 32.65225594\n",
            "Iteration 1115, loss = 32.51407302\n",
            "Iteration 1116, loss = 32.53839209\n",
            "Iteration 1117, loss = 32.45448881\n",
            "Iteration 1118, loss = 32.51141892\n",
            "Iteration 1119, loss = 32.36757166\n",
            "Iteration 1120, loss = 32.28282848\n",
            "Iteration 1121, loss = 32.24320185\n",
            "Iteration 1122, loss = 32.20712306\n",
            "Iteration 1123, loss = 32.14452517\n",
            "Iteration 1124, loss = 32.10539741\n",
            "Iteration 1125, loss = 32.06452264\n",
            "Iteration 1126, loss = 32.00683706\n",
            "Iteration 1127, loss = 31.98601236\n",
            "Iteration 1128, loss = 31.95843009\n",
            "Iteration 1129, loss = 31.90025178\n",
            "Iteration 1130, loss = 31.85124603\n",
            "Iteration 1131, loss = 31.79762661\n",
            "Iteration 1132, loss = 31.75133974\n",
            "Iteration 1133, loss = 31.70811383\n",
            "Iteration 1134, loss = 31.66605074\n",
            "Iteration 1135, loss = 31.62499801\n",
            "Iteration 1136, loss = 31.57624132\n",
            "Iteration 1137, loss = 31.53127303\n",
            "Iteration 1138, loss = 31.50109724\n",
            "Iteration 1139, loss = 31.45394598\n",
            "Iteration 1140, loss = 31.40482717\n",
            "Iteration 1141, loss = 31.38058240\n",
            "Iteration 1142, loss = 31.31823165\n",
            "Iteration 1143, loss = 31.29140842\n",
            "Iteration 1144, loss = 31.21722375\n",
            "Iteration 1145, loss = 31.18620472\n",
            "Iteration 1146, loss = 31.11317767\n",
            "Iteration 1147, loss = 31.09433958\n",
            "Iteration 1148, loss = 31.05482188\n",
            "Iteration 1149, loss = 30.96961200\n",
            "Iteration 1150, loss = 30.94031543\n",
            "Iteration 1151, loss = 30.85390417\n",
            "Iteration 1152, loss = 30.82876453\n",
            "Iteration 1153, loss = 30.84649094\n",
            "Iteration 1154, loss = 30.75559258\n",
            "Iteration 1155, loss = 30.68369686\n",
            "Iteration 1156, loss = 30.64288209\n",
            "Iteration 1157, loss = 30.58145009\n",
            "Iteration 1158, loss = 30.54433804\n",
            "Iteration 1159, loss = 30.47295818\n",
            "Iteration 1160, loss = 30.42445781\n",
            "Iteration 1161, loss = 30.37693825\n",
            "Iteration 1162, loss = 30.35174391\n",
            "Iteration 1163, loss = 30.30434079\n",
            "Iteration 1164, loss = 30.25809347\n",
            "Iteration 1165, loss = 30.20922819\n",
            "Iteration 1166, loss = 30.17089550\n",
            "Iteration 1167, loss = 30.13960023\n",
            "Iteration 1168, loss = 30.08552840\n",
            "Iteration 1169, loss = 30.05249067\n",
            "Iteration 1170, loss = 29.99869451\n",
            "Iteration 1171, loss = 29.99481693\n",
            "Iteration 1172, loss = 29.94606699\n",
            "Iteration 1173, loss = 29.93201721\n",
            "Iteration 1174, loss = 29.85833742\n",
            "Iteration 1175, loss = 29.80131988\n",
            "Iteration 1176, loss = 29.77519987\n",
            "Iteration 1177, loss = 29.72094695\n",
            "Iteration 1178, loss = 29.70302612\n",
            "Iteration 1179, loss = 29.65649989\n",
            "Iteration 1180, loss = 29.60194615\n",
            "Iteration 1181, loss = 29.59373568\n",
            "Iteration 1182, loss = 29.52788345\n",
            "Iteration 1183, loss = 29.51309283\n",
            "Iteration 1184, loss = 29.44626637\n",
            "Iteration 1185, loss = 29.43930106\n",
            "Iteration 1186, loss = 29.39432514\n",
            "Iteration 1187, loss = 29.32884063\n",
            "Iteration 1188, loss = 29.35497870\n",
            "Iteration 1189, loss = 29.28691855\n",
            "Iteration 1190, loss = 29.23966889\n",
            "Iteration 1191, loss = 29.20526996\n",
            "Iteration 1192, loss = 29.13837939\n",
            "Iteration 1193, loss = 29.12688231\n",
            "Iteration 1194, loss = 29.09116837\n",
            "Iteration 1195, loss = 29.03257429\n",
            "Iteration 1196, loss = 29.00753234\n",
            "Iteration 1197, loss = 28.99020367\n",
            "Iteration 1198, loss = 28.92686296\n",
            "Iteration 1199, loss = 28.91321479\n",
            "Iteration 1200, loss = 28.93936230\n",
            "Iteration 1201, loss = 28.83917261\n",
            "Iteration 1202, loss = 28.78886792\n",
            "Iteration 1203, loss = 28.74996424\n",
            "Iteration 1204, loss = 28.71681669\n",
            "Iteration 1205, loss = 28.67097829\n",
            "Iteration 1206, loss = 28.64575689\n",
            "Iteration 1207, loss = 28.59016535\n",
            "Iteration 1208, loss = 28.58600725\n",
            "Iteration 1209, loss = 28.54825119\n",
            "Iteration 1210, loss = 28.49993391\n",
            "Iteration 1211, loss = 28.50781463\n",
            "Iteration 1212, loss = 28.43356425\n",
            "Iteration 1213, loss = 28.37202237\n",
            "Iteration 1214, loss = 28.35973989\n",
            "Iteration 1215, loss = 28.30105906\n",
            "Iteration 1216, loss = 28.26255903\n",
            "Iteration 1217, loss = 28.23998769\n",
            "Iteration 1218, loss = 28.24118321\n",
            "Iteration 1219, loss = 28.17782835\n",
            "Iteration 1220, loss = 28.17060944\n",
            "Iteration 1221, loss = 28.09686243\n",
            "Iteration 1222, loss = 28.05409689\n",
            "Iteration 1223, loss = 28.02052708\n",
            "Iteration 1224, loss = 27.99846662\n",
            "Iteration 1225, loss = 27.95436479\n",
            "Iteration 1226, loss = 27.92489920\n",
            "Iteration 1227, loss = 27.89299361\n",
            "Iteration 1228, loss = 27.85205559\n",
            "Iteration 1229, loss = 27.81441564\n",
            "Iteration 1230, loss = 27.79683449\n",
            "Iteration 1231, loss = 27.75197358\n",
            "Iteration 1232, loss = 27.72385960\n",
            "Iteration 1233, loss = 27.68278514\n",
            "Iteration 1234, loss = 27.67012231\n",
            "Iteration 1235, loss = 27.66337370\n",
            "Iteration 1236, loss = 27.61225886\n",
            "Iteration 1237, loss = 27.57046208\n",
            "Iteration 1238, loss = 27.55634439\n",
            "Iteration 1239, loss = 27.48371089\n",
            "Iteration 1240, loss = 27.48481566\n",
            "Iteration 1241, loss = 27.42515654\n",
            "Iteration 1242, loss = 27.40425670\n",
            "Iteration 1243, loss = 27.35713188\n",
            "Iteration 1244, loss = 27.31890414\n",
            "Iteration 1245, loss = 27.31185656\n",
            "Iteration 1246, loss = 27.24097696\n",
            "Iteration 1247, loss = 27.22482157\n",
            "Iteration 1248, loss = 27.17107400\n",
            "Iteration 1249, loss = 27.14577207\n",
            "Iteration 1250, loss = 27.10834036\n",
            "Iteration 1251, loss = 27.06300116\n",
            "Iteration 1252, loss = 27.03928226\n",
            "Iteration 1253, loss = 26.97937983\n",
            "Iteration 1254, loss = 26.95044389\n",
            "Iteration 1255, loss = 26.92860529\n",
            "Iteration 1256, loss = 26.86161442\n",
            "Iteration 1257, loss = 26.86707581\n",
            "Iteration 1258, loss = 26.80895505\n",
            "Iteration 1259, loss = 26.80257734\n",
            "Iteration 1260, loss = 26.76396738\n",
            "Iteration 1261, loss = 26.70892689\n",
            "Iteration 1262, loss = 26.71280133\n",
            "Iteration 1263, loss = 26.62083592\n",
            "Iteration 1264, loss = 26.60583431\n",
            "Iteration 1265, loss = 26.56096592\n",
            "Iteration 1266, loss = 26.52570796\n",
            "Iteration 1267, loss = 26.50417951\n",
            "Iteration 1268, loss = 26.44826042\n",
            "Iteration 1269, loss = 26.45050591\n",
            "Iteration 1270, loss = 26.41971677\n",
            "Iteration 1271, loss = 26.35623986\n",
            "Iteration 1272, loss = 26.32613227\n",
            "Iteration 1273, loss = 26.29160646\n",
            "Iteration 1274, loss = 26.27048996\n",
            "Iteration 1275, loss = 26.22722005\n",
            "Iteration 1276, loss = 26.21727737\n",
            "Iteration 1277, loss = 26.18208370\n",
            "Iteration 1278, loss = 26.13672072\n",
            "Iteration 1279, loss = 26.11927727\n",
            "Iteration 1280, loss = 26.07933780\n",
            "Iteration 1281, loss = 26.03640328\n",
            "Iteration 1282, loss = 26.00983459\n",
            "Iteration 1283, loss = 25.97284926\n",
            "Iteration 1284, loss = 25.96658006\n",
            "Iteration 1285, loss = 25.94129455\n",
            "Iteration 1286, loss = 25.88707106\n",
            "Iteration 1287, loss = 25.86325248\n",
            "Iteration 1288, loss = 25.84024095\n",
            "Iteration 1289, loss = 25.79839487\n",
            "Iteration 1290, loss = 25.76683275\n",
            "Iteration 1291, loss = 25.74189521\n",
            "Iteration 1292, loss = 25.75555985\n",
            "Iteration 1293, loss = 25.69130225\n",
            "Iteration 1294, loss = 25.65862282\n",
            "Iteration 1295, loss = 25.63342399\n",
            "Iteration 1296, loss = 25.57713741\n",
            "Iteration 1297, loss = 25.58590624\n",
            "Iteration 1298, loss = 25.53462313\n",
            "Iteration 1299, loss = 25.53501767\n",
            "Iteration 1300, loss = 25.48210956\n",
            "Iteration 1301, loss = 25.44982731\n",
            "Iteration 1302, loss = 25.49196935\n",
            "Iteration 1303, loss = 25.37228514\n",
            "Iteration 1304, loss = 25.40875716\n",
            "Iteration 1305, loss = 25.32185937\n",
            "Iteration 1306, loss = 25.32484831\n",
            "Iteration 1307, loss = 25.26753193\n",
            "Iteration 1308, loss = 25.25106736\n",
            "Iteration 1309, loss = 25.23198833\n",
            "Iteration 1310, loss = 25.19524020\n",
            "Iteration 1311, loss = 25.18766959\n",
            "Iteration 1312, loss = 25.12990116\n",
            "Iteration 1313, loss = 25.10748077\n",
            "Iteration 1314, loss = 25.07653888\n",
            "Iteration 1315, loss = 25.05463293\n",
            "Iteration 1316, loss = 25.01788919\n",
            "Iteration 1317, loss = 24.98325240\n",
            "Iteration 1318, loss = 24.96115433\n",
            "Iteration 1319, loss = 24.95073569\n",
            "Iteration 1320, loss = 24.92965700\n",
            "Iteration 1321, loss = 24.87726351\n",
            "Iteration 1322, loss = 24.88265572\n",
            "Iteration 1323, loss = 24.87827989\n",
            "Iteration 1324, loss = 24.82724896\n",
            "Iteration 1325, loss = 24.80756730\n",
            "Iteration 1326, loss = 24.80938295\n",
            "Iteration 1327, loss = 24.72346328\n",
            "Iteration 1328, loss = 24.73131125\n",
            "Iteration 1329, loss = 24.70915125\n",
            "Iteration 1330, loss = 24.63452637\n",
            "Iteration 1331, loss = 24.64083496\n",
            "Iteration 1332, loss = 24.61289257\n",
            "Iteration 1333, loss = 24.56352779\n",
            "Iteration 1334, loss = 24.54985759\n",
            "Iteration 1335, loss = 24.51089189\n",
            "Iteration 1336, loss = 24.52193737\n",
            "Iteration 1337, loss = 24.45802493\n",
            "Iteration 1338, loss = 24.42390116\n",
            "Iteration 1339, loss = 24.39498686\n",
            "Iteration 1340, loss = 24.36566556\n",
            "Iteration 1341, loss = 24.35218011\n",
            "Iteration 1342, loss = 24.32005318\n",
            "Iteration 1343, loss = 24.28530902\n",
            "Iteration 1344, loss = 24.26531892\n",
            "Iteration 1345, loss = 24.30009935\n",
            "Iteration 1346, loss = 24.19272237\n",
            "Iteration 1347, loss = 24.23913315\n",
            "Iteration 1348, loss = 24.19689290\n",
            "Iteration 1349, loss = 24.16804343\n",
            "Iteration 1350, loss = 24.19594132\n",
            "Iteration 1351, loss = 24.10588299\n",
            "Iteration 1352, loss = 24.16922484\n",
            "Iteration 1353, loss = 24.05429596\n",
            "Iteration 1354, loss = 24.03256451\n",
            "Iteration 1355, loss = 23.99844184\n",
            "Iteration 1356, loss = 23.99504140\n",
            "Iteration 1357, loss = 23.92456437\n",
            "Iteration 1358, loss = 23.93880451\n",
            "Iteration 1359, loss = 23.88593440\n",
            "Iteration 1360, loss = 23.87933997\n",
            "Iteration 1361, loss = 23.84843690\n",
            "Iteration 1362, loss = 23.81168384\n",
            "Iteration 1363, loss = 23.80117632\n",
            "Iteration 1364, loss = 23.76631051\n",
            "Iteration 1365, loss = 23.77244054\n",
            "Iteration 1366, loss = 23.70553853\n",
            "Iteration 1367, loss = 23.71132842\n",
            "Iteration 1368, loss = 23.65417253\n",
            "Iteration 1369, loss = 23.63902745\n",
            "Iteration 1370, loss = 23.65875302\n",
            "Iteration 1371, loss = 23.57604362\n",
            "Iteration 1372, loss = 23.57873841\n",
            "Iteration 1373, loss = 23.57359516\n",
            "Iteration 1374, loss = 23.52726855\n",
            "Iteration 1375, loss = 23.50724337\n",
            "Iteration 1376, loss = 23.47396450\n",
            "Iteration 1377, loss = 23.45734136\n",
            "Iteration 1378, loss = 23.44336619\n",
            "Iteration 1379, loss = 23.40253014\n",
            "Iteration 1380, loss = 23.39562215\n",
            "Iteration 1381, loss = 23.34619760\n",
            "Iteration 1382, loss = 23.35076674\n",
            "Iteration 1383, loss = 23.29810198\n",
            "Iteration 1384, loss = 23.29870416\n",
            "Iteration 1385, loss = 23.26889237\n",
            "Iteration 1386, loss = 23.25053144\n",
            "Iteration 1387, loss = 23.21818952\n",
            "Iteration 1388, loss = 23.18522807\n",
            "Iteration 1389, loss = 23.17402547\n",
            "Iteration 1390, loss = 23.15961361\n",
            "Iteration 1391, loss = 23.12032594\n",
            "Iteration 1392, loss = 23.11329008\n",
            "Iteration 1393, loss = 23.07628323\n",
            "Iteration 1394, loss = 23.06341252\n",
            "Iteration 1395, loss = 23.03693239\n",
            "Iteration 1396, loss = 23.06429002\n",
            "Iteration 1397, loss = 22.99973252\n",
            "Iteration 1398, loss = 22.97821689\n",
            "Iteration 1399, loss = 22.98637333\n",
            "Iteration 1400, loss = 22.92410959\n",
            "Iteration 1401, loss = 22.91212442\n",
            "Iteration 1402, loss = 22.90150749\n",
            "Iteration 1403, loss = 22.87888487\n",
            "Iteration 1404, loss = 22.84347288\n",
            "Iteration 1405, loss = 22.81648812\n",
            "Iteration 1406, loss = 22.83931094\n",
            "Iteration 1407, loss = 22.75915122\n",
            "Iteration 1408, loss = 22.77040916\n",
            "Iteration 1409, loss = 22.74368011\n",
            "Iteration 1410, loss = 22.72569073\n",
            "Iteration 1411, loss = 22.70462928\n",
            "Iteration 1412, loss = 22.67661294\n",
            "Iteration 1413, loss = 22.65180507\n",
            "Iteration 1414, loss = 22.63185116\n",
            "Iteration 1415, loss = 22.59308000\n",
            "Iteration 1416, loss = 22.57621187\n",
            "Iteration 1417, loss = 22.56085465\n",
            "Iteration 1418, loss = 22.55619739\n",
            "Iteration 1419, loss = 22.55054354\n",
            "Iteration 1420, loss = 22.48977758\n",
            "Iteration 1421, loss = 22.48754832\n",
            "Iteration 1422, loss = 22.45602823\n",
            "Iteration 1423, loss = 22.44539767\n",
            "Iteration 1424, loss = 22.41771915\n",
            "Iteration 1425, loss = 22.40597131\n",
            "Iteration 1426, loss = 22.36349338\n",
            "Iteration 1427, loss = 22.33840927\n",
            "Iteration 1428, loss = 22.31882867\n",
            "Iteration 1429, loss = 22.29220740\n",
            "Iteration 1430, loss = 22.27521936\n",
            "Iteration 1431, loss = 22.25841479\n",
            "Iteration 1432, loss = 22.24501538\n",
            "Iteration 1433, loss = 22.22897185\n",
            "Iteration 1434, loss = 22.20426515\n",
            "Iteration 1435, loss = 22.16351944\n",
            "Iteration 1436, loss = 22.15078179\n",
            "Iteration 1437, loss = 22.11766243\n",
            "Iteration 1438, loss = 22.10121552\n",
            "Iteration 1439, loss = 22.08680644\n",
            "Iteration 1440, loss = 22.04217801\n",
            "Iteration 1441, loss = 22.04817342\n",
            "Iteration 1442, loss = 22.00289171\n",
            "Iteration 1443, loss = 21.99932584\n",
            "Iteration 1444, loss = 21.95303723\n",
            "Iteration 1445, loss = 21.91924318\n",
            "Iteration 1446, loss = 21.89920548\n",
            "Iteration 1447, loss = 21.87996327\n",
            "Iteration 1448, loss = 21.82883830\n",
            "Iteration 1449, loss = 21.80816743\n",
            "Iteration 1450, loss = 21.70020165\n",
            "Iteration 1451, loss = 21.70481238\n",
            "Iteration 1452, loss = 21.58340759\n",
            "Iteration 1453, loss = 21.53230331\n",
            "Iteration 1454, loss = 21.45581005\n",
            "Iteration 1455, loss = 21.41221688\n",
            "Iteration 1456, loss = 21.36874991\n",
            "Iteration 1457, loss = 21.30889740\n",
            "Iteration 1458, loss = 21.26733934\n",
            "Iteration 1459, loss = 21.19946473\n",
            "Iteration 1460, loss = 21.17233914\n",
            "Iteration 1461, loss = 21.11351189\n",
            "Iteration 1462, loss = 21.09314272\n",
            "Iteration 1463, loss = 21.03823077\n",
            "Iteration 1464, loss = 21.02594185\n",
            "Iteration 1465, loss = 20.96550635\n",
            "Iteration 1466, loss = 20.91086993\n",
            "Iteration 1467, loss = 20.87523701\n",
            "Iteration 1468, loss = 20.84279972\n",
            "Iteration 1469, loss = 20.81167167\n",
            "Iteration 1470, loss = 20.78387664\n",
            "Iteration 1471, loss = 20.78753300\n",
            "Iteration 1472, loss = 20.70923290\n",
            "Iteration 1473, loss = 20.68369949\n",
            "Iteration 1474, loss = 20.66057056\n",
            "Iteration 1475, loss = 20.63032846\n",
            "Iteration 1476, loss = 20.60623020\n",
            "Iteration 1477, loss = 20.53658784\n",
            "Iteration 1478, loss = 20.50792084\n",
            "Iteration 1479, loss = 20.48999639\n",
            "Iteration 1480, loss = 20.44952401\n",
            "Iteration 1481, loss = 20.40204844\n",
            "Iteration 1482, loss = 20.39080290\n",
            "Iteration 1483, loss = 20.35727923\n",
            "Iteration 1484, loss = 20.32157755\n",
            "Iteration 1485, loss = 20.30330210\n",
            "Iteration 1486, loss = 20.25949917\n",
            "Iteration 1487, loss = 20.24790133\n",
            "Iteration 1488, loss = 20.28901687\n",
            "Iteration 1489, loss = 20.18549946\n",
            "Iteration 1490, loss = 20.15986494\n",
            "Iteration 1491, loss = 20.15328662\n",
            "Iteration 1492, loss = 20.11221437\n",
            "Iteration 1493, loss = 20.07732989\n",
            "Iteration 1494, loss = 20.09881524\n",
            "Iteration 1495, loss = 19.97667140\n",
            "Iteration 1496, loss = 20.02080224\n",
            "Iteration 1497, loss = 19.96214038\n",
            "Iteration 1498, loss = 19.93357356\n",
            "Iteration 1499, loss = 19.89482352\n",
            "Iteration 1500, loss = 19.88014120\n",
            "Iteration 1501, loss = 19.83988849\n",
            "Iteration 1502, loss = 19.81186017\n",
            "Iteration 1503, loss = 19.78324258\n",
            "Iteration 1504, loss = 19.75285300\n",
            "Iteration 1505, loss = 19.72136903\n",
            "Iteration 1506, loss = 19.70082001\n",
            "Iteration 1507, loss = 19.65957414\n",
            "Iteration 1508, loss = 19.63920767\n",
            "Iteration 1509, loss = 19.62743771\n",
            "Iteration 1510, loss = 19.58380508\n",
            "Iteration 1511, loss = 19.56377161\n",
            "Iteration 1512, loss = 19.51534058\n",
            "Iteration 1513, loss = 19.48553904\n",
            "Iteration 1514, loss = 19.46521058\n",
            "Iteration 1515, loss = 19.43042465\n",
            "Iteration 1516, loss = 19.40963126\n",
            "Iteration 1517, loss = 19.36964819\n",
            "Iteration 1518, loss = 19.36811968\n",
            "Iteration 1519, loss = 19.32455089\n",
            "Iteration 1520, loss = 19.30588290\n",
            "Iteration 1521, loss = 19.28965343\n",
            "Iteration 1522, loss = 19.24769799\n",
            "Iteration 1523, loss = 19.21153128\n",
            "Iteration 1524, loss = 19.21847281\n",
            "Iteration 1525, loss = 19.15402877\n",
            "Iteration 1526, loss = 19.17023137\n",
            "Iteration 1527, loss = 19.10463125\n",
            "Iteration 1528, loss = 19.10227636\n",
            "Iteration 1529, loss = 19.05005162\n",
            "Iteration 1530, loss = 19.03477259\n",
            "Iteration 1531, loss = 18.99951902\n",
            "Iteration 1532, loss = 18.96492797\n",
            "Iteration 1533, loss = 18.94521209\n",
            "Iteration 1534, loss = 18.92931253\n",
            "Iteration 1535, loss = 18.92088465\n",
            "Iteration 1536, loss = 18.86947022\n",
            "Iteration 1537, loss = 18.85023046\n",
            "Iteration 1538, loss = 18.85195853\n",
            "Iteration 1539, loss = 18.79431867\n",
            "Iteration 1540, loss = 18.79588692\n",
            "Iteration 1541, loss = 18.78346146\n",
            "Iteration 1542, loss = 18.71503306\n",
            "Iteration 1543, loss = 18.71070112\n",
            "Iteration 1544, loss = 18.65107918\n",
            "Iteration 1545, loss = 18.62240052\n",
            "Iteration 1546, loss = 18.59177397\n",
            "Iteration 1547, loss = 18.56831617\n",
            "Iteration 1548, loss = 18.53644491\n",
            "Iteration 1549, loss = 18.52936219\n",
            "Iteration 1550, loss = 18.49068244\n",
            "Iteration 1551, loss = 18.47002774\n",
            "Iteration 1552, loss = 18.43702578\n",
            "Iteration 1553, loss = 18.42530141\n",
            "Iteration 1554, loss = 18.37854254\n",
            "Iteration 1555, loss = 18.36239017\n",
            "Iteration 1556, loss = 18.34341591\n",
            "Iteration 1557, loss = 18.29924672\n",
            "Iteration 1558, loss = 18.27783623\n",
            "Iteration 1559, loss = 18.25873948\n",
            "Iteration 1560, loss = 18.24888651\n",
            "Iteration 1561, loss = 18.19064319\n",
            "Iteration 1562, loss = 18.21445643\n",
            "Iteration 1563, loss = 18.15070448\n",
            "Iteration 1564, loss = 18.16565154\n",
            "Iteration 1565, loss = 18.15213764\n",
            "Iteration 1566, loss = 18.08622014\n",
            "Iteration 1567, loss = 18.10710466\n",
            "Iteration 1568, loss = 18.03560801\n",
            "Iteration 1569, loss = 18.03521323\n",
            "Iteration 1570, loss = 18.03079038\n",
            "Iteration 1571, loss = 17.99087883\n",
            "Iteration 1572, loss = 17.95810408\n",
            "Iteration 1573, loss = 17.93751320\n",
            "Iteration 1574, loss = 17.88845114\n",
            "Iteration 1575, loss = 17.89944278\n",
            "Iteration 1576, loss = 17.83004751\n",
            "Iteration 1577, loss = 17.85666066\n",
            "Iteration 1578, loss = 17.79045189\n",
            "Iteration 1579, loss = 17.76965937\n",
            "Iteration 1580, loss = 17.73200168\n",
            "Iteration 1581, loss = 17.74432592\n",
            "Iteration 1582, loss = 17.67988334\n",
            "Iteration 1583, loss = 17.66259435\n",
            "Iteration 1584, loss = 17.62484401\n",
            "Iteration 1585, loss = 17.60793139\n",
            "Iteration 1586, loss = 17.58560821\n",
            "Iteration 1587, loss = 17.55479874\n",
            "Iteration 1588, loss = 17.53690369\n",
            "Iteration 1589, loss = 17.49486650\n",
            "Iteration 1590, loss = 17.50036784\n",
            "Iteration 1591, loss = 17.44890023\n",
            "Iteration 1592, loss = 17.42901086\n",
            "Iteration 1593, loss = 17.41964594\n",
            "Iteration 1594, loss = 17.40109857\n",
            "Iteration 1595, loss = 17.37879035\n",
            "Iteration 1596, loss = 17.34312832\n",
            "Iteration 1597, loss = 17.33129917\n",
            "Iteration 1598, loss = 17.30445795\n",
            "Iteration 1599, loss = 17.26980816\n",
            "Iteration 1600, loss = 17.27168304\n",
            "Iteration 1601, loss = 17.21051839\n",
            "Iteration 1602, loss = 17.19802231\n",
            "Iteration 1603, loss = 17.18803449\n",
            "Iteration 1604, loss = 17.13153782\n",
            "Iteration 1605, loss = 17.15981088\n",
            "Iteration 1606, loss = 17.09368829\n",
            "Iteration 1607, loss = 17.07934737\n",
            "Iteration 1608, loss = 17.06697105\n",
            "Iteration 1609, loss = 17.06164098\n",
            "Iteration 1610, loss = 17.12509238\n",
            "Iteration 1611, loss = 17.00370371\n",
            "Iteration 1612, loss = 16.97932044\n",
            "Iteration 1613, loss = 16.93236059\n",
            "Iteration 1614, loss = 16.93633614\n",
            "Iteration 1615, loss = 16.95453376\n",
            "Iteration 1616, loss = 16.88927521\n",
            "Iteration 1617, loss = 16.86893766\n",
            "Iteration 1618, loss = 16.82450556\n",
            "Iteration 1619, loss = 16.78624565\n",
            "Iteration 1620, loss = 16.77770368\n",
            "Iteration 1621, loss = 16.74637745\n",
            "Iteration 1622, loss = 16.71280660\n",
            "Iteration 1623, loss = 16.70200200\n",
            "Iteration 1624, loss = 16.66443806\n",
            "Iteration 1625, loss = 16.64946163\n",
            "Iteration 1626, loss = 16.62803647\n",
            "Iteration 1627, loss = 16.60407765\n",
            "Iteration 1628, loss = 16.61275337\n",
            "Iteration 1629, loss = 16.58112665\n",
            "Iteration 1630, loss = 16.56504952\n",
            "Iteration 1631, loss = 16.52771993\n",
            "Iteration 1632, loss = 16.50478793\n",
            "Iteration 1633, loss = 16.47134425\n",
            "Iteration 1634, loss = 16.45738062\n",
            "Iteration 1635, loss = 16.41305119\n",
            "Iteration 1636, loss = 16.48179189\n",
            "Iteration 1637, loss = 16.35168533\n",
            "Iteration 1638, loss = 16.45276693\n",
            "Iteration 1639, loss = 16.46379241\n",
            "Iteration 1640, loss = 16.32135085\n",
            "Iteration 1641, loss = 16.37463686\n",
            "Iteration 1642, loss = 16.23768526\n",
            "Iteration 1643, loss = 16.32525417\n",
            "Iteration 1644, loss = 16.24811121\n",
            "Iteration 1645, loss = 16.23593932\n",
            "Iteration 1646, loss = 16.28913432\n",
            "Iteration 1647, loss = 16.17227407\n",
            "Iteration 1648, loss = 16.26785583\n",
            "Iteration 1649, loss = 16.14053180\n",
            "Iteration 1650, loss = 16.10337212\n",
            "Iteration 1651, loss = 16.08405115\n",
            "Iteration 1652, loss = 16.09479520\n",
            "Iteration 1653, loss = 16.04745244\n",
            "Iteration 1654, loss = 16.03965333\n",
            "Iteration 1655, loss = 16.01777885\n",
            "Iteration 1656, loss = 16.01508092\n",
            "Iteration 1657, loss = 15.98208647\n",
            "Iteration 1658, loss = 15.92860146\n",
            "Iteration 1659, loss = 15.90713033\n",
            "Iteration 1660, loss = 15.88855352\n",
            "Iteration 1661, loss = 15.85588761\n",
            "Iteration 1662, loss = 15.83041386\n",
            "Iteration 1663, loss = 15.81566666\n",
            "Iteration 1664, loss = 15.79004590\n",
            "Iteration 1665, loss = 15.75784109\n",
            "Iteration 1666, loss = 15.75731480\n",
            "Iteration 1667, loss = 15.74432401\n",
            "Iteration 1668, loss = 15.71146642\n",
            "Iteration 1669, loss = 15.69088866\n",
            "Iteration 1670, loss = 15.67099729\n",
            "Iteration 1671, loss = 15.64506292\n",
            "Iteration 1672, loss = 15.62816090\n",
            "Iteration 1673, loss = 15.60298671\n",
            "Iteration 1674, loss = 15.60289366\n",
            "Iteration 1675, loss = 15.58431666\n",
            "Iteration 1676, loss = 15.54094314\n",
            "Iteration 1677, loss = 15.55381113\n",
            "Iteration 1678, loss = 15.53776257\n",
            "Iteration 1679, loss = 15.48381291\n",
            "Iteration 1680, loss = 15.52218246\n",
            "Iteration 1681, loss = 15.45646239\n",
            "Iteration 1682, loss = 15.43905882\n",
            "Iteration 1683, loss = 15.41952615\n",
            "Iteration 1684, loss = 15.37741451\n",
            "Iteration 1685, loss = 15.38881296\n",
            "Iteration 1686, loss = 15.33935945\n",
            "Iteration 1687, loss = 15.34592974\n",
            "Iteration 1688, loss = 15.30798035\n",
            "Iteration 1689, loss = 15.27789291\n",
            "Iteration 1690, loss = 15.26162565\n",
            "Iteration 1691, loss = 15.26752259\n",
            "Iteration 1692, loss = 15.21381895\n",
            "Iteration 1693, loss = 15.21399252\n",
            "Iteration 1694, loss = 15.23746218\n",
            "Iteration 1695, loss = 15.22900223\n",
            "Iteration 1696, loss = 15.16762644\n",
            "Iteration 1697, loss = 15.16792550\n",
            "Iteration 1698, loss = 15.08493467\n",
            "Iteration 1699, loss = 15.14780021\n",
            "Iteration 1700, loss = 15.10452211\n",
            "Iteration 1701, loss = 15.08351782\n",
            "Iteration 1702, loss = 15.03795661\n",
            "Iteration 1703, loss = 15.01805514\n",
            "Iteration 1704, loss = 14.98807345\n",
            "Iteration 1705, loss = 14.97898239\n",
            "Iteration 1706, loss = 14.96839168\n",
            "Iteration 1707, loss = 14.93724591\n",
            "Iteration 1708, loss = 14.91699349\n",
            "Iteration 1709, loss = 14.87378924\n",
            "Iteration 1710, loss = 14.94895112\n",
            "Iteration 1711, loss = 14.81964470\n",
            "Iteration 1712, loss = 14.93795188\n",
            "Iteration 1713, loss = 14.82177142\n",
            "Iteration 1714, loss = 14.85920327\n",
            "Iteration 1715, loss = 14.81578141\n",
            "Iteration 1716, loss = 14.74235308\n",
            "Iteration 1717, loss = 14.78216465\n",
            "Iteration 1718, loss = 14.72282462\n",
            "Iteration 1719, loss = 14.71623010\n",
            "Iteration 1720, loss = 14.68156980\n",
            "Iteration 1721, loss = 14.66202418\n",
            "Iteration 1722, loss = 14.63569184\n",
            "Iteration 1723, loss = 14.65418383\n",
            "Iteration 1724, loss = 14.61060959\n",
            "Iteration 1725, loss = 14.60030192\n",
            "Iteration 1726, loss = 14.57941352\n",
            "Iteration 1727, loss = 14.54932701\n",
            "Iteration 1728, loss = 14.54637277\n",
            "Iteration 1729, loss = 14.51462795\n",
            "Iteration 1730, loss = 14.48104175\n",
            "Iteration 1731, loss = 14.47102580\n",
            "Iteration 1732, loss = 14.45111029\n",
            "Iteration 1733, loss = 14.43986422\n",
            "Iteration 1734, loss = 14.40456083\n",
            "Iteration 1735, loss = 14.39244048\n",
            "Iteration 1736, loss = 14.36969066\n",
            "Iteration 1737, loss = 14.35593505\n",
            "Iteration 1738, loss = 14.32618770\n",
            "Iteration 1739, loss = 14.31031408\n",
            "Iteration 1740, loss = 14.28391842\n",
            "Iteration 1741, loss = 14.28466039\n",
            "Iteration 1742, loss = 14.25440620\n",
            "Iteration 1743, loss = 14.22710231\n",
            "Iteration 1744, loss = 14.21293427\n",
            "Iteration 1745, loss = 14.21406853\n",
            "Iteration 1746, loss = 14.16895631\n",
            "Iteration 1747, loss = 14.17312992\n",
            "Iteration 1748, loss = 14.13310526\n",
            "Iteration 1749, loss = 14.13490853\n",
            "Iteration 1750, loss = 14.13145203\n",
            "Iteration 1751, loss = 14.07919020\n",
            "Iteration 1752, loss = 14.11305227\n",
            "Iteration 1753, loss = 14.07129481\n",
            "Iteration 1754, loss = 14.03228426\n",
            "Iteration 1755, loss = 14.08812485\n",
            "Iteration 1756, loss = 13.99348574\n",
            "Iteration 1757, loss = 14.03237555\n",
            "Iteration 1758, loss = 13.98412466\n",
            "Iteration 1759, loss = 13.94754206\n",
            "Iteration 1760, loss = 13.98784974\n",
            "Iteration 1761, loss = 13.91184200\n",
            "Iteration 1762, loss = 13.93182036\n",
            "Iteration 1763, loss = 13.97376835\n",
            "Iteration 1764, loss = 13.85824639\n",
            "Iteration 1765, loss = 13.90241480\n",
            "Iteration 1766, loss = 13.85172028\n",
            "Iteration 1767, loss = 13.81294983\n",
            "Iteration 1768, loss = 13.80896471\n",
            "Iteration 1769, loss = 13.78766301\n",
            "Iteration 1770, loss = 13.77493031\n",
            "Iteration 1771, loss = 13.75566055\n",
            "Iteration 1772, loss = 13.73978228\n",
            "Iteration 1773, loss = 13.70248101\n",
            "Iteration 1774, loss = 13.69997280\n",
            "Iteration 1775, loss = 13.67965371\n",
            "Iteration 1776, loss = 13.66382252\n",
            "Iteration 1777, loss = 13.65825148\n",
            "Iteration 1778, loss = 13.61401796\n",
            "Iteration 1779, loss = 13.59220542\n",
            "Iteration 1780, loss = 13.58628589\n",
            "Iteration 1781, loss = 13.55285278\n",
            "Iteration 1782, loss = 13.56499180\n",
            "Iteration 1783, loss = 13.52211907\n",
            "Iteration 1784, loss = 13.50277274\n",
            "Iteration 1785, loss = 13.48442727\n",
            "Iteration 1786, loss = 13.48305401\n",
            "Iteration 1787, loss = 13.44336467\n",
            "Iteration 1788, loss = 13.44592123\n",
            "Iteration 1789, loss = 13.42331469\n",
            "Iteration 1790, loss = 13.39911125\n",
            "Iteration 1791, loss = 13.39630897\n",
            "Iteration 1792, loss = 13.37918000\n",
            "Iteration 1793, loss = 13.35221485\n",
            "Iteration 1794, loss = 13.33686447\n",
            "Iteration 1795, loss = 13.31780818\n",
            "Iteration 1796, loss = 13.29183488\n",
            "Iteration 1797, loss = 13.28345204\n",
            "Iteration 1798, loss = 13.29474093\n",
            "Iteration 1799, loss = 13.23612561\n",
            "Iteration 1800, loss = 13.27561666\n",
            "Iteration 1801, loss = 13.21221753\n",
            "Iteration 1802, loss = 13.25854570\n",
            "Iteration 1803, loss = 13.19391303\n",
            "Iteration 1804, loss = 13.17816612\n",
            "Iteration 1805, loss = 13.15672825\n",
            "Iteration 1806, loss = 13.14111319\n",
            "Iteration 1807, loss = 13.11834833\n",
            "Iteration 1808, loss = 13.11718756\n",
            "Iteration 1809, loss = 13.07882062\n",
            "Iteration 1810, loss = 13.06742958\n",
            "Iteration 1811, loss = 13.06168301\n",
            "Iteration 1812, loss = 13.02689792\n",
            "Iteration 1813, loss = 13.02898089\n",
            "Iteration 1814, loss = 13.01264861\n",
            "Iteration 1815, loss = 12.98221185\n",
            "Iteration 1816, loss = 12.96946821\n",
            "Iteration 1817, loss = 12.94338833\n",
            "Iteration 1818, loss = 12.92371158\n",
            "Iteration 1819, loss = 12.92293804\n",
            "Iteration 1820, loss = 12.90780331\n",
            "Iteration 1821, loss = 12.87316507\n",
            "Iteration 1822, loss = 12.86449195\n",
            "Iteration 1823, loss = 12.84931645\n",
            "Iteration 1824, loss = 12.82936631\n",
            "Iteration 1825, loss = 12.82034780\n",
            "Iteration 1826, loss = 12.80442468\n",
            "Iteration 1827, loss = 12.79566750\n",
            "Iteration 1828, loss = 12.76411018\n",
            "Iteration 1829, loss = 12.76252849\n",
            "Iteration 1830, loss = 12.76800346\n",
            "Iteration 1831, loss = 12.72576715\n",
            "Iteration 1832, loss = 12.70588472\n",
            "Iteration 1833, loss = 12.70627269\n",
            "Iteration 1834, loss = 12.65687436\n",
            "Iteration 1835, loss = 12.68883013\n",
            "Iteration 1836, loss = 12.67166252\n",
            "Iteration 1837, loss = 12.65660130\n",
            "Iteration 1838, loss = 12.63569313\n",
            "Iteration 1839, loss = 12.60769227\n",
            "Iteration 1840, loss = 12.59222048\n",
            "Iteration 1841, loss = 12.58424383\n",
            "Iteration 1842, loss = 12.54023443\n",
            "Iteration 1843, loss = 12.54148378\n",
            "Iteration 1844, loss = 12.54107236\n",
            "Iteration 1845, loss = 12.53759127\n",
            "Iteration 1846, loss = 12.49006379\n",
            "Iteration 1847, loss = 12.48606379\n",
            "Iteration 1848, loss = 12.44755398\n",
            "Iteration 1849, loss = 12.44292488\n",
            "Iteration 1850, loss = 12.41201922\n",
            "Iteration 1851, loss = 12.40593653\n",
            "Iteration 1852, loss = 12.39723647\n",
            "Iteration 1853, loss = 12.37605460\n",
            "Iteration 1854, loss = 12.36314166\n",
            "Iteration 1855, loss = 12.33177611\n",
            "Iteration 1856, loss = 12.33084614\n",
            "Iteration 1857, loss = 12.29717142\n",
            "Iteration 1858, loss = 12.29475040\n",
            "Iteration 1859, loss = 12.25210888\n",
            "Iteration 1860, loss = 12.26737504\n",
            "Iteration 1861, loss = 12.22014097\n",
            "Iteration 1862, loss = 12.21957582\n",
            "Iteration 1863, loss = 12.18444765\n",
            "Iteration 1864, loss = 12.17904680\n",
            "Iteration 1865, loss = 12.16467416\n",
            "Iteration 1866, loss = 12.14162918\n",
            "Iteration 1867, loss = 12.10940581\n",
            "Iteration 1868, loss = 12.09845797\n",
            "Iteration 1869, loss = 12.08867363\n",
            "Iteration 1870, loss = 12.05560593\n",
            "Iteration 1871, loss = 12.08473930\n",
            "Iteration 1872, loss = 12.01987900\n",
            "Iteration 1873, loss = 12.04125016\n",
            "Iteration 1874, loss = 12.02928759\n",
            "Iteration 1875, loss = 11.98147670\n",
            "Iteration 1876, loss = 12.01546649\n",
            "Iteration 1877, loss = 11.96163950\n",
            "Iteration 1878, loss = 11.91717122\n",
            "Iteration 1879, loss = 11.92640909\n",
            "Iteration 1880, loss = 11.90674791\n",
            "Iteration 1881, loss = 11.93105938\n",
            "Iteration 1882, loss = 11.86671496\n",
            "Iteration 1883, loss = 11.91449451\n",
            "Iteration 1884, loss = 11.85588771\n",
            "Iteration 1885, loss = 11.83251177\n",
            "Iteration 1886, loss = 11.89975491\n",
            "Iteration 1887, loss = 11.80891437\n",
            "Iteration 1888, loss = 11.85842984\n",
            "Iteration 1889, loss = 11.81481793\n",
            "Iteration 1890, loss = 11.72582587\n",
            "Iteration 1891, loss = 11.82164884\n",
            "Iteration 1892, loss = 11.75368362\n",
            "Iteration 1893, loss = 11.78136819\n",
            "Iteration 1894, loss = 11.74407660\n",
            "Iteration 1895, loss = 11.75324474\n",
            "Iteration 1896, loss = 11.72489278\n",
            "Iteration 1897, loss = 11.70065243\n",
            "Iteration 1898, loss = 11.66817076\n",
            "Iteration 1899, loss = 11.63633962\n",
            "Iteration 1900, loss = 11.65959679\n",
            "Iteration 1901, loss = 11.62886061\n",
            "Iteration 1902, loss = 11.59229042\n",
            "Iteration 1903, loss = 11.59206785\n",
            "Iteration 1904, loss = 11.60058910\n",
            "Iteration 1905, loss = 11.52261360\n",
            "Iteration 1906, loss = 11.55613702\n",
            "Iteration 1907, loss = 11.58238527\n",
            "Iteration 1908, loss = 11.45847021\n",
            "Iteration 1909, loss = 11.56229508\n",
            "Iteration 1910, loss = 11.45670690\n",
            "Iteration 1911, loss = 11.46782892\n",
            "Iteration 1912, loss = 11.47797246\n",
            "Iteration 1913, loss = 11.42764169\n",
            "Iteration 1914, loss = 11.42072922\n",
            "Iteration 1915, loss = 11.37465660\n",
            "Iteration 1916, loss = 11.46522589\n",
            "Iteration 1917, loss = 11.32523656\n",
            "Iteration 1918, loss = 11.34521774\n",
            "Iteration 1919, loss = 11.31094451\n",
            "Iteration 1920, loss = 11.32864849\n",
            "Iteration 1921, loss = 11.27681095\n",
            "Iteration 1922, loss = 11.28356531\n",
            "Iteration 1923, loss = 11.25474301\n",
            "Iteration 1924, loss = 11.21502995\n",
            "Iteration 1925, loss = 11.27250804\n",
            "Iteration 1926, loss = 11.22282950\n",
            "Iteration 1927, loss = 11.20082312\n",
            "Iteration 1928, loss = 11.19758152\n",
            "Iteration 1929, loss = 11.16885071\n",
            "Iteration 1930, loss = 11.16573312\n",
            "Iteration 1931, loss = 11.13843688\n",
            "Iteration 1932, loss = 11.11777267\n",
            "Iteration 1933, loss = 11.09054030\n",
            "Iteration 1934, loss = 11.08701641\n",
            "Iteration 1935, loss = 11.05564015\n",
            "Iteration 1936, loss = 11.04390818\n",
            "Iteration 1937, loss = 11.02818180\n",
            "Iteration 1938, loss = 11.01369570\n",
            "Iteration 1939, loss = 11.00254627\n",
            "Iteration 1940, loss = 10.99474230\n",
            "Iteration 1941, loss = 10.97481860\n",
            "Iteration 1942, loss = 10.96547113\n",
            "Iteration 1943, loss = 10.98535954\n",
            "Iteration 1944, loss = 10.93139997\n",
            "Iteration 1945, loss = 10.94444049\n",
            "Iteration 1946, loss = 10.92250772\n",
            "Iteration 1947, loss = 10.87688610\n",
            "Iteration 1948, loss = 10.91420280\n",
            "Iteration 1949, loss = 10.89838483\n",
            "Iteration 1950, loss = 10.90774098\n",
            "Iteration 1951, loss = 10.83727985\n",
            "Iteration 1952, loss = 10.85872224\n",
            "Iteration 1953, loss = 10.80822892\n",
            "Iteration 1954, loss = 10.81172394\n",
            "Iteration 1955, loss = 10.77951351\n",
            "Iteration 1956, loss = 10.74727629\n",
            "Iteration 1957, loss = 10.74979042\n",
            "Iteration 1958, loss = 10.72821477\n",
            "Iteration 1959, loss = 10.70997400\n",
            "Iteration 1960, loss = 10.68998497\n",
            "Iteration 1961, loss = 10.68566081\n",
            "Iteration 1962, loss = 10.68047250\n",
            "Iteration 1963, loss = 10.67387367\n",
            "Iteration 1964, loss = 10.65030857\n",
            "Iteration 1965, loss = 10.64444669\n",
            "Iteration 1966, loss = 10.61549589\n",
            "Iteration 1967, loss = 10.62410910\n",
            "Iteration 1968, loss = 10.60777724\n",
            "Iteration 1969, loss = 10.56666176\n",
            "Iteration 1970, loss = 10.59177927\n",
            "Iteration 1971, loss = 10.57658808\n",
            "Iteration 1972, loss = 10.53909675\n",
            "Iteration 1973, loss = 10.56442240\n",
            "Iteration 1974, loss = 10.50103667\n",
            "Iteration 1975, loss = 10.50726300\n",
            "Iteration 1976, loss = 10.54249222\n",
            "Iteration 1977, loss = 10.46523999\n",
            "Iteration 1978, loss = 10.50103083\n",
            "Iteration 1979, loss = 10.46418503\n",
            "Iteration 1980, loss = 10.45160961\n",
            "Iteration 1981, loss = 10.44083505\n",
            "Iteration 1982, loss = 10.41101089\n",
            "Iteration 1983, loss = 10.39230983\n",
            "Iteration 1984, loss = 10.37678216\n",
            "Iteration 1985, loss = 10.38418562\n",
            "Iteration 1986, loss = 10.34501286\n",
            "Iteration 1987, loss = 10.34140340\n",
            "Iteration 1988, loss = 10.33232309\n",
            "Iteration 1989, loss = 10.28907049\n",
            "Iteration 1990, loss = 10.31495068\n",
            "Iteration 1991, loss = 10.26923251\n",
            "Iteration 1992, loss = 10.27786561\n",
            "Iteration 1993, loss = 10.25485611\n",
            "Iteration 1994, loss = 10.24290908\n",
            "Iteration 1995, loss = 10.23086019\n",
            "Iteration 1996, loss = 10.20498074\n",
            "Iteration 1997, loss = 10.19643708\n",
            "Iteration 1998, loss = 10.17032758\n",
            "Iteration 1999, loss = 10.19376633\n",
            "Iteration 2000, loss = 10.14682924\n",
            "Iteration 2001, loss = 10.16964115\n",
            "Iteration 2002, loss = 10.15096569\n",
            "Iteration 2003, loss = 10.13793818\n",
            "Iteration 2004, loss = 10.14520416\n",
            "Iteration 2005, loss = 10.08250334\n",
            "Iteration 2006, loss = 10.07342758\n",
            "Iteration 2007, loss = 10.05768453\n",
            "Iteration 2008, loss = 10.04398319\n",
            "Iteration 2009, loss = 10.05692693\n",
            "Iteration 2010, loss = 10.01563672\n",
            "Iteration 2011, loss = 10.00471664\n",
            "Iteration 2012, loss = 10.01232547\n",
            "Iteration 2013, loss = 9.98373439\n",
            "Iteration 2014, loss = 9.99056315\n",
            "Iteration 2015, loss = 9.95834534\n",
            "Iteration 2016, loss = 9.93808694\n",
            "Iteration 2017, loss = 9.92705694\n",
            "Iteration 2018, loss = 9.92157785\n",
            "Iteration 2019, loss = 9.90386482\n",
            "Iteration 2020, loss = 9.88287263\n",
            "Iteration 2021, loss = 9.90202073\n",
            "Iteration 2022, loss = 9.90501910\n",
            "Iteration 2023, loss = 9.85019336\n",
            "Iteration 2024, loss = 9.88753601\n",
            "Iteration 2025, loss = 9.86900225\n",
            "Iteration 2026, loss = 9.83464722\n",
            "Iteration 2027, loss = 9.82695175\n",
            "Iteration 2028, loss = 9.80615326\n",
            "Iteration 2029, loss = 9.80647744\n",
            "Iteration 2030, loss = 9.87013763\n",
            "Iteration 2031, loss = 9.75580047\n",
            "Iteration 2032, loss = 9.76993378\n",
            "Iteration 2033, loss = 9.81908417\n",
            "Iteration 2034, loss = 9.73555256\n",
            "Iteration 2035, loss = 9.75962519\n",
            "Iteration 2036, loss = 9.80246038\n",
            "Iteration 2037, loss = 9.82357142\n",
            "Iteration 2038, loss = 9.73035331\n",
            "Iteration 2039, loss = 9.77654114\n",
            "Iteration 2040, loss = 9.83599499\n",
            "Iteration 2041, loss = 9.68099194\n",
            "Iteration 2042, loss = 9.75915222\n",
            "Iteration 2043, loss = 9.69303474\n",
            "Iteration 2044, loss = 9.62102895\n",
            "Iteration 2045, loss = 9.65733546\n",
            "Iteration 2046, loss = 9.65177414\n",
            "Iteration 2047, loss = 9.56339121\n",
            "Iteration 2048, loss = 9.60937651\n",
            "Iteration 2049, loss = 9.53835920\n",
            "Iteration 2050, loss = 9.52851316\n",
            "Iteration 2051, loss = 9.61938443\n",
            "Iteration 2052, loss = 9.49891014\n",
            "Iteration 2053, loss = 9.52819016\n",
            "Iteration 2054, loss = 9.53693116\n",
            "Iteration 2055, loss = 9.43370590\n",
            "Iteration 2056, loss = 9.48562578\n",
            "Iteration 2057, loss = 9.44773202\n",
            "Iteration 2058, loss = 9.41569742\n",
            "Iteration 2059, loss = 9.44854194\n",
            "Iteration 2060, loss = 9.37805323\n",
            "Iteration 2061, loss = 9.39009988\n",
            "Iteration 2062, loss = 9.39776048\n",
            "Iteration 2063, loss = 9.36674786\n",
            "Iteration 2064, loss = 9.35293619\n",
            "Iteration 2065, loss = 9.37175900\n",
            "Iteration 2066, loss = 9.34162823\n",
            "Iteration 2067, loss = 9.33153439\n",
            "Iteration 2068, loss = 9.30770391\n",
            "Iteration 2069, loss = 9.29905460\n",
            "Iteration 2070, loss = 9.25973035\n",
            "Iteration 2071, loss = 9.28008663\n",
            "Iteration 2072, loss = 9.26785375\n",
            "Iteration 2073, loss = 9.24527543\n",
            "Iteration 2074, loss = 9.24837678\n",
            "Iteration 2075, loss = 9.22244309\n",
            "Iteration 2076, loss = 9.21489134\n",
            "Iteration 2077, loss = 9.19015598\n",
            "Iteration 2078, loss = 9.18373766\n",
            "Iteration 2079, loss = 9.18464493\n",
            "Iteration 2080, loss = 9.14996526\n",
            "Iteration 2081, loss = 9.16778207\n",
            "Iteration 2082, loss = 9.14532143\n",
            "Iteration 2083, loss = 9.12687588\n",
            "Iteration 2084, loss = 9.16417790\n",
            "Iteration 2085, loss = 9.10657817\n",
            "Iteration 2086, loss = 9.12991724\n",
            "Iteration 2087, loss = 9.08709669\n",
            "Iteration 2088, loss = 9.05233868\n",
            "Iteration 2089, loss = 9.05151954\n",
            "Iteration 2090, loss = 9.05071697\n",
            "Iteration 2091, loss = 9.05239802\n",
            "Iteration 2092, loss = 9.00233438\n",
            "Iteration 2093, loss = 9.02350710\n",
            "Iteration 2094, loss = 8.99585404\n",
            "Iteration 2095, loss = 8.98335585\n",
            "Iteration 2096, loss = 8.99251000\n",
            "Iteration 2097, loss = 8.94729697\n",
            "Iteration 2098, loss = 8.95526661\n",
            "Iteration 2099, loss = 8.93821259\n",
            "Iteration 2100, loss = 8.91919117\n",
            "Iteration 2101, loss = 8.92574510\n",
            "Iteration 2102, loss = 8.89757842\n",
            "Iteration 2103, loss = 8.90318658\n",
            "Iteration 2104, loss = 8.87528409\n",
            "Iteration 2105, loss = 8.88052879\n",
            "Iteration 2106, loss = 8.84389923\n",
            "Iteration 2107, loss = 8.84536722\n",
            "Iteration 2108, loss = 8.82602659\n",
            "Iteration 2109, loss = 8.84104421\n",
            "Iteration 2110, loss = 8.80957844\n",
            "Iteration 2111, loss = 8.79932800\n",
            "Iteration 2112, loss = 8.78079612\n",
            "Iteration 2113, loss = 8.79637227\n",
            "Iteration 2114, loss = 8.76351347\n",
            "Iteration 2115, loss = 8.78237566\n",
            "Iteration 2116, loss = 8.75157856\n",
            "Iteration 2117, loss = 8.74639516\n",
            "Iteration 2118, loss = 8.76951791\n",
            "Iteration 2119, loss = 8.70263349\n",
            "Iteration 2120, loss = 8.73986507\n",
            "Iteration 2121, loss = 8.73565396\n",
            "Iteration 2122, loss = 8.66590288\n",
            "Iteration 2123, loss = 8.72159976\n",
            "Iteration 2124, loss = 8.77437856\n",
            "Iteration 2125, loss = 8.67535037\n",
            "Iteration 2126, loss = 8.70105228\n",
            "Iteration 2127, loss = 8.73515683\n",
            "Iteration 2128, loss = 8.77382143\n",
            "Iteration 2129, loss = 8.57955309\n",
            "Iteration 2130, loss = 8.75667222\n",
            "Iteration 2131, loss = 8.62091317\n",
            "Iteration 2132, loss = 8.63554404\n",
            "Iteration 2133, loss = 8.70461532\n",
            "Iteration 2134, loss = 8.63632597\n",
            "Iteration 2135, loss = 8.62202775\n",
            "Iteration 2136, loss = 8.60202319\n",
            "Iteration 2137, loss = 8.54786766\n",
            "Iteration 2138, loss = 8.54413419\n",
            "Iteration 2139, loss = 8.55507544\n",
            "Iteration 2140, loss = 8.51763064\n",
            "Iteration 2141, loss = 8.52570292\n",
            "Iteration 2142, loss = 8.48223074\n",
            "Iteration 2143, loss = 8.47069867\n",
            "Iteration 2144, loss = 8.46947168\n",
            "Iteration 2145, loss = 8.46457456\n",
            "Iteration 2146, loss = 8.41319216\n",
            "Iteration 2147, loss = 8.44051721\n",
            "Iteration 2148, loss = 8.40731788\n",
            "Iteration 2149, loss = 8.43179954\n",
            "Iteration 2150, loss = 8.38633402\n",
            "Iteration 2151, loss = 8.38066614\n",
            "Iteration 2152, loss = 8.40329575\n",
            "Iteration 2153, loss = 8.40945250\n",
            "Iteration 2154, loss = 8.37616512\n",
            "Iteration 2155, loss = 8.36941347\n",
            "Iteration 2156, loss = 8.37688519\n",
            "Iteration 2157, loss = 8.33284343\n",
            "Iteration 2158, loss = 8.30035164\n",
            "Iteration 2159, loss = 8.31018939\n",
            "Iteration 2160, loss = 8.28411269\n",
            "Iteration 2161, loss = 8.28834622\n",
            "Iteration 2162, loss = 8.28547141\n",
            "Iteration 2163, loss = 8.27559520\n",
            "Iteration 2164, loss = 8.26501630\n",
            "Iteration 2165, loss = 8.25263125\n",
            "Iteration 2166, loss = 8.22462209\n",
            "Iteration 2167, loss = 8.22717331\n",
            "Iteration 2168, loss = 8.20170323\n",
            "Iteration 2169, loss = 8.18937238\n",
            "Iteration 2170, loss = 8.18786482\n",
            "Iteration 2171, loss = 8.16170223\n",
            "Iteration 2172, loss = 8.16826662\n",
            "Iteration 2173, loss = 8.17385527\n",
            "Iteration 2174, loss = 8.20547702\n",
            "Iteration 2175, loss = 8.12815734\n",
            "Iteration 2176, loss = 8.16950756\n",
            "Iteration 2177, loss = 8.14699917\n",
            "Iteration 2178, loss = 8.12901661\n",
            "Iteration 2179, loss = 8.19219854\n",
            "Iteration 2180, loss = 8.14780203\n",
            "Iteration 2181, loss = 8.10666067\n",
            "Iteration 2182, loss = 8.14878671\n",
            "Iteration 2183, loss = 8.07866023\n",
            "Iteration 2184, loss = 8.05104037\n",
            "Iteration 2185, loss = 8.08899205\n",
            "Iteration 2186, loss = 8.08854441\n",
            "Iteration 2187, loss = 8.01553560\n",
            "Iteration 2188, loss = 8.06356966\n",
            "Iteration 2189, loss = 8.02736220\n",
            "Iteration 2190, loss = 8.01761200\n",
            "Iteration 2191, loss = 7.99327530\n",
            "Iteration 2192, loss = 7.99211066\n",
            "Iteration 2193, loss = 7.96149354\n",
            "Iteration 2194, loss = 7.97934331\n",
            "Iteration 2195, loss = 7.96338270\n",
            "Iteration 2196, loss = 7.93941980\n",
            "Iteration 2197, loss = 7.93639699\n",
            "Iteration 2198, loss = 7.92531631\n",
            "Iteration 2199, loss = 7.91557718\n",
            "Iteration 2200, loss = 7.96105593\n",
            "Iteration 2201, loss = 7.88184519\n",
            "Iteration 2202, loss = 7.91330311\n",
            "Iteration 2203, loss = 7.93316749\n",
            "Iteration 2204, loss = 7.86044591\n",
            "Iteration 2205, loss = 7.88784496\n",
            "Iteration 2206, loss = 7.88829463\n",
            "Iteration 2207, loss = 7.85465016\n",
            "Iteration 2208, loss = 7.86341880\n",
            "Iteration 2209, loss = 7.85760716\n",
            "Iteration 2210, loss = 7.86158855\n",
            "Iteration 2211, loss = 7.79295250\n",
            "Iteration 2212, loss = 7.83713531\n",
            "Iteration 2213, loss = 7.84988568\n",
            "Iteration 2214, loss = 7.95763614\n",
            "Iteration 2215, loss = 7.85217263\n",
            "Iteration 2216, loss = 7.83949314\n",
            "Iteration 2217, loss = 7.80283881\n",
            "Iteration 2218, loss = 7.74653409\n",
            "Iteration 2219, loss = 7.72692311\n",
            "Iteration 2220, loss = 7.74527666\n",
            "Iteration 2221, loss = 7.70545698\n",
            "Iteration 2222, loss = 7.71348661\n",
            "Iteration 2223, loss = 7.71600817\n",
            "Iteration 2224, loss = 7.71983155\n",
            "Iteration 2225, loss = 7.73977948\n",
            "Iteration 2226, loss = 7.66934720\n",
            "Iteration 2227, loss = 7.73427368\n",
            "Iteration 2228, loss = 7.68549851\n",
            "Iteration 2229, loss = 7.68923248\n",
            "Iteration 2230, loss = 7.70971262\n",
            "Iteration 2231, loss = 7.61729915\n",
            "Iteration 2232, loss = 7.66144625\n",
            "Iteration 2233, loss = 7.60026128\n",
            "Iteration 2234, loss = 7.67383742\n",
            "Iteration 2235, loss = 7.59541041\n",
            "Iteration 2236, loss = 7.59428520\n",
            "Iteration 2237, loss = 7.66757691\n",
            "Iteration 2238, loss = 7.57452234\n",
            "Iteration 2239, loss = 7.59999294\n",
            "Iteration 2240, loss = 7.61671376\n",
            "Iteration 2241, loss = 7.62923322\n",
            "Iteration 2242, loss = 7.57250747\n",
            "Iteration 2243, loss = 7.64815218\n",
            "Iteration 2244, loss = 7.64916106\n",
            "Iteration 2245, loss = 7.58074129\n",
            "Iteration 2246, loss = 7.60580986\n",
            "Iteration 2247, loss = 7.68191458\n",
            "Iteration 2248, loss = 7.57253114\n",
            "Iteration 2249, loss = 7.55480398\n",
            "Iteration 2250, loss = 7.58383983\n",
            "Iteration 2251, loss = 7.49528639\n",
            "Iteration 2252, loss = 7.53547201\n",
            "Iteration 2253, loss = 7.49296887\n",
            "Iteration 2254, loss = 7.46398344\n",
            "Iteration 2255, loss = 7.45926242\n",
            "Iteration 2256, loss = 7.42222727\n",
            "Iteration 2257, loss = 7.40931006\n",
            "Iteration 2258, loss = 7.39324551\n",
            "Iteration 2259, loss = 7.42081612\n",
            "Iteration 2260, loss = 7.53520942\n",
            "Iteration 2261, loss = 7.36136330\n",
            "Iteration 2262, loss = 7.44031848\n",
            "Iteration 2263, loss = 7.41275705\n",
            "Iteration 2264, loss = 7.37378621\n",
            "Iteration 2265, loss = 7.40520325\n",
            "Iteration 2266, loss = 7.33233217\n",
            "Iteration 2267, loss = 7.33296130\n",
            "Iteration 2268, loss = 7.30551992\n",
            "Iteration 2269, loss = 7.29935256\n",
            "Iteration 2270, loss = 7.28320738\n",
            "Iteration 2271, loss = 7.30977300\n",
            "Iteration 2272, loss = 7.28864914\n",
            "Iteration 2273, loss = 7.28502113\n",
            "Iteration 2274, loss = 7.26570547\n",
            "Iteration 2275, loss = 7.24905566\n",
            "Iteration 2276, loss = 7.25975014\n",
            "Iteration 2277, loss = 7.22620174\n",
            "Iteration 2278, loss = 7.24011010\n",
            "Iteration 2279, loss = 7.21473105\n",
            "Iteration 2280, loss = 7.21280225\n",
            "Iteration 2281, loss = 7.21397225\n",
            "Iteration 2282, loss = 7.19303052\n",
            "Iteration 2283, loss = 7.21464599\n",
            "Iteration 2284, loss = 7.23635585\n",
            "Iteration 2285, loss = 7.16535848\n",
            "Iteration 2286, loss = 7.24999640\n",
            "Iteration 2287, loss = 7.22319208\n",
            "Iteration 2288, loss = 7.18641027\n",
            "Iteration 2289, loss = 7.19131173\n",
            "Iteration 2290, loss = 7.16091417\n",
            "Iteration 2291, loss = 7.13878791\n",
            "Iteration 2292, loss = 7.15026437\n",
            "Iteration 2293, loss = 7.17752480\n",
            "Iteration 2294, loss = 7.18001990\n",
            "Iteration 2295, loss = 7.14475748\n",
            "Iteration 2296, loss = 7.15283562\n",
            "Iteration 2297, loss = 7.10646325\n",
            "Iteration 2298, loss = 7.07356354\n",
            "Iteration 2299, loss = 7.09188239\n",
            "Iteration 2300, loss = 7.04090475\n",
            "Iteration 2301, loss = 7.04183208\n",
            "Iteration 2302, loss = 7.03470916\n",
            "Iteration 2303, loss = 7.00786713\n",
            "Iteration 2304, loss = 7.00871130\n",
            "Iteration 2305, loss = 7.01332499\n",
            "Iteration 2306, loss = 6.97854321\n",
            "Iteration 2307, loss = 7.00498626\n",
            "Iteration 2308, loss = 6.98261843\n",
            "Iteration 2309, loss = 6.98085789\n",
            "Iteration 2310, loss = 6.95868158\n",
            "Iteration 2311, loss = 6.94802154\n",
            "Iteration 2312, loss = 6.94205501\n",
            "Iteration 2313, loss = 6.93939541\n",
            "Iteration 2314, loss = 6.93525113\n",
            "Iteration 2315, loss = 6.92692134\n",
            "Iteration 2316, loss = 6.90662272\n",
            "Iteration 2317, loss = 6.89927647\n",
            "Iteration 2318, loss = 6.88687224\n",
            "Iteration 2319, loss = 6.87887155\n",
            "Iteration 2320, loss = 6.87124268\n",
            "Iteration 2321, loss = 6.85598871\n",
            "Iteration 2322, loss = 6.85215998\n",
            "Iteration 2323, loss = 6.85010556\n",
            "Iteration 2324, loss = 6.83694539\n",
            "Iteration 2325, loss = 6.84460820\n",
            "Iteration 2326, loss = 6.82662966\n",
            "Iteration 2327, loss = 6.83663686\n",
            "Iteration 2328, loss = 6.87146231\n",
            "Iteration 2329, loss = 6.85409688\n",
            "Iteration 2330, loss = 6.79923457\n",
            "Iteration 2331, loss = 6.85376930\n",
            "Iteration 2332, loss = 6.79241122\n",
            "Iteration 2333, loss = 6.78767358\n",
            "Iteration 2334, loss = 6.75137165\n",
            "Iteration 2335, loss = 6.76727153\n",
            "Iteration 2336, loss = 6.73895463\n",
            "Iteration 2337, loss = 6.73841150\n",
            "Iteration 2338, loss = 6.74808607\n",
            "Iteration 2339, loss = 6.73566414\n",
            "Iteration 2340, loss = 6.71558877\n",
            "Iteration 2341, loss = 6.70597326\n",
            "Iteration 2342, loss = 6.68275421\n",
            "Iteration 2343, loss = 6.67749854\n",
            "Iteration 2344, loss = 6.68781962\n",
            "Iteration 2345, loss = 6.69377846\n",
            "Iteration 2346, loss = 6.65870188\n",
            "Iteration 2347, loss = 6.65887086\n",
            "Iteration 2348, loss = 6.67270051\n",
            "Iteration 2349, loss = 6.65643140\n",
            "Iteration 2350, loss = 6.63381186\n",
            "Iteration 2351, loss = 6.65262753\n",
            "Iteration 2352, loss = 6.69286689\n",
            "Iteration 2353, loss = 6.62285014\n",
            "Iteration 2354, loss = 6.64046774\n",
            "Iteration 2355, loss = 6.63305938\n",
            "Iteration 2356, loss = 6.61665735\n",
            "Iteration 2357, loss = 6.59666614\n",
            "Iteration 2358, loss = 6.57132516\n",
            "Iteration 2359, loss = 6.57657237\n",
            "Iteration 2360, loss = 6.56585180\n",
            "Iteration 2361, loss = 6.55404965\n",
            "Iteration 2362, loss = 6.53725914\n",
            "Iteration 2363, loss = 6.53905282\n",
            "Iteration 2364, loss = 6.53958004\n",
            "Iteration 2365, loss = 6.52434109\n",
            "Iteration 2366, loss = 6.54285672\n",
            "Iteration 2367, loss = 6.50689159\n",
            "Iteration 2368, loss = 6.48681126\n",
            "Iteration 2369, loss = 6.52714779\n",
            "Iteration 2370, loss = 6.56768404\n",
            "Iteration 2371, loss = 6.48779430\n",
            "Iteration 2372, loss = 6.52894249\n",
            "Iteration 2373, loss = 6.54266758\n",
            "Iteration 2374, loss = 6.43035031\n",
            "Iteration 2375, loss = 6.49183163\n",
            "Iteration 2376, loss = 6.55854564\n",
            "Iteration 2377, loss = 6.51044390\n",
            "Iteration 2378, loss = 6.48520612\n",
            "Iteration 2379, loss = 6.55129237\n",
            "Iteration 2380, loss = 6.49061279\n",
            "Iteration 2381, loss = 6.46950197\n",
            "Iteration 2382, loss = 6.39502684\n",
            "Iteration 2383, loss = 6.44950983\n",
            "Iteration 2384, loss = 6.37174782\n",
            "Iteration 2385, loss = 6.37857778\n",
            "Iteration 2386, loss = 6.38082262\n",
            "Iteration 2387, loss = 6.37597579\n",
            "Iteration 2388, loss = 6.34445761\n",
            "Iteration 2389, loss = 6.37411423\n",
            "Iteration 2390, loss = 6.36595269\n",
            "Iteration 2391, loss = 6.38023026\n",
            "Iteration 2392, loss = 6.33973755\n",
            "Iteration 2393, loss = 6.33196497\n",
            "Iteration 2394, loss = 6.29964063\n",
            "Iteration 2395, loss = 6.28469147\n",
            "Iteration 2396, loss = 6.30491827\n",
            "Iteration 2397, loss = 6.28045239\n",
            "Iteration 2398, loss = 6.26911997\n",
            "Iteration 2399, loss = 6.25708018\n",
            "Iteration 2400, loss = 6.25277834\n",
            "Iteration 2401, loss = 6.23925766\n",
            "Iteration 2402, loss = 6.25286508\n",
            "Iteration 2403, loss = 6.23656973\n",
            "Iteration 2404, loss = 6.22247038\n",
            "Iteration 2405, loss = 6.23334217\n",
            "Iteration 2406, loss = 6.25338090\n",
            "Iteration 2407, loss = 6.22833284\n",
            "Iteration 2408, loss = 6.20982693\n",
            "Iteration 2409, loss = 6.21631133\n",
            "Iteration 2410, loss = 6.19458241\n",
            "Iteration 2411, loss = 6.16780665\n",
            "Iteration 2412, loss = 6.18484737\n",
            "Iteration 2413, loss = 6.16848314\n",
            "Iteration 2414, loss = 6.14193256\n",
            "Iteration 2415, loss = 6.15196173\n",
            "Iteration 2416, loss = 6.14416168\n",
            "Iteration 2417, loss = 6.13253676\n",
            "Iteration 2418, loss = 6.12781243\n",
            "Iteration 2419, loss = 6.11898294\n",
            "Iteration 2420, loss = 6.13562972\n",
            "Iteration 2421, loss = 6.11483758\n",
            "Iteration 2422, loss = 6.10542007\n",
            "Iteration 2423, loss = 6.11341099\n",
            "Iteration 2424, loss = 6.08501591\n",
            "Iteration 2425, loss = 6.06658839\n",
            "Iteration 2426, loss = 6.07550966\n",
            "Iteration 2427, loss = 6.11251098\n",
            "Iteration 2428, loss = 6.07370523\n",
            "Iteration 2429, loss = 6.05865378\n",
            "Iteration 2430, loss = 6.07075564\n",
            "Iteration 2431, loss = 6.05569032\n",
            "Iteration 2432, loss = 6.02219711\n",
            "Iteration 2433, loss = 6.02843506\n",
            "Iteration 2434, loss = 6.01938206\n",
            "Iteration 2435, loss = 6.01257203\n",
            "Iteration 2436, loss = 6.00624898\n",
            "Iteration 2437, loss = 5.99789877\n",
            "Iteration 2438, loss = 5.98394500\n",
            "Iteration 2439, loss = 5.99552947\n",
            "Iteration 2440, loss = 5.97235327\n",
            "Iteration 2441, loss = 5.98675919\n",
            "Iteration 2442, loss = 5.95890130\n",
            "Iteration 2443, loss = 5.96307665\n",
            "Iteration 2444, loss = 5.98779605\n",
            "Iteration 2445, loss = 5.95811716\n",
            "Iteration 2446, loss = 5.97484017\n",
            "Iteration 2447, loss = 5.94744930\n",
            "Iteration 2448, loss = 5.93550758\n",
            "Iteration 2449, loss = 5.94988588\n",
            "Iteration 2450, loss = 5.93972514\n",
            "Iteration 2451, loss = 5.92032601\n",
            "Iteration 2452, loss = 5.92933545\n",
            "Iteration 2453, loss = 5.89610054\n",
            "Iteration 2454, loss = 5.89395539\n",
            "Iteration 2455, loss = 5.88889843\n",
            "Iteration 2456, loss = 5.93861050\n",
            "Iteration 2457, loss = 5.86736328\n",
            "Iteration 2458, loss = 5.89401014\n",
            "Iteration 2459, loss = 5.87332843\n",
            "Iteration 2460, loss = 5.84205449\n",
            "Iteration 2461, loss = 5.85934546\n",
            "Iteration 2462, loss = 5.84782293\n",
            "Iteration 2463, loss = 5.82511241\n",
            "Iteration 2464, loss = 5.81475066\n",
            "Iteration 2465, loss = 5.82456793\n",
            "Iteration 2466, loss = 5.80725523\n",
            "Iteration 2467, loss = 5.78707828\n",
            "Iteration 2468, loss = 5.78557871\n",
            "Iteration 2469, loss = 5.77405670\n",
            "Iteration 2470, loss = 5.77743857\n",
            "Iteration 2471, loss = 5.76846295\n",
            "Iteration 2472, loss = 5.75164321\n",
            "Iteration 2473, loss = 5.76860976\n",
            "Iteration 2474, loss = 5.75703294\n",
            "Iteration 2475, loss = 5.74660454\n",
            "Iteration 2476, loss = 5.75752735\n",
            "Iteration 2477, loss = 5.72620635\n",
            "Iteration 2478, loss = 5.73340049\n",
            "Iteration 2479, loss = 5.74859975\n",
            "Iteration 2480, loss = 5.72557648\n",
            "Iteration 2481, loss = 5.71432485\n",
            "Iteration 2482, loss = 5.72557819\n",
            "Iteration 2483, loss = 5.72481760\n",
            "Iteration 2484, loss = 5.69620354\n",
            "Iteration 2485, loss = 5.71986824\n",
            "Iteration 2486, loss = 5.71769229\n",
            "Iteration 2487, loss = 5.77623930\n",
            "Iteration 2488, loss = 5.76294413\n",
            "Iteration 2489, loss = 5.69777520\n",
            "Iteration 2490, loss = 5.69603569\n",
            "Iteration 2491, loss = 5.78114971\n",
            "Iteration 2492, loss = 5.70553704\n",
            "Iteration 2493, loss = 5.62640446\n",
            "Iteration 2494, loss = 5.71348739\n",
            "Iteration 2495, loss = 5.67650402\n",
            "Iteration 2496, loss = 5.62461418\n",
            "Iteration 2497, loss = 5.62395133\n",
            "Iteration 2498, loss = 5.69444654\n",
            "Iteration 2499, loss = 5.76194710\n",
            "Iteration 2500, loss = 5.70878126\n",
            "Iteration 2501, loss = 5.78996650\n",
            "Iteration 2502, loss = 5.77002138\n",
            "Iteration 2503, loss = 5.67768035\n",
            "Iteration 2504, loss = 5.57746162\n",
            "Iteration 2505, loss = 5.60296886\n",
            "Iteration 2506, loss = 5.58491899\n",
            "Iteration 2507, loss = 5.54077951\n",
            "Iteration 2508, loss = 5.55576366\n",
            "Iteration 2509, loss = 5.56563639\n",
            "Iteration 2510, loss = 5.57227621\n",
            "Iteration 2511, loss = 5.53765954\n",
            "Iteration 2512, loss = 5.54103758\n",
            "Iteration 2513, loss = 5.52827431\n",
            "Iteration 2514, loss = 5.55888004\n",
            "Iteration 2515, loss = 5.55158205\n",
            "Iteration 2516, loss = 5.47889158\n",
            "Iteration 2517, loss = 5.52855159\n",
            "Iteration 2518, loss = 5.52760609\n",
            "Iteration 2519, loss = 5.46234003\n",
            "Iteration 2520, loss = 5.48039485\n",
            "Iteration 2521, loss = 5.46755713\n",
            "Iteration 2522, loss = 5.45205143\n",
            "Iteration 2523, loss = 5.47136579\n",
            "Iteration 2524, loss = 5.44251212\n",
            "Iteration 2525, loss = 5.45270010\n",
            "Iteration 2526, loss = 5.41931023\n",
            "Iteration 2527, loss = 5.44049793\n",
            "Iteration 2528, loss = 5.46938706\n",
            "Iteration 2529, loss = 5.44059669\n",
            "Iteration 2530, loss = 5.40160323\n",
            "Iteration 2531, loss = 5.47235487\n",
            "Iteration 2532, loss = 5.47571621\n",
            "Iteration 2533, loss = 5.43967536\n",
            "Iteration 2534, loss = 5.39626651\n",
            "Iteration 2535, loss = 5.48046645\n",
            "Iteration 2536, loss = 5.39813791\n",
            "Iteration 2537, loss = 5.38136280\n",
            "Iteration 2538, loss = 5.39642253\n",
            "Iteration 2539, loss = 5.36657755\n",
            "Iteration 2540, loss = 5.35877742\n",
            "Iteration 2541, loss = 5.33215721\n",
            "Iteration 2542, loss = 5.35716164\n",
            "Iteration 2543, loss = 5.38575292\n",
            "Iteration 2544, loss = 5.39259487\n",
            "Iteration 2545, loss = 5.40247831\n",
            "Iteration 2546, loss = 5.31941687\n",
            "Iteration 2547, loss = 5.39004578\n",
            "Iteration 2548, loss = 5.33256289\n",
            "Iteration 2549, loss = 5.36075685\n",
            "Iteration 2550, loss = 5.29122584\n",
            "Iteration 2551, loss = 5.30437755\n",
            "Iteration 2552, loss = 5.35674063\n",
            "Iteration 2553, loss = 5.33042692\n",
            "Iteration 2554, loss = 5.27365664\n",
            "Iteration 2555, loss = 5.31244965\n",
            "Iteration 2556, loss = 5.26819477\n",
            "Iteration 2557, loss = 5.28277996\n",
            "Iteration 2558, loss = 5.25105395\n",
            "Iteration 2559, loss = 5.24249659\n",
            "Iteration 2560, loss = 5.25299497\n",
            "Iteration 2561, loss = 5.23987476\n",
            "Iteration 2562, loss = 5.20486332\n",
            "Iteration 2563, loss = 5.23056795\n",
            "Iteration 2564, loss = 5.21851986\n",
            "Iteration 2565, loss = 5.22269010\n",
            "Iteration 2566, loss = 5.19528568\n",
            "Iteration 2567, loss = 5.19838623\n",
            "Iteration 2568, loss = 5.21766133\n",
            "Iteration 2569, loss = 5.19167126\n",
            "Iteration 2570, loss = 5.17736747\n",
            "Iteration 2571, loss = 5.18379917\n",
            "Iteration 2572, loss = 5.18536706\n",
            "Iteration 2573, loss = 5.15643316\n",
            "Iteration 2574, loss = 5.15209316\n",
            "Iteration 2575, loss = 5.15151384\n",
            "Iteration 2576, loss = 5.15073541\n",
            "Iteration 2577, loss = 5.14648434\n",
            "Iteration 2578, loss = 5.12019410\n",
            "Iteration 2579, loss = 5.14745823\n",
            "Iteration 2580, loss = 5.13561677\n",
            "Iteration 2581, loss = 5.12066353\n",
            "Iteration 2582, loss = 5.09397197\n",
            "Iteration 2583, loss = 5.12396390\n",
            "Iteration 2584, loss = 5.10715219\n",
            "Iteration 2585, loss = 5.08763105\n",
            "Iteration 2586, loss = 5.09114325\n",
            "Iteration 2587, loss = 5.09582817\n",
            "Iteration 2588, loss = 5.06589483\n",
            "Iteration 2589, loss = 5.06986325\n",
            "Iteration 2590, loss = 5.06501534\n",
            "Iteration 2591, loss = 5.06786257\n",
            "Iteration 2592, loss = 5.03209580\n",
            "Iteration 2593, loss = 5.07390546\n",
            "Iteration 2594, loss = 5.02723384\n",
            "Iteration 2595, loss = 5.03259896\n",
            "Iteration 2596, loss = 5.02006785\n",
            "Iteration 2597, loss = 5.02429120\n",
            "Iteration 2598, loss = 5.02573469\n",
            "Iteration 2599, loss = 5.00586908\n",
            "Iteration 2600, loss = 5.00550649\n",
            "Iteration 2601, loss = 5.04880185\n",
            "Iteration 2602, loss = 5.04120555\n",
            "Iteration 2603, loss = 4.99168234\n",
            "Iteration 2604, loss = 5.06213052\n",
            "Iteration 2605, loss = 5.02344817\n",
            "Iteration 2606, loss = 5.00794066\n",
            "Iteration 2607, loss = 4.96451211\n",
            "Iteration 2608, loss = 4.98519228\n",
            "Iteration 2609, loss = 4.95576640\n",
            "Iteration 2610, loss = 4.95434576\n",
            "Iteration 2611, loss = 4.97744417\n",
            "Iteration 2612, loss = 4.95040426\n",
            "Iteration 2613, loss = 4.93210858\n",
            "Iteration 2614, loss = 4.93799112\n",
            "Iteration 2615, loss = 4.94174797\n",
            "Iteration 2616, loss = 4.93550886\n",
            "Iteration 2617, loss = 4.93230337\n",
            "Iteration 2618, loss = 4.95324466\n",
            "Iteration 2619, loss = 4.93451510\n",
            "Iteration 2620, loss = 4.89368588\n",
            "Iteration 2621, loss = 4.93686626\n",
            "Iteration 2622, loss = 4.99301151\n",
            "Iteration 2623, loss = 5.07706940\n",
            "Iteration 2624, loss = 4.93209310\n",
            "Iteration 2625, loss = 4.95170764\n",
            "Iteration 2626, loss = 5.05112135\n",
            "Iteration 2627, loss = 4.93487474\n",
            "Iteration 2628, loss = 4.86743120\n",
            "Iteration 2629, loss = 4.88528208\n",
            "Iteration 2630, loss = 4.85046576\n",
            "Iteration 2631, loss = 4.87145151\n",
            "Iteration 2632, loss = 4.82715445\n",
            "Iteration 2633, loss = 4.84240689\n",
            "Iteration 2634, loss = 4.82145524\n",
            "Iteration 2635, loss = 4.82441523\n",
            "Iteration 2636, loss = 4.81460512\n",
            "Iteration 2637, loss = 4.81181252\n",
            "Iteration 2638, loss = 4.80214667\n",
            "Iteration 2639, loss = 4.78622114\n",
            "Iteration 2640, loss = 4.79782123\n",
            "Iteration 2641, loss = 4.78261894\n",
            "Iteration 2642, loss = 4.77982803\n",
            "Iteration 2643, loss = 4.77727499\n",
            "Iteration 2644, loss = 4.76257189\n",
            "Iteration 2645, loss = 4.76487970\n",
            "Iteration 2646, loss = 4.75885399\n",
            "Iteration 2647, loss = 4.76684585\n",
            "Iteration 2648, loss = 4.75965135\n",
            "Iteration 2649, loss = 4.73435793\n",
            "Iteration 2650, loss = 4.74909594\n",
            "Iteration 2651, loss = 4.73814954\n",
            "Iteration 2652, loss = 4.72246761\n",
            "Iteration 2653, loss = 4.70572932\n",
            "Iteration 2654, loss = 4.70737319\n",
            "Iteration 2655, loss = 4.70012965\n",
            "Iteration 2656, loss = 4.69808535\n",
            "Iteration 2657, loss = 4.68867854\n",
            "Iteration 2658, loss = 4.67543061\n",
            "Iteration 2659, loss = 4.67611051\n",
            "Iteration 2660, loss = 4.66304158\n",
            "Iteration 2661, loss = 4.66254762\n",
            "Iteration 2662, loss = 4.66326141\n",
            "Iteration 2663, loss = 4.65223041\n",
            "Iteration 2664, loss = 4.65835876\n",
            "Iteration 2665, loss = 4.64429374\n",
            "Iteration 2666, loss = 4.63058428\n",
            "Iteration 2667, loss = 4.65816567\n",
            "Iteration 2668, loss = 4.62751918\n",
            "Iteration 2669, loss = 4.62072049\n",
            "Iteration 2670, loss = 4.63973856\n",
            "Iteration 2671, loss = 4.64094133\n",
            "Iteration 2672, loss = 4.63042696\n",
            "Iteration 2673, loss = 4.64219914\n",
            "Iteration 2674, loss = 4.65810768\n",
            "Iteration 2675, loss = 4.63161276\n",
            "Iteration 2676, loss = 4.59830145\n",
            "Iteration 2677, loss = 4.64444091\n",
            "Iteration 2678, loss = 4.63310055\n",
            "Iteration 2679, loss = 4.58416584\n",
            "Iteration 2680, loss = 4.59387403\n",
            "Iteration 2681, loss = 4.58996904\n",
            "Iteration 2682, loss = 4.57541380\n",
            "Iteration 2683, loss = 4.55219074\n",
            "Iteration 2684, loss = 4.53089388\n",
            "Iteration 2685, loss = 4.57094583\n",
            "Iteration 2686, loss = 4.52505490\n",
            "Iteration 2687, loss = 4.53548645\n",
            "Iteration 2688, loss = 4.54324879\n",
            "Iteration 2689, loss = 4.52976836\n",
            "Iteration 2690, loss = 4.56126413\n",
            "Iteration 2691, loss = 4.58430771\n",
            "Iteration 2692, loss = 4.54888464\n",
            "Iteration 2693, loss = 4.49286320\n",
            "Iteration 2694, loss = 4.55179020\n",
            "Iteration 2695, loss = 4.51244152\n",
            "Iteration 2696, loss = 4.49763706\n",
            "Iteration 2697, loss = 4.56868932\n",
            "Iteration 2698, loss = 4.55921941\n",
            "Iteration 2699, loss = 4.52899188\n",
            "Iteration 2700, loss = 4.51087137\n",
            "Iteration 2701, loss = 4.56935645\n",
            "Iteration 2702, loss = 4.48854986\n",
            "Iteration 2703, loss = 4.45501912\n",
            "Iteration 2704, loss = 4.44498708\n",
            "Iteration 2705, loss = 4.44864778\n",
            "Iteration 2706, loss = 4.42214644\n",
            "Iteration 2707, loss = 4.43764092\n",
            "Iteration 2708, loss = 4.41354512\n",
            "Iteration 2709, loss = 4.41390937\n",
            "Iteration 2710, loss = 4.40313327\n",
            "Iteration 2711, loss = 4.40138359\n",
            "Iteration 2712, loss = 4.39372911\n",
            "Iteration 2713, loss = 4.39781348\n",
            "Iteration 2714, loss = 4.40838395\n",
            "Iteration 2715, loss = 4.38651566\n",
            "Iteration 2716, loss = 4.38220057\n",
            "Iteration 2717, loss = 4.38424133\n",
            "Iteration 2718, loss = 4.36641030\n",
            "Iteration 2719, loss = 4.36314229\n",
            "Iteration 2720, loss = 4.36252671\n",
            "Iteration 2721, loss = 4.37012220\n",
            "Iteration 2722, loss = 4.34923060\n",
            "Iteration 2723, loss = 4.36291087\n",
            "Iteration 2724, loss = 4.34522131\n",
            "Iteration 2725, loss = 4.33625167\n",
            "Iteration 2726, loss = 4.34918322\n",
            "Iteration 2727, loss = 4.36379259\n",
            "Iteration 2728, loss = 4.33332747\n",
            "Iteration 2729, loss = 4.29813354\n",
            "Iteration 2730, loss = 4.32541759\n",
            "Iteration 2731, loss = 4.32567004\n",
            "Iteration 2732, loss = 4.29275991\n",
            "Iteration 2733, loss = 4.33104071\n",
            "Iteration 2734, loss = 4.32247286\n",
            "Iteration 2735, loss = 4.27376804\n",
            "Iteration 2736, loss = 4.28895758\n",
            "Iteration 2737, loss = 4.28709975\n",
            "Iteration 2738, loss = 4.29846719\n",
            "Iteration 2739, loss = 4.31793708\n",
            "Iteration 2740, loss = 4.24407852\n",
            "Iteration 2741, loss = 4.28148651\n",
            "Iteration 2742, loss = 4.28599819\n",
            "Iteration 2743, loss = 4.25735011\n",
            "Iteration 2744, loss = 4.25438421\n",
            "Iteration 2745, loss = 4.25660785\n",
            "Iteration 2746, loss = 4.22500329\n",
            "Iteration 2747, loss = 4.23639609\n",
            "Iteration 2748, loss = 4.23022410\n",
            "Iteration 2749, loss = 4.21684020\n",
            "Iteration 2750, loss = 4.19781231\n",
            "Iteration 2751, loss = 4.21047265\n",
            "Iteration 2752, loss = 4.19203466\n",
            "Iteration 2753, loss = 4.20135421\n",
            "Iteration 2754, loss = 4.18878556\n",
            "Iteration 2755, loss = 4.18387264\n",
            "Iteration 2756, loss = 4.17548645\n",
            "Iteration 2757, loss = 4.17791653\n",
            "Iteration 2758, loss = 4.16229268\n",
            "Iteration 2759, loss = 4.16501779\n",
            "Iteration 2760, loss = 4.15831826\n",
            "Iteration 2761, loss = 4.17910800\n",
            "Iteration 2762, loss = 4.15418093\n",
            "Iteration 2763, loss = 4.14597763\n",
            "Iteration 2764, loss = 4.15621885\n",
            "Iteration 2765, loss = 4.13638137\n",
            "Iteration 2766, loss = 4.13030382\n",
            "Iteration 2767, loss = 4.12376090\n",
            "Iteration 2768, loss = 4.11466117\n",
            "Iteration 2769, loss = 4.11591161\n",
            "Iteration 2770, loss = 4.11465432\n",
            "Iteration 2771, loss = 4.10689597\n",
            "Iteration 2772, loss = 4.10895376\n",
            "Iteration 2773, loss = 4.10076598\n",
            "Iteration 2774, loss = 4.11090948\n",
            "Iteration 2775, loss = 4.08324545\n",
            "Iteration 2776, loss = 4.08065025\n",
            "Iteration 2777, loss = 4.07043155\n",
            "Iteration 2778, loss = 4.06750634\n",
            "Iteration 2779, loss = 4.06111817\n",
            "Iteration 2780, loss = 4.05366425\n",
            "Iteration 2781, loss = 4.07204260\n",
            "Iteration 2782, loss = 4.07648302\n",
            "Iteration 2783, loss = 4.07130993\n",
            "Iteration 2784, loss = 4.11351507\n",
            "Iteration 2785, loss = 4.13584188\n",
            "Iteration 2786, loss = 4.05012993\n",
            "Iteration 2787, loss = 4.05013328\n",
            "Iteration 2788, loss = 4.13502024\n",
            "Iteration 2789, loss = 4.07493677\n",
            "Iteration 2790, loss = 4.08198361\n",
            "Iteration 2791, loss = 4.12622795\n",
            "Iteration 2792, loss = 4.01582776\n",
            "Iteration 2793, loss = 4.01911587\n",
            "Iteration 2794, loss = 4.14098141\n",
            "Iteration 2795, loss = 4.01178105\n",
            "Iteration 2796, loss = 4.02090647\n",
            "Iteration 2797, loss = 4.03492941\n",
            "Iteration 2798, loss = 3.99281235\n",
            "Iteration 2799, loss = 4.03144415\n",
            "Iteration 2800, loss = 4.00483078\n",
            "Iteration 2801, loss = 3.97109859\n",
            "Iteration 2802, loss = 3.98193583\n",
            "Iteration 2803, loss = 3.96230067\n",
            "Iteration 2804, loss = 3.95979496\n",
            "Iteration 2805, loss = 3.94477383\n",
            "Iteration 2806, loss = 3.94296600\n",
            "Iteration 2807, loss = 3.93256386\n",
            "Iteration 2808, loss = 3.92301911\n",
            "Iteration 2809, loss = 3.92016372\n",
            "Iteration 2810, loss = 3.91862755\n",
            "Iteration 2811, loss = 3.91689894\n",
            "Iteration 2812, loss = 3.90460534\n",
            "Iteration 2813, loss = 3.92743852\n",
            "Iteration 2814, loss = 3.90098047\n",
            "Iteration 2815, loss = 3.90081702\n",
            "Iteration 2816, loss = 3.88121010\n",
            "Iteration 2817, loss = 3.91345600\n",
            "Iteration 2818, loss = 3.91179430\n",
            "Iteration 2819, loss = 3.88368115\n",
            "Iteration 2820, loss = 3.87082517\n",
            "Iteration 2821, loss = 3.89264174\n",
            "Iteration 2822, loss = 3.86931594\n",
            "Iteration 2823, loss = 3.86658319\n",
            "Iteration 2824, loss = 3.85188812\n",
            "Iteration 2825, loss = 3.85993552\n",
            "Iteration 2826, loss = 3.84673588\n",
            "Iteration 2827, loss = 3.85234623\n",
            "Iteration 2828, loss = 3.85463968\n",
            "Iteration 2829, loss = 3.84161872\n",
            "Iteration 2830, loss = 3.84766711\n",
            "Iteration 2831, loss = 3.84391941\n",
            "Iteration 2832, loss = 3.84153177\n",
            "Iteration 2833, loss = 3.80167958\n",
            "Iteration 2834, loss = 3.84869674\n",
            "Iteration 2835, loss = 3.91602178\n",
            "Iteration 2836, loss = 3.92267103\n",
            "Iteration 2837, loss = 3.90940981\n",
            "Iteration 2838, loss = 3.83171259\n",
            "Iteration 2839, loss = 3.91915725\n",
            "Iteration 2840, loss = 3.83614348\n",
            "Iteration 2841, loss = 3.86674713\n",
            "Iteration 2842, loss = 3.85832095\n",
            "Iteration 2843, loss = 3.87946052\n",
            "Iteration 2844, loss = 3.85287377\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPRegressor(activation='logistic', hidden_layer_sizes=(150, 20), max_iter=5000,\n",
              "             random_state=1, verbose=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-2 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-2 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-2 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-2 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-2 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPRegressor(activation=&#x27;logistic&#x27;, hidden_layer_sizes=(150, 20), max_iter=5000,\n",
              "             random_state=1, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>MLPRegressor</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.neural_network.MLPRegressor.html\">?<span>Documentation for MLPRegressor</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>MLPRegressor(activation=&#x27;logistic&#x27;, hidden_layer_sizes=(150, 20), max_iter=5000,\n",
              "             random_state=1, verbose=True)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yhat=model.predict(x.values)\n",
        "yhat_train=model.predict(x_train.values)\n",
        "yhat_test=model.predict(x_test.values)\n"
      ],
      "metadata": {
        "id": "P8Ve3Va9dww1",
        "outputId": "1d9a64ee-7f72-4a7f-eef1-abd63e8db010",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(y_test,yhat_test)\n",
        "plt.plot(y_test,y_test,'r--')\n",
        "plt.xlabel('prices')\n",
        "plt.ylabel('predicted_price')"
      ],
      "metadata": {
        "id": "5iXAsGNyepQl",
        "outputId": "5f1b8edc-4557-428a-c67e-624c1e5c4bf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'predicted_price')"
            ]
          },
          "metadata": {},
          "execution_count": 38
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "met.mean_squared_error(y_test,yhat_test)"
      ],
      "metadata": {
        "id": "IP9lZHiiA7NT",
        "outputId": "e4bb618e-2ef2-475e-f01f-aac5caafb4c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16.145861533345432"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(x_train.values,y_train)"
      ],
      "metadata": {
        "id": "dnJy_RIiDGRL",
        "outputId": "121fa2aa-323d-4e86-d6fa-3bdca4b2419b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9140080054203732"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(x_test.values,y_test)"
      ],
      "metadata": {
        "id": "a3uOZtAxDb_4",
        "outputId": "8cd2286b-7289-4b0d-9f33-ffae2629dd8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7833151151445631"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "scratchpad",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}